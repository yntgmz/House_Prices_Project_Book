
---
title: "Pre-Processing"
output:
  html_document:
    df_print: paged
bibliography: [book.bib, packages.bib]
---

```{r setup, include=FALSE, eval =TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(bookdown)
library(kableExtra)
library(knitr)          #for tables
library(DataExplorer)   #for plot missing 
library(RColorBrewer)   #for color palattes
library(data.table)     #for custom function
library(ggplot2)        #for general plotting
library("GGally")       #for ggcor
library(Rcpp)           #missmap
library(Amelia)         #missmap
library(dplyr)          #mutate(), select()
library(ggridges)       #for ridge plot
library(stringr)        #for looking at columns containg "string"
library(imputeMissings) #impute missing values with median/mode or random forest
library(mice)           #impute missing values
library(forcats)        #for factor reorder
library(caret)          #for hot one encoding
library(plyr)           #to reorder factors 
library(vip)            #to show 
library(psych)          #corplot
require(Metrics)        #for rmse
library(DiagrammeR)     #for workflow diagram
library(gridExtra)      #for tables and graps
library(data.table)     #to create tables
library(RColorBrewer)   #For graph colors
library(naniar)         #plot missing values
library(car)            #for VIF
library(vip)            #for variable importance
library(gbm)            #for gbm model and plot
library(Metrics)        #for RMSE
library(Boruta)         #Boruta() for feature selection
library(gridExtra)
library(xgboost)
library(doParallel)
library(lattice)        #for plots
library(Matrix)
library(glmnet)         #fitting regularized models  
library(coefplot)       #for extracting coefficients

```
# Data Pre-processing
The goal of this project is to predict the sale price of houses using a large set of house features by applying advanced regression techniques. However, before we jump into building a highly accurate prediction model, an extensive amount of data pre-processing has to be done. The major pre-processing tasks employed in this project are: data cleaning, to address missing values and noisy data; data transformation via smoothing, attribute construction, aggregation, discretization and hierarchy generation in order to transform the data into appropriate forms for mining; and feature selection by removing redundant features and features with near zero variability.

The first step is to explore the data and identify any discrepancies or opportunities for improvement by examining the following: 

- Missing Values 

- Attribute types 

- Distribution, Skewness and Relationships

- Dimensionality 


## Identifying and Correcting Missing Values

### Detect Missing Values

```{r data, echo=FALSE}
# Missing Values per column and rows 
train_raw<-read.csv("train.csv",stringsAsFactors = TRUE) # 1460 rows,  81 columns
test_raw<-read.csv("test.csv", stringsAsFactors = TRUE)

#Merge Datasets for preprocessing steps. Separate into train and test sets later for modeling and prediction. 
test_raw$"SalePrice"<-0                   # Create "SalePrice" feature for test set
data_full<- rbind(train_raw, test_raw)    # Combine hdb_train and hdb_test
```


```{r DetectMissVals, eval=TRUE, collapse = TRUE, comment="***"}
col_na<-which(colSums(is.na(data_full)) > 0)
col_na_sum<-sum(colSums(is.na(data_full)) > 0)
cat("The total number of missing values in the combined data-set is:", sum(is.na(data_full)),"\nThe number of columns containing missing values is:", col_na_sum)
```

We can see from the heatmap in \@ref(fig:MissValsViz1), that about 6% of the data is "missing". A simple strategy to deal with missing values is to eliminate the features or examples containing missing values. However, given that there are a total of 13965 missing values spread across 34 features, dropping these data points could mean loss of data critical to the analysis. Instead, we will deal with missing values by imputation using a variety of methods. 

```{r MissValsViz1, eval=TRUE, fig.cap= "Missing Values Heatmap", fig.align = 'center'}
# Heat map for missing values of the housing dataset with function missmap from library(Rcpp) 
missmap(data_full,col = c("blue","gray"), main ="Heatmap showing Missing values")
```

Figure \@ref(fig:MissValsViz2) shows that the features "PoolQC", "MiscFeature", "Alley", and "Fence" have a high number of missing values. It might be tempting to drop these features, but with one quick look at the data description provided with the data [@de2011ames] we learn that "NA" in these cases means that the feature is not applicable, so it should be either "0" or "None".


```{r, MissValsViz2, eval=TRUE, fig.cap= "Number of Missing Values per Column", fig.align = 'center'}
NA_col_list <- sort(col_na, decreasing = T) # arrange the named list with descending order
gg_miss_var(data_full[,NA_col_list], show_pct = FALSE )+theme(text = element_text(size = 8,))
```

Table \@ref(tab:MissValsSumm) shows the number and percentge of missing values per column in the combined dataset. 

```{r MissValsSumm, eval=TRUE, collapse = TRUE}
# Summary of Missing Values per Column/Variable
kable(miss_var_summary(data_full[,NA_col_list]), caption='Summary of Missing Values per Column', booktabs = TRUE)

```

### Dealing With Missing Values

There are two types of missing values in this data-set, some values classified as "NA" are truly missing from the data, the information was not collected. The second type of "NA" means that the feature is not present, so "NA" in this case means either 'Zero' for numerical features,  'None' for categorical features, or 'No' for binary features. By examining the data and reading the data description provided [@de2011ames], we are able to determine each case. 

#### Manual Imputation of NAs due to Inconsistent Values

Starting with groups of related features for garage, basement, pool and masonry attributes to explore the columns containing missing values, we discover that there are a few inconsistencies in the raw data, for example for the feature MasVnrType, there is a missing value for row ID=2611, but the column "MasVnrArea" shows a value, this obviously indicates that there is a MasVnrType associated with this instance, so instead of replacing it with "None", we will impute the missing value with the most common value for this feature. 


```{r NAMasonry, eval=TRUE}
masonry_features <- names(data_full)[sapply(names(data_full), function(x) str_detect(x, "Mas"))]
mas_na<-which(is.na(data_full$MasVnrType) & data_full$MasVnrArea >0)
data_full[mas_na,masonry_features]

```

For features containing inconsistent values, listed in table \@ref(tab:manual), we will impute the missing values mannually according to each case. 

```{r manual, echo=TRUE,eval=TRUE, collapse = TRUE}
Inconsitent_Values<- c("BsmtQual", "BsmtCond",  "GarageQual","GarageFinish", "GarageCond",  "GarageYrBlt","BsmtCond", "  PoolQC", "MasVnrArea")
Inconsitent_Values<-tibble("Feature"=Inconsitent_Values, "Method"=rep("Manual Imputation", length(Inconsitent_Values)))
kable(Inconsitent_Values, caption = "Inconsistent Values")
```
**Garage Features**

_Row 2127 has a garage, given that it has values for "GarageArea" (360), "GarageType" (Detchd), and "GarageCars" (1), so fill in the 'GarageQual','GarageFinish', and 'GarageCond' with most the common  for those features values._

```{r Garage Features Missing Values, collapse = TRUE, eval=TRUE, echo=TRUE, comment="***"}
garage_features <- names(data_full)[sapply(names(data_full), function(x) str_detect(x, "Garage"))]
#View(data_full[which(is.na(data_full$GarageCond)), garage_features]) # To look at all garage features containing missing values
#count(data_full[which(is.na(data_full$GarageCond)), garage_features]) #159
kable(data_full[2127,garage_features])
#Get most commom values
kable(names(sapply(data_full[which( data_full$GarageCars == 1 & data_full$GarageType=="Detchd" ) ,garage_features], function(x) sort(table(x),  decreasing=TRUE)[1])), col.names = "Most Common Value per Feature") 
# Replace Values Manually
data_full[2127,'GarageQual']    = 'TA'
data_full[2127, 'GarageFinish'] = 'Unf'
data_full[2127, 'GarageCond']   = 'TA'
#Check changes
data_full[2127,garage_features]
```

_Likewise, we can see that row 2577 has no garage, so we can fill in garage type with none._

```{r Garage Features Missing Values2, eval=TRUE, echo=TRUE, collapse = TRUE}
data_full[2577,garage_features]
#Check and correct levels in Garage Type
levels(data_full$GarageType)
data_full$GarageType<-factor(data_full$GarageType, levels=c("None","2Types","Attchd",  "Basment","BuiltIn","CarPort","Detchd" ), ordered=FALSE)
#Update Garage type for row 2577
data_full[2577, 'GarageType'] = 'None'

```

_There is an error in GarageYrBlt, from the summary we see a year 2207. We will update to 2007._

```{r Garage Features Missing Values3, eval=TRUE, echo=TRUE, collapse = TRUE}
summary(data_full$GarageYrBlt)
subset(data_full[garage_features], GarageYrBlt >= 2011)
data_full$GarageYrBlt[data_full$GarageYrBlt==2207] <- 2007
summary(data_full$GarageYrBlt) 
```

_For the missing values in GarageYrBlt, we will fill in NAs with with the year the house was built._

```{r Garage Year Built Missing Values, eval=TRUE, echo=TRUE,collapse = TRUE, comment="***"}
cat("GarageYrBlt has missing values:", sum(is.na(data_full$GarageYrBlt)))
# Fill in year garage built in the same year when house was built. ***
data_full$GarageYrBlt[is.na(data_full$GarageYrBlt)]<-data_full$YearBuilt[is.na(data_full$GarageYrBlt)]
cat(" After inputing NA's with year YearBuilt values, GarageYrBlt has", sum(is.na(data_full$GarageYrBlt)), "missing values")

```
**Basement  Features**

_From viewing at all the basement columns we can see that basement condition is missing in rows 2041, 2186, 2525, and will replace with most the common feature for each column._

```{r BasementFeaturesNACorrected, eval=TRUE, echo=TRUE, collapse = TRUE}

basement_features <- names(data_full)[sapply(names(data_full), function(x) str_detect(x, "Bsmt"))]
#View(data_full[is.na(data_full$BsmtCond), basement_features]) 
data_full[c(2041, 2186,2525),basement_features]
#names(which.max(table(data_full$BsmtCond)))
data_full[c(2041,2186, 2525),'BsmtCond']=names(which.max(table(data_full$BsmtCond)))
#Check changes
data_full[c(2041, 2186,2525), basement_features]

```

**Pool Features**

_There are three pools where values for the 'quality' are missing, so we will fill in with the most common value based on the area of the pool._

```{r, PoolMissVals, echo=TRUE, eval=TRUE, collapse = TRUE}
pool_features <- names(data_full)[sapply(names(data_full), function(x) str_detect(x, "Pool"))]
data_full[c(2421,2504,2600),pool_features]
pool_na<-which(is.na(data_full$PoolQC) & data_full$PoolArea >0)
aggregate(data=data_full, PoolArea~PoolQC, mean, na.rm=TRUE)
data_full$PoolArea[which(is.na(data_full$PoolQC) & data_full$PoolArea >0)]
#Replace NA with most common values
data_full$PoolQC[data_full$Id == 2421] <- "Ex"
data_full$PoolQC[data_full$Id == 2504] <- "Ex"
data_full$PoolQC[data_full$Id == 2600] <- "Fa"
#Check changes
data_full[c(2421,2504,2600),pool_features]

```

**MasVnrType Features**

_There is a missing value for row ID=2611 in the MasVnrType column, we will replace with most common "MasVnrType"._ 

```{r, Mansory Features Missing Values, eval=TRUE, echo=TRUE, collapse = TRUE}
masonry_features <- names(data_full)[sapply(names(data_full), function(x) str_detect(x, "Mas"))]
mas_na<-which(is.na(data_full$MasVnrType) & data_full$MasVnrArea >0)
mas_na #ID=2611
data_full[2611,masonry_features]
data_full$MasVnrArea[which(is.na(data_full$MasVnrType) & data_full$MasVnrArea >0)]  #198
aggregate(data=data_full, MasVnrArea~MasVnrType, mean, na.rm=TRUE)
names(which.max(table(data_full$MasVnrType)))
summary(data_full$MasVnrType)  #most common is BrkFace
data_full$MasVnrType[data_full$Id == 2611] <- "BrkFace"
#Check Changes
data_full[2611,masonry_features]
```

#### Imputation with Mode for Categorical Features with a Few NAs
For some of the categorical features only missing a few values, we filled in the missing values with the most commonly occurring attribute value. Specially because for many of these the most frequent category was already over-represented, so it is pretty safe to assume that the missing values are more likely to be in the most common category. 

^[The graphs were produced with ggplot() function from package 'ggplot2' [@ggplot2].]

```{r plotcat, echo=FALSE, eval=TRUE, fig.cap="Categorical Features with Few NAs", fig.align = 'center'}

ut<-ggplot(data_full)+aes(Utilities)+ labs(title="Utilities")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))

func<-ggplot(data_full)+aes(Functional)+ labs(title="Functional")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))

elec<-ggplot(data_full)+aes(Electrical)+ labs(title="Electrical")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))

sale<-ggplot(data_full)+aes(SaleType)+ labs(title="SaleType")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))


kitch<-ggplot(data_full)+aes(KitchenQual)+ labs(title="KitchenQual")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))

ext1<-ggplot(data_full)+aes(Exterior1st)+ labs(title="Exterior1st")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))

ext2<-ggplot(data_full)+aes(Exterior2nd)+ labs(title="Exterior2nd")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))


grid.arrange(ut, func, elec, sale,
             ncol = 2, nrow = 2)

#grid.arrange(kitch, ext1, ext2,ncol = 2, nrow = 2)

```

```{r, HPFewMiss, eval=TRUE, echo=FALSE}
Few_Missing<-c("Utilities", "Functional", "Exterior1st", "Exterior2nd", "Electrical", "KitchenQual", "SaleType")
Few_Missing<-tibble("Feature"=Few_Missing, "Method"=rep("Mode", length(Few_Missing)))
kable(Few_Missing, caption = "Few NAs")

```


```{r CatMode, eval=TRUE}
#Replace with most common value since these are missing very few values: 
na_m <- c( "Utilities", "Functional", "Exterior1st", "Exterior2nd", "Electrical", "KitchenQual", "SaleType")
data_full[,na_m] <- apply(data_full[,na_m], 2,
                         function(x) {replace(x, is.na(x), names(which.max(table(x))))})

```

#### Imputation with "None" for Categorical Features 

According to the data description file [@de2011ames] for the categorical features listed in MissValsSumm, NA means that the feature is not present in the house. For example, the house doesn't have a garage. For these features the NA values were replaced with "None". 

```{r HPCat, eval=TRUE, echo=FALSE}
Categorical <- c("GarageFinish", "GarageQual", "GarageType", "GarageCond", "BsmtCond", "BsmtExposure", "BsmtQual", "BsmtFinType1", "BsmtFinType2", "FireplaceQu", "EnclosedPorch","MiscFeature","Alley", "Fence", "MasVnrType")
Categorical<-tibble("Feature"=Categorical, "Method"=rep("None", length(Categorical)))
kable(Categorical, caption = "Categorical Features where NA means 'None'")
```


```{r, Categorical Features Missing Values, eval=TRUE, echo=TRUE, collapse = TRUE}
na_none <- c("GarageFinish", "GarageQual", "GarageType", "GarageCond", "BsmtCond", "BsmtExposure", "BsmtQual", "BsmtFinType1", "BsmtFinType2", "FireplaceQu", "EnclosedPorch","PoolQC","MiscFeature","Alley", "Fence", "MasVnrType")

data_full[,na_none] <- apply(data_full[,na_none], 2,
                          function(x) {replace(x, is.na(x), "None")})

```
#### Imputation with Zero (0) for Numerical Features where NA means Feature is not Present
According to the data description file [@de2011ames] for the numerical features listed in table \@ref(tab:HPCont).  NA means that the feature is not present in the house, so we replaced NA with the number zero for these features. 
```{r HPCont, eval=TRUE, echo=TRUE, collapse = TRUE}
Continous <- c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath","GarageCars","GarageArea", "MasVnrArea")
Continous <- tibble("Feature"=Continous, "Method"=rep("0",length(Continous)))
kable(Continous, caption = "NA means '0'")
```


```{r, NumMissVals, eval=TRUE, echo=TRUE, collapse = TRUE}
na_z <- c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath","GarageCars","GarageArea", "MasVnrArea")
data_full[,na_z] <- apply(data_full[,na_z], 2,
                          function(x) {replace(x, is.na(x), 0)})
```


#### Imputation with estimation for Features with Large Number of Missing Values
Lastly, we were left with two features as seen in \@ref(fig:RMissVals). LotFrontage, which captures the linear feet of street-connected to the property; and MSZoning, which indicates the  different zoning classifications ranging from agricultural to residential. Because these features had a large percentage of missing values, it might be better to estimate their value based on other attributes. I used the mice package[@mice], which uses a Random Forest algorithm to estimate the missing values.


```{r RMissVals,eval=TRUE, fig.cap= "Remaining Missing Values", fig.align = 'center', echo=TRUE}
cols_missing_values2<-data_full[which(colSums(is.na(data_full))>0)]
plot_missing(cols_missing_values2)
```
^[The graph was produced with function plot_missing() from package 'DataExplorer' [@plotmissing].]

```{r ManyMiss,eval=TRUE, echo=FALSE}
Many_Missing<-c("LotFrontage", "MSZoning")
Many_Missing <- tibble("Feature"=Many_Missing, "Method"=rep("Prediction",length(Many_Missing)));
kable(Many_Missing,caption = "Large Number of Missing Values")

```

```{r MissValsMany, echo=TRUE, collapse = TRUE}
#Impute LotFrontage (This takes a long time to run)
library(mice)
set.seed(123)
mice_rf_mod<- mice(data_full[, !names(data_full) %in% c('Id', 'SalePrice')], method ='rf', printFlag = FALSE)
mice_output <- complete(mice_rf_mod)
#Inpute LotFrontage
sum(is.na(data_full$LotFrontage))
data_full$LotFrontage[is.na(data_full$LotFrontage)] <- mice_output$LotFrontage[is.na(data_full$LotFrontage)]
sum(is.na(data_full$LotFrontage))
#Inpute MSZoning
set.seed(123)
sum(is.na(data_full$MSZoning))
data_full$MSZoning[is.na(data_full$MSZoning)] <- mice_output$MSZoning[is.na(data_full$MSZoning)]
sum(is.na(data_full$MSZoning))
```
**After imputing values for all NA's, confirm all  missing values are now cleared.**

```{r MissValsFinal,eval=TRUE, fig.cap= "Final Missing Values HeatMap", echo=TRUE, fig.align = 'center', collapse = TRUE}
sum(is.na(data_full))           
missmap(data_full,col = c("red","gray"), main ="Heatmap showing Missing values")
```

## Correct Data Types
The next step in pre-processing, after all the NA values have been cleared, is to identify and correct data type inconsistencies.  The raw data shows that there are 43 factors, 37 integers, and 1 numeric data types. 

```{r data_type_raw, eval=TRUE,collapse = TRUE, echo=FALSE}
table(sapply(train_raw, class))    
table(sapply(test_raw, class))
#table(sapply(data_full, class))
```

**Our current dataset looks like this:**

```{r data_type_combined, eval=TRUE,collapse = TRUE, echo=FALSE}

Integers <- which(lapply(data_full,class) == "integer")     # Integer features
cat("Integer Features (int):", names(Integers))

Factors <- which(lapply(data_full, class) == "factor")    # Categorical features
cat("Categorical Features:", names(Factors))

Characters <- which(lapply(data_full, class) == c("character"))    # Character features
cat("Character features:", names(Characters))

Numeric <- which(lapply(data_full, class) == "numeric")     # Target feature: numeric
cat("Numerical Features(num):", names(Numeric))

```

However, the documentation on the data [@de2011ames] says that the data-set consist of 20 continuous features that refer to area dimensions, 14 discrete features that quantify the number of items in the house, 23 nominal categorical features that refer to types of dwellings, materials and conditions, and 23 ordinal categorical features that rate various property related items. After correcting the data types according to the documentstion, it should look as  shown in table \@ref(tab:datatypes). 

```{r datatypes, eval=TRUE, echo=FALSE}

#How the data should be :
Cont_Features_20<- c("LotFrontage", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "GarageArea", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "X3SsnPorch", "PoolArea", "WoodDeckSF", "SalePrice" ,"EnclosedPorch", "ScreenPorch", "LotArea",  "MiscVal", "OpenPorchSF", " ", " ", " ")

Disc_Features_14<- c("BsmtFullBath","BsmtHalfBath","FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars","GarageYrBlt","YearBuilt","YearRemodAdd", "MoSold", "YrSold", " "," ", " ", " ",  " ", " ", " ", "", "")

Nom_Categorical_23<- c("Neighborhood", "SaleCondition", "HouseStyle", "Street", "Alley", "CentralAir", "LandContour", "Condition1", "Condition2", "BldgType", "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd", "Foundation", "Heating", "GarageType", "MiscFeature", "SaleType", "MSSubClass","MSZoning", "MasVnrType", "LotConfig")
  
Ord_Categorical_23<- c("Utilities", "LandSlope", "ExterQual", "ExterCond","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2",
"HeatingQC","Electrical","KitchenQual","Functional","FireplaceQu", "GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","OverallQual","OverallCond", "LotShape" )

data_types<-data.frame(Cont_Features_20,Disc_Features_14,Nom_Categorical_23,Ord_Categorical_23)
kable(data_types,caption = "How the Data Should Look")

```
**The first step is to convert the numeric features to the appropriate data type.**
```{r,eval=TRUE,collapse = TRUE}
to_num <-c( "LotFrontage", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "GarageArea","X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "X3SsnPorch", "PoolArea", "WoodDeckSF", "SalePrice" ,"EnclosedPorch", "ScreenPorch", "LotArea",  "MiscVal", "OpenPorchSF")
data_full[,to_num] <- lapply(data_full[,to_num], as.numeric)
to_int<-c("BsmtFullBath","BsmtHalfBath","FullBath", "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageCars", "MoSold", "YrSold","YearRemodAdd","GarageYrBlt","YearBuilt")
data_full[,to_int] <- lapply(data_full[,to_int], as.integer)
```
**The second step, is to convert missclassified numeric features to categorical.**
```{r, eval=TRUE, collapse = TRUE}
nom_to_cat <-c("MasVnrType","MSSubClass", "Neighborhood","CentralAir", "SaleCondition", "HouseStyle", "Street", "Alley", "LandContour", "Condition1", "Condition2", "BldgType", "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd", "Foundation", "BsmtExposure", "Heating", "GarageType", "PavedDrive", "MiscFeature", "SaleType")
data_full[,nom_to_cat] <- lapply(data_full[,nom_to_cat], factor)
```
**The third step is to add levels to ordinal categorical features.**
```{r, eval=TRUE, collapse = TRUE}
data_full$Utilities<-factor(data_full$Utilities,levels=c("ELO","NoSeWa","NoSewr","AllPub"), ordered=TRUE)

data_full$ExterQual<-factor(data_full$ExterQual, levels=c("Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$ExterCond<-factor(data_full$ExterCond, levels=c("Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$FireplaceQu<-factor(data_full$FireplaceQu, levels=c("None","Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$Functional<-factor(data_full$Functional,levels=c("Sal", "Sev", "Maj2", "Maj1","Mod","Min2","Min1","Typ"), ordered=TRUE)

data_full$PoolQC<-factor(data_full$PoolQC,levels=c("None","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$BsmtCond<-factor(data_full$BsmtCond, levels=c("None","Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$BsmtQual<-factor(data_full$BsmtQual,levels=c("None","Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$BsmtExposure<-factor(data_full$BsmtExposure, levels=c("None", "No", "Mn", "Av", "Gd"), ordered=TRUE)

data_full$BsmtFinType1<-factor(data_full$BsmtFinType1, levels=c("None","Unf","LwQ","Rec","BLQ","ALQ","GLQ"), ordered=TRUE)

data_full$BsmtFinType2<-factor(data_full$BsmtFinType2, levels=c("None","Unf","LwQ","Rec","BLQ","ALQ","GLQ"), ordered=TRUE)

data_full$HeatingQC<-factor(data_full$HeatingQC, levels=c("Po", "Fa", "TA", "Gd", "Ex"), ordered=TRUE)

data_full$KitchenQual<-factor(data_full$KitchenQual, levels=c("Po", "Fa", "TA", "Gd", "Ex"), ordered=TRUE)

data_full$GarageQual<-factor(data_full$GarageQual, levels=c("None","Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$GarageCond<-factor(data_full$GarageCond, levels=c("None","Po","Fa","TA","Gd","Ex"), ordered=TRUE)

data_full$Electrical<-factor(data_full$Electrical, levels=c("Mix","FuseP","FuseF","FuseA","SBrkr"), ordered=TRUE)

data_full$GarageFinish<-factor(data_full$GarageFinish, levels=c("None","Unf","RFn","Fin"), ordered=TRUE)

data_full$PavedDrive<-factor(data_full$PavedDrive, levels=c("N","P","Y"), ordered=TRUE)

data_full$Fence<-factor(data_full$Fence, levels=c("None","MnWw","GdWo", "MnPrv", "GdPrv"), ordered=TRUE)

data_full$OverallQual<-factor(data_full$OverallQual, levels=c("1", "2","3","4", "5", "6", "7", "8", "9", "10"), ordered=TRUE)

data_full$OverallCond<-factor(data_full$OverallCond, levels=c("1", "2","3","4", "5", "6", "7", "8", "9", "10"), ordered=TRUE)

data_full$GarageType<-factor(data_full$GarageType, levels=c("None","2Types","Attchd", "Basment","BuiltIn","CarPort","Detchd" ), ordered=FALSE)

data_full$LandSlope<-factor(data_full$LandSlope, levels=c("Gtl","Mod","Sev" ), ordered=TRUE)

data_full$LotShape<-factor(data_full$LotShape, levels=c("IR1","IR2", "IR3", "Reg" ), ordered=TRUE)

```

**Confirm all features are now approrpiately classified by their data type.**

```{r data_type_corrected, echo=FALSE, eval=TRUE, collapse = TRUE,comment="***" }

integer_var <- which(lapply(data_full,class) == "integer")     # Integer features
cat(length(integer_var),"Discrete Features:", names(integer_var))

numeric_var <- which(lapply(data_full, class) == "numeric")     # Target feature: numeric
cat(length(numeric_var),"Continous Feature:", names(numeric_var ))

factor_var <- which(lapply(data_full, class) == "factor")    # Categorical features
cat(length(factor_var), "Nominal Categorical Features:", names(factor_var))

factor_var2 <- which(sapply(data_full, is.ordered))  # Categorical features
cat(length(factor_var2),"Ordered Categorical Features:", names(factor_var2))

char_var <- which(lapply(data_full, class) == "character") # Zero character types 

```

## Feature Transformation

With clean data in the correct form, it is time to start the Exploratory Data Analysis (EDA). By visualizing the data we hope to uncover interesting patterns and relationships which we could use to enhance the predictive value of the features via transformation or new feature creation. 

 
### Visualizing the Distribution and Spread of Target Feature: SalesPrice 

The histogram of the "SalesPrice" feature in figure \@ref(fig:SalePriceHistogram) shows a few very large values on the right, making the distribution of the data right skewed. Since the goal is to predict the continuous numerical variable "SalesPrice" with regression models, it might be useful to transform it, since one of the assumptions of regression  analysis is that the error between the observed and expected values (the residuals) should be normally distributed, and violations of this assumption often stem from a skewed response variable. We can make the distribution more normal by taking the natural logarithm, since in a right-skewed distribution where there are a few very large values, the log transformation helps bring these values into the center. After applying the log transformation, as seen in \@ref(fig:SalePriceHistogramLog), the distribution looks more symmetrical.
```{r SalePriceHistogram, echo=TRUE, collapse = TRUE, fig.cap= "Target Feature: SalesPrice", echo=TRUE, fig.align = 'center'}

#Create a copy of the whole dataset to work from here on out
data_2<-data_full  
#Create a set that includes only the training examples
data_2_train<-data_2[1:1460,]
par(mfrow=c(1,1))
ggplot(data_2_train, aes(SalePrice)) +
geom_histogram(fill="#053061", alpha=.5, color="#F5F5F5" , bins = 30)+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE))+
        geom_vline(aes(xintercept = mean(SalePrice), 
                   color = "mean"), linetype = "dashed", size = .7) +
        geom_vline(aes(xintercept = median(SalePrice), 
                   color = "median"), linetype = "dashed", size = .7) +
        scale_color_manual(name = "Central Tendency", values = c(mean = "#67001F", median = "#01665E"))
```


```{r SalePriceHistogramLog, echo=TRUE, collapse = TRUE, fig.cap= "Log-Transformed SalesPrice", echo=TRUE, fig.align = 'center'}
#Log transform the Target Feature
data_2_log<-data_2_train
data_2_log$SalePrice<-log(data_2_log$SalePrice)

ggplot(data_2_log, aes((SalePrice))) + 
geom_histogram(fill="#053061", alpha=.5, color="#F5F5F5", bins = 30)+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
        geom_vline(aes(xintercept = mean(SalePrice), 
                   color = "mean"), linetype = "dashed", size = .7) +
        geom_vline(aes(xintercept = median(SalePrice), 
                   color = "median"), linetype = "dashed", size = .7) +
        scale_color_manual(name = "Central Tendency", values = c(mean = "#67001F", median = "#01665E"))

```

### Features Highly Correlated with Sales Price 

Given the large number of features in the data-set, we will focus our data engineering efforts on those features which are highly correlated with our variable of interest. 

For continuous features, the correlation plot shows Garage Area, Great Living Room Area, First Floor SF, Total Basement SF, and Masonry Veneer Area have a strong positive correlation with Sale Price. 

```{r contCorr,echo=FALSE, collapse = TRUE, fig.cap= "Continous Features and Sale Price Correlation Plot", fig.align = 'center'}
ggcorr(data_2_train[,numeric_var], label = TRUE, label_size = 2.9, hjust = 1, layout.exp = 2)
```
^[The correlation plot was produced with the ggcorr() function from package 'GGally' [@GGally].]

From the histograms we can see that many of the continuous independent variables are right skewed, similar to the response, so it might be a good idea to normalize these prior to modeling, since many algorithms perform better with normalized data because it improves the numerical stability of the model and reduces training time (Zhang_2019). 


```{r, ContinousFeatures2, collapse = TRUE, echo=FALSE, fig.cap= "Continous Features Distribution Plots", fig.align = 'center'}
#Garage Area
gar_a<-ggplot(data_2_train, aes(x=GarageArea)) + ggtitle("Garage Area")+ geom_histogram(aes(y=..density..), bins = 30, colour="#F5F5F5", fill="#053061", alpha=.5) + geom_density(adjust = 5, colour= "#053061") +  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE))+
        geom_vline(aes(xintercept = mean(GarageArea), 
                   color = "mean"), linetype = "dashed", size = .7) +
        geom_vline(aes(xintercept = median(GarageArea), 
                   color = "median"), linetype = "dashed", size = .7) +
        scale_color_manual(name = "Central Tendency", values = c(mean = "#B2182B" , median = "#01665E"))

# Living Area
liv_a<-ggplot(data_2_train, aes(x=GrLivArea)) + ggtitle("Living Room Area")+ geom_histogram(aes(y=..density..), bins = 30, colour="#F5F5F5", fill="#053061", alpha=.5) + geom_density(adjust = 5, colour= "#053061") 

#First Floor SF
first_f<-ggplot(data_2_train, aes(x=X1stFlrSF)) + ggtitle("First Floor SF")+ geom_histogram(aes(y=..density..), bins = 30, colour="#F5F5F5", fill="#053061", alpha=.5) + geom_density(adjust = 5, colour= "#053061") 

#Total Basement SF
tot_bas<-ggplot(data_2_train, aes(x=TotalBsmtSF)) + ggtitle("Total Basement SF")+ geom_histogram(aes(y=..density..), bins = 30, colour="#F5F5F5", fill="#053061", alpha=.5) + geom_density(adjust = 5, colour= "#053061") 

#and Masonry Veneer Area
mans_a<-ggplot(data_2_train, aes(x=MasVnrArea)) + ggtitle("MasVnrArea")+ geom_histogram(aes(y=..density..), bins = 30, colour="#F5F5F5", fill="#053061", alpha=.5) + geom_density( colour= "#053061")

grid.arrange(mans_a, liv_a, first_f, tot_bas, 
             ncol = 2, nrow = 2)

```
^[The histograms were produced with ggplot() function from package 'ggplot2' [@ggplot2].]

The correlation plot for the discrete features in \@ref(fig:discCorr) shows that YearBuilt, YearRemodAdd, GarageYearBuilt, GarageCars, FullBath, FirePlaces, and TotRmsAbvGrd are strongly and positively correlated with SalePrice: 

```{r discCorr,echo=FALSE, collapse = TRUE, fig.cap= "Discrete Features and Sale Price Correlation Plot", fig.align = 'center'}
ggcorr(data_2_train[,c(integer_var[-1], 81)], label = TRUE, label_size = 2.9, hjust = 1, layout.exp = 2)
```
^[The correlation plot was produced with the ggcorr() function from package 'GGally' [@GGally].]

### Attribute Construction from Year Features 

The barplot of Year Built shows a clear distinction between the number of houses built and sold in the 2000s vs the number built and sold in before that time. The distribution is left skewed, as we can see that the mean of the distribution is less than the median. Most importantly, since there are so many unique years, it might be helpful to create a new “age” variable by subtracting the year built from 2010, which is the upper limit of year sold in the data-set. And we can further refine this new age variable later on by binning age groups into buckets from old to new, thereby reducing the number of levels.

```{r, YearBuilt, collapse = TRUE, echo=FALSE, fig.cap= "Year Built vs Sales Price", fig.align = 'center'}
ggplot(data_2_train, aes(YearBuilt))+ 
  geom_bar(fill="#053061", color="#F5F5F5")+
        geom_vline(aes(xintercept = mean(YearBuilt), 
                   color = "mean"), linetype = "dashed", size = .7) +
        geom_vline(aes(xintercept = median(YearBuilt), 
                   color = "median"), linetype = "dashed", size = .7) +
        scale_color_manual(name = "Statistics", values = c(mean = "red", median = "darkgreen"))+
        xlab("Year Built") + ylab("Number of Homes")

```
^[The bar-plot was produced with ggplot() function from package 'ggplot2' [@ggplot2].]

Curiously, in the scatter plot shown in \@ref(fig:YearRemodeled), where the dots have been colored in light blue by the YearRemodAdd feature,  newer houses are displaying as having been remodeled, apparently if the house has not been remodeled the year remodeled defaults to the year built. To improve the value of this metric we can create a new binary feature for Remodeled: Yes or No. Also, it might be useful to create a feature to show how recently the house was remodeled, and bin these into categories from most recently remodeled. 

```{r YearRemodeled, collapse = TRUE, fig.cap= "Year Bult vs Sales Prie Colored by Year Remodeled", fig.align = 'center'}
ggplot(data_2_train, aes(x=YearBuilt, y=SalePrice, color=(YearRemodAdd))) + geom_jitter(height = 1)
```

^[The scatter plot was produced with ggplot() function from package 'ggplot2' [@ggplot2].]

In order to make the year features more meaningful, we created new attributes as indicated in the table below. In addition to the building age and  time since remodeled features, we added a feature to indicate if the home is a new build, since it is likely that being a new home will have an impact on the sale's price. We also added a feature for the year the house was sold, since macro economic events, like the economic depression in 2008, could also impact the sale price. Later, we simplify some of these newly created features by binning them into categories with fewer levels. Since all of the categorical features will have to be converted into numeric via dummy coding for modeling, having fewer levels will help with model performance and dimensionality reduction.

**Attribute Construction from Year Features**

| New Feature    | Method                                                    |
| :------------- | :-------------------------------------------------------- |
| BldgAge        | 2010 - YearBuilt                                          |
| NewBuild       | If YearBuilt or YearBuilt + 1 =  YrSold then NewBuild = 1 |
| Remod          | If YearBuilt = YearRemodAdd then Remod = 0                |
| TimeSinceRemod | 2010 - YearRemodAdd                                       |
| LastSold       | 2010 - YrSold                                             |

```{r, NewYearFeatures, collapse = TRUE}
# Remodeled
data_2['Remod'] <- ifelse(data_2$YearBuilt==data_2$YearRemodAdd, 0, 1) #0=No Remodeling, 1=Remodeling
#New Build 
data_2['NewBuild'] <-  ifelse(data_2$YearBuilt == data_2$YrSold|data_2$YearBuilt+1 == data_2$YrSold, 1,0) 
# Age of property based on last year of dataset
data_2['BldgAge']<- max(data_2$YearBuilt) - data_2$YearBuilt
head(data_2[,c("YearBuilt","YrSold", "BldgAge","YearRemodAdd", "Remod", "NewBuild")])
```

### Reducing Levels by Grouping Unrepresented Categories
Another way that we can improve the data for mining is to reduce the levels in categorical variables by merging under-represented categories. For example, in the case of the fireplaces, we can see in scatterplot \@ref(fig:fireplacesScatter)  that there are only a few houses that have 3 fireplaces, and the scatter plot shows that the relationship between three fireplaces and price is not much different than between two fireplaces and price, so we can merge these into one level. 


```{r fireplacesScatter, collapse = TRUE, fig.cap= "Fireplaces Scatter Plot", fig.align = 'center'}
ggplot(data_2_train, aes(x=Fireplaces, y=SalePrice, color=(Fireplaces))) + geom_jitter(height = 1) 

```

**Grouping Fireplace Features**

```{r GrouppingFireplaces, collapse = TRUE, fig.cap= "Fireplaces After Groupping", fig.align = 'center'}
#FirePlaces 
data_2$Fireplaces[data_2$Fireplaces==3]<-2
data_2$Fireplaces[data_2$Fireplaces==4]<-2
ggplot(data_2[1:1460,], aes(x=as.factor(Fireplaces), y=SalePrice, color=Fireplaces)) + geom_jitter(height = 1) 
```

Similarly, the scatter plot in \@ref(fig:garagecarsScatter) of the "GarageCars" variable shows that few houses have more than three garages, so we could simplify the levels in this feature by keeping only three levels: 1,2, and 3+. 

```{r garagecarsScatter, collapse = TRUE, fig.cap= "Discrete Features Scatter Plots", fig.align = 'center'}
ggplot(data_2_train, aes(x=GarageCars, y=SalePrice, color=(GarageCars))) + geom_jitter(height = 1) + labs(title="Garage vs Sale Price", subtitle="Training Data")
```


```{r GrouppingGaragecars, collapse = TRUE, fig.cap= "Garage Cars After Groupping", fig.align = 'center'}
data_2$GarageCars[data_2$GarageCars==4]<-3
data_2$GarageCars[data_2$GarageCars==5]<-3
ggplot(data_2[1:1460,], aes(x=GarageCars, y=SalePrice, color=(GarageCars))) + geom_jitter(height = 1) + labs(title="GarageCars vs Sale Price", subtitle="Training Data")
```

^[The plots were produced with ggplot() function from package 'ggplot2' [@ggplot2].]

We applied this treatment to the features listed in the table below. This will make these features more robust for modeling. 

**Reducing Levels by Grouping**

| New Feature | Method                             |
| :-----------| :--------------------------------- |
| GarageCars  | Merge 4 and 3                      |
| Fireplaces  | Merge 3 and  2                     |
| Electrical  | Merge FuseF and FuseA              |
| Function    | Merge Maj1 and Mod, Min1 and Min2  |
                                

**Grouping Electrical Features**

```{r GrouppingElectrical , collapse = TRUE, fig.cap= "Electrical vs Sales Price Scatterplot", fig.align = 'center'}
data_full$Electrical<-factor(data_full$Electrical, levels=c("Mix","FuseP","FuseF","FuseA","SBrkr"), ordered=TRUE)
data_2$Electrical<-recode_factor(data_2$Electrical, "Mix"=1, "FuseP"=2, "FuseF"=3, "FuseA"=3, "SBrkr"=4)
ggplot(data_2[1:1460,], aes(x=Electrical, y=SalePrice, color=Electrical)) + geom_jitter(height = 1)
```

**Grouping Functional Features** 

```{r GrouppingFunctional, collapse = TRUE, fig.cap= "Functional vs Sales Price Scatterplot",fig.align = 'center'}
data_2$Functional<-recode_factor(data_2$Functional,  "Sal"=1 , "Sev"=1 , "Maj2"=2 ,"Maj1"=3 ,"Mod"=3,  "Min2"=4 ,"Min1"=4 ,"Typ"=5)
ggplot(data_2[1:1460,], aes(x=Functional, y=SalePrice, color=(Functional))) + geom_jitter(height = 1) 
```

### Attribute Construction by Combining some Features to make New Features. 
There are four different columns related to the number of bathrooms in different areas of the home.  Individually these features may not carry as mush weight as combined into a single feature for total number baths. 

```{r BathroomFeatures,  collapse = TRUE, fig.cap= "Bath Features", fig.align = 'center'}
bsmtFB<-ggplot(data_2_train)+aes(BsmtFullBath)+ labs(title="Basement Full Bath", subtitle="Training Data")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 

BsmtHB<-ggplot(data_2_train)+aes(BsmtHalfBath)+ labs(title="Basement Half Bath", subtitle="Training Data")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 

hFB<-ggplot(data_2_train)+aes(FullBath)+ labs(title="Number of Full Baths in the Home", subtitle="Training Data")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

hhB<-ggplot(data_2_train)+aes(HalfBath)+ labs(title="Number of Half Baths in the Home", subtitle="Training Data")+
geom_bar(fill="#053061", color="#F5F5F5")+  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 

grid.arrange(bsmtFB, BsmtHB, hFB, hhB, 
             ncol = 2, nrow = 2)
```

^[The plots were produced with ggplot() function from package 'ggplot2' [@ggplot2].]


**Close look at fullbath = 0 **
Some homes are showing as having zero baths, this might be a mistake, so it will be usefull to look closer are these examples. The nine rows which had zero full bath, did have other bathrooms in either the basement, or half bath, it is ok to leave them for now, since it will be better to combine all baths together and have one feature for the total baths. 

```{rBathroomZero,  collapse = TRUE}
missing_bath <- filter(data_2_train, FullBath ==0)
bath_features <- names(data_2_train)[sapply(names(data_full), function(x) str_detect(x, "Bath"))]

data_2_train[c(54, 189, 376, 598, 635, 917, 1164,1214, 1271),c("BsmtFullBath","BsmtHalfBath","FullBath","HalfBath")]

```

We will also combine the basement finished squared feet and the porch features. Furthermore, we created a new feature for total area by adding the living area square feet and the total basement square feet.  The goal here is to create new features that might have a stronger predictive power than the individual original features. Prior to modeling, we will be removing all of the features rendered redundant by the feature creation process. The table below summarizes the features we constructed by adding the same type of feature together to form a total.

**New Feature Construction by Combining Existing Features**

| New Feature   | Method                                                  |
| :------------ | :------------------------------------------------------ |
| BSmtFinSFComb | BsmtFinSF1 + BsmtFinSF2                                 |
| TotalArea     | GrLivArea + TotalBsmtSF                                 |
| TotalBaths    | FullBath + HalfBath + BsmtFullBath + BsmtHalfBath       |
| TotalPorchSF  | OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch  |
            

```{r CombineFeatures, collapse = TRUE}
data_2 <- mutate(data_2, BSmtFinSFComb = BsmtFinSF1 + BsmtFinSF2,
                   TotalArea= GrLivArea + TotalBsmtSF,
                   TotalBaths = BsmtFullBath + (0.5*BsmtHalfBath) + FullBath+ (0.5*HalfBath),
                   PorchSF = OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch)
#To view cobined fatures
#data_2[,c("X1stFlrSF","X2ndFlrSF", "BSmtFinSFComb", "TotalBsmtSF", "GrLivArea", "TotalArea")]

```

### Attribute Construction by Binning
Some of the categorical variables have a large number of levels. For example, "Neighborhood",  one of the features most highly correlated with sale price, has 25 different levels. 

```{r Neighborhoodplot, collapse = TRUE, fig.cap= "Neighborhood vs Sale Price", fig.align = 'center'}

ggplot(data_2_train)+ 
  geom_boxplot(mapping=aes(x=reorder(Neighborhood, SalePrice, FUN=median), y=SalePrice), fill = "#053061", color="#053061", alpha=.3,)+theme(axis.text.x = element_text(angle = 45,hjust = 1))

```

**Descretization and Binning-Neighboorhood**
Concept hirearchy discretization and binning is a way to reduce the the number of distinc values per attribute, we will group the neighborhoods together to create fewer categories.  We can reference the  SalePrice to find similar neighboorhoods. 

```{r NighborhoodsSummary, collapse = TRUE}
# Summary statistics
ames_train<-as.data.frame(data_2[1:1460,])
log_price<-log(ames_train$SalePrice)
neigh_brk<-setDT(ames_train)[ , list(mean_1 = mean(SalePrice), median_1 = median(SalePrice), st_dev = sd(SalePrice),  sum_gr = sum(SalePrice), range=range(SalePrice)[2] - range(SalePrice)[1]) , by = .(Neighborhood)]

```

```{r top3Neighborhoods, collapse = TRUE}
# Sort by median and show top, bottom
top_3<-kable(neigh_brk %>%
        arrange(desc(median_1)) %>%
        head(3), caption = "Most Expensive Neighborhoods")

```

```{r bottom3Neighborhoods, collapse = TRUE}
bottom_3<-kable(neigh_brk %>%
        arrange(desc(median_1)) %>%
        tail(3), caption = "Least Expensive Neighborhoods")
```

```{r median, collapse = TRUE, echo=FALSE }
kable(neigh_brk %>%arrange(desc(median_1)), caption = "All Neighborhoods by Median Sales Price")
```

```{r NeighHet, collapse = TRUE, echo=FALSE}
kable(neigh_brk %>%arrange(desc(st_dev)) %>%tail(6), caption = "The Most Heterogeneous Neighborhoods")
```
**Grouping Neighborhoods**
```{r NighborhoodRidges, collapse = TRUE, fig.cap= "Spread Based on Sale Price ", fig.align = 'center', echo=FALSE}

data_2[1:1460,] %>%
        ggplot(aes(x = SalePrice, y = Neighborhood, fill = Neighborhood)) +
        geom_density_ridges() +
        theme_ridges() +
        theme(legend.position = "none") +
        xlab("Sale Price") +
        theme(axis.title.y = element_blank())

data_2$Neigh_Cat<-recode_factor(data_2$Neighborhood, 'MeadowV' = 1, 'IDOTRR' = 1, 'BrDale' = 1, 'BrkSide' = 2, 'Edwards' = 2,'OldTown' = 2,'Sawyer' = 2, 'Blueste' = 2, 'SWISU' = 2,  'NPkVill' = 2, 'NAmes' = 2, 'Mitchel' = 2,'SawyerW' = 3,'NWAmes' = 3,  'Gilbert' = 3, 'Blmngtn' = 3, 'CollgCr' = 3, 'Crawfor' = 4, 'ClearCr' = 4,'Somerst' = 4, 'Veenker' = 4, 'Timber' = 4, 'StoneBr' = 5, 'NoRidge' = 5, 'NridgHt' = 5)

data_2$Neigh_Cat <- as.numeric(data_2$Neigh_Cat)

```

In order to avoid data leakage we could use the “Overall Quality” feature, which shows a similar relationship with the neighborhoods, to create 5 bins for neighborhood categories ranging from low (1) to high (5). As we can see below grouping neighborhoods into 5 categorires looks similar whether we look at it by the “Overall Quality” feature or by SalePrice. 

```{r NighborhoodGroupingQ,  collapse = TRUE, fig.cap= "Neighborhood Category by Overal Quality", fig.align = 'center', echo=FALSE}

ggplot(data_2, aes(x=Neigh_Cat, y=OverallQual, color=(Neighborhood))) + geom_point() + geom_jitter(height = 1.5) + theme(axis.text.x = element_text(angle = 45,hjust = 1))

```


```{r NighborhoodGroupingSP,  collapse = TRUE, fig.cap= "Neighborhood Category by SalePrice", fig.align = 'center', echo=FALSE}

ggplot(data_2[1:1460,], aes(x=Neigh_Cat, y=SalePrice, color=(Neighborhood))) + geom_point(alpha=0.2) + geom_jitter(height = 1.5) + labs(title="Sale Price and Neighborhood Category") +theme(axis.text.x = element_text(angle = 45,hjust = 1))

```
^[The plots were produced with ggplot() function from package 'ggplot2' [@ggplot2].]

In addition to the "Neighborhood" feature, we also used binning as a means to reduce the number of unique levels for the "Month" feature.  For the Month feature, we ploted the months, and put them into bins according to low, medium, or high season, based on the number of houses sold. The AgeCat and RemodelFromCat are new features based of other features we contructed previously from the year features. These features contain fewer levels, which is one of our goals here, to reduce the nuber of levels in categorical features, so that the prediction model is simpler, which as mentioned before reduces the error. The table below shows the attributes constructed by binning.

**Attribute Construction by Binning**

| New Feature    | Method                                                    |
| :------------- | :-------------------------------------------------------- |
| AgeCat         | BldgAge into 4 age categories: Antique, Old, Mid, New     |
| RemodelFromCat | Max year in Data (2010) - YearRemodAdd into 4 categories  |
| SeasonSale     | MoSold into 3 categories (Low, Mid & High)                |
| NeighCat       | Neighborhood into 5 categories                            | 
            

**Bin the age of the house into 4 categories** 

```{r BinningAge,collapse = TRUE}
min(data_2$BldgAge) 
max(data_2$BldgAge) 

head(data_2[,c("BldgAge","YearBuilt")])

age<-data_2$BldgAge
AgeCat<- case_when(age<= 9 ~ 'New',
                  between(age, 10, 40) ~ 'Mid',
                  between(age, 41, 70) ~ 'Old',
                  age >= 71 ~ 'Antique')

data_2$AgeCat<-as.numeric(factor(AgeCat, levels=c('Antique','Old', 'Mid', 'New'), ordered=TRUE))
head(data_2[,c("BldgAge", "AgeCat","YearBuilt")])

#Time when property sold  based on max year of dataset
data_2$LastSold <- 2010 - data_2$YrSold  #This might be interesting to look at from economic boom and bust effects
```

**Bin the year-since-remodeled of the house into 4 categories** 

```{r BinningRemodeled, collapse = TRUE}
##Years Since Remodeled 
data_2['YearRemodAdd2'] <- ifelse(data_2$YearBuilt == data_2$YearRemodAdd, 0, data_2$YearRemodAdd) #fix the year remodeled column
head(data_2[,c("YearBuilt","YearRemodAdd", 'YearRemodAdd2','Remod' )])
data_2['TimeSinceRemod'] <- ifelse(data_2$Remod == 1, 2010 - data_2$YearRemodAdd2,0) 
data_2$RemodelFromCat<- case_when(data_2$TimeSinceRemod <= 0 & data_2$NewBuild == 1 ~ 'New',
                  data_2$TimeSinceRemod <= 0 & data_2$YearRemodAdd2 == 0 ~ 'None',
                  between(data_2$TimeSinceRemod, 0, 5) ~ 'Recent',
                  between(data_2$TimeSinceRemod, 6, 10) ~ 'Mid',
                  between(data_2$TimeSinceRemod, 11, 20) ~ 'Old',
                  data_2$TimeSinceRemod >= 20 ~ 'Outdated')
data_2$RemodelFromCat<-factor(data_2$RemodelFromCat, levels=c('None','Outdated','Old', 'Mid','Recent', 'New'), ordered=TRUE)
ggplot(data_2, aes(x=RemodelFromCat)) +
  geom_bar(fill = '#053061') +
  geom_text(aes(label=..count..), stat='count', vjust = -.5) +
  theme_minimal() 
data_2$RemodelFromCat<-as.integer(data_2$RemodelFromCat)
head(data_2[,c("YrSold","YearBuilt","YearRemodAdd", "YearRemodAdd2","Remod", 'TimeSinceRemod', 'BldgAge', 'NewBuild')])
```

**Bin the sale month into seasons from low season to high season based on sales** 

```{r BinningSeason, collapse = TRUE}
ggplot(data_2, aes(x=MoSold)) +
  geom_bar(fill = '#053061') +
  geom_text(aes(label=..count..), stat='count', vjust = -.5) +
  theme_minimal() 
data_2$SeasonSale <- mapvalues(data_2$MoSold, from = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"), to = c("LowSeason", "LowSeason", "MidSeason",'MidSeason', 'HighSeason','HighSeason','HighSeason','MidSeason', 'MidSeason','MidSeason','LowSeason','LowSeason'))
head(data_2[,c("MoSold","SeasonSale")])
data_2$SeasonSale<-as.numeric(factor(data_2$SeasonSale,levels=c('LowSeason','MidSeason', 'HighSeason'),ordered=TRUE))
head(data_2[,c("MoSold","SeasonSale")])

```

**Update Categorical Features to have numerical value**

```{r catetonum, collapse = TRUE}
data_2$Utilities <-as.numeric(data_2$Utilities)
data_2$LandSlope<-as.numeric(data_2$LandSlope)
data_2$ExterQual <-as.numeric(data_2$ExterQual)
data_2$ExterCond <-as.numeric(data_2$ExterCond)
data_2$BsmtQual <-as.numeric(data_2$BsmtQual)
data_2$BsmtCond <-as.numeric(data_2$BsmtCond)
data_2$BsmtExposure <-as.numeric(data_2$BsmtExposure)
data_2$BsmtFinType1 <-as.numeric(data_2$BsmtFinType1)
data_2$BsmtFinType2 <-as.numeric(data_2$BsmtFinType2)
data_2$HeatingQC <-as.numeric(data_2$HeatingQC)
data_2$CentralAir <-as.numeric(data_2$CentralAir)
data_2$Electrical <-as.numeric(data_2$Electrical)
data_2$KitchenQual <-as.numeric(data_2$KitchenQual)
data_2$Functional <-as.numeric(data_2$Functional)
data_2$FireplaceQu <-as.numeric(data_2$FireplaceQu)
data_2$GarageFinish <-as.numeric(data_2$GarageFinish)
data_2$GarageQual <-as.numeric(data_2$GarageQual)
data_2$GarageCond <-as.numeric(data_2$GarageCond)
data_2$PavedDrive <-as.numeric(data_2$PavedDrive)
data_2$PoolQC <-as.numeric(data_2$PoolQC)
data_2$Fence <-as.numeric(data_2$Fence)
data_2$OverallQual<-as.numeric(data_2$OverallQual)
data_2$OverallCond <-as.numeric(data_2$OverallCond)

```

### New Variables by Interactions
The last type of feature engineering we attempted is attribute construction from interactions by multiplying certain features listed in the table below. 

**New Variables from Interactions**

| New Feature    | Method                     |
| :--------------| :------------------------- |
| OverallScore   | GarageQual * GarageCond    |
| GarageScore    | OverallQual * OverallCond  |
| ExterScore     | ExterQual * ExterCond      |
| KitchenScore   | KitchenAbvGr * KitchenQual |
| GarageGrade    | GarageArea * GarageQual    |           



```{r Interactions, collapse = TRUE}
data_2 <- mutate(data_2,GarageScore = GarageQual * GarageCond,
                   OverallScore= OverallQual * OverallCond,
                   ExterScore = ExterQual * ExterCond,
                   KitchenScore = KitchenAbvGr * KitchenQual,
                   GarageGrade = GarageArea * GarageQual)
```

## Dimensionality and Numerosity Reduction
The last major step left in pre-processing is dimensionality and numerosity reduction through the following steps:

* 1. Delete Redundant Features Due to Data Engineering
* 2. Delete Outliers
* 3. Delete Features with Near Zero Variability

```{r redundant}
data_2 <- subset(data_2, select = -c(BsmtFinSF1,BsmtFinSF2,BsmtFullBath,GarageArea,GarageQual,
                                         BsmtHalfBath,FullBath,HalfBath,X1stFlrSF, X2ndFlrSF,
                                         OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch,
                                         GarageQual,GarageCond,ExterCond,KitchenAbvGr,KitchenQual,
                                         BsmtFinType1,BsmtFinType2,BsmtCond,BsmtQual,
                                         Neighborhood,YearBuilt,YrSold, BldgAge,YearRemodAdd, MoSold, GarageYrBlt,YearRemodAdd2, Remod, TimeSinceRemod,PoolQC))
```

### Outliers 
According to the data collector: 
“There are 5 observations that an instructor may wish to remove from the data set before giving it to students. Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations)”

As we can see from the scatterplot 4 of the outlier are in the training set, which means that one is in the test set, because we are using the test set to submit to Kaggle.com for evalueation, we can not remove any rows from the test set (or it would be incomplete). Here, we will only remove the outliers indicated by the data collector in the training set only. Additional outlier detection could be useful in the feature. 

```{r OutliersID, collapse = TRUE}
highlight_df <- data_2_train %>% 
             filter(GrLivArea>=4000)

outlier_plot2<-ggplot(data_2_train, aes(x =GrLivArea , y = SalePrice, color="red")) + ggtitle("Outliers: Living Area vs SalesPrice")+
geom_point(color="#053061", alpha=.5)+ 
geom_point(data=highlight_df,
             aes(x=GrLivArea,y=SalePrice, color="red"))+
  theme(
  panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  axis.line = element_line(colour = "#053061"))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) 
outlier_plot2$labels$colour <- "Outliers"
outlier_plot2
```

**For now, we are only deleting the outliers in the training set.**

```{r outliersDel, collapse = TRUE}
highlight_df$SalePrice
data_2 <- data_2[-c(524, 692, 1183, 1299),]  
```

**View data shape after feature engineering**

```{r datashape, collapse = TRUE}
length(data_2)               
#sapply(data_2 , class)
names(data_2) 
```

**Export Pre-processed Data**

```{r cleandata, collapse = TRUE}
write.csv(data_2,"AmesDataClean.csv", row.names = FALSE)
```