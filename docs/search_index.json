[["index.html", "House Prices Advanced Regression Techniques Chapter 1 Introduction", " House Prices Advanced Regression Techniques Yanet Gomez 2021-09-07 Chapter 1 Introduction This is a data analytics project I completed while enrolled in the Master’s of Science in Data Analytics (MSDA) program at the University of Houston Downtown (UHD). It is based on a getting started Kaggle competition: “House Prices - Advanced Regression Techniques”. I chose this project because it provides an opportunity to explore all of the stages in the Data Analytics Workflow: data pre-processing, modeling, prediction, and validation. The goal of this project is to predict the sale price of houses using a large set of house features by applying advanced regression techniques. However, before we could jump into building a highly accurate prediction model, an extensive amount of data pre-processing had to be done. The major pre-processing tasks employed in this project were: data cleaning, to address missing values and noisy data; data transformation via smoothing, attribute construction, aggregation, discretization and hierarchy generation in order to transform the data into appropriate forms for mining; and feature selection by removing redundant features and features with near zero variability. The next step is the modeling portion of the data analytics workflow. In this step I address ways to deal with “the curse of dimensionality”, model selection, model evaluation, and model improvement methods. The final steps involve prediction and validation. The primary evaluation metric was the log Root-Mean-Squared-Error (RMSE), since that is how Kaggle submissions would be evaluated for this competition. Required Packages #install.packages(&quot;bookdown&quot;) #install.packages(&quot;glmnet&quot;) #for fitting regularized models, Repeated cross validation #install.packages(&quot;vip&quot;) #for variable importance #install.packages(&quot;car&quot;) #for VIF #install.packages(&quot;DataExplorer&quot;) #for graphing missing data #install.packages(&quot;data.table&quot;) #for tables #install.packages(&quot;imputeMissings&quot;) #for NAs #install.packages(&quot;mice&quot;) #for RF prediction #install.packages(&quot;naniar&quot;) #plot missing values #install.packages(&quot;mlr3pipelines&quot;) #install.packages(&quot;kableExtra&quot;) #for tables #install.packages(&quot;ggridges&quot;) #for ploting #install.packages(&quot;DiagrammeR&quot;) #for workflow diagram #install.packages(&quot;webshot&quot;) #for workflow diagram #install.packages(&quot;ggplot2&quot;) #for plotting #install.packages(&quot;caret&quot;) #for tuning and crossvalidation #install.packages(&quot;xgboost&quot;) #for xgboost algorithm #install.packages(&quot;Boruta&quot;) #for feature selection #install.packages(&quot;GGally&quot;) #for plotting #install.packages(&quot;lattice&quot;) #for plotting "],["origin.html", "Chapter 2 Origin 2.1 Starting Point 2.2 Technical Skills 2.3 Problem Definition 2.4 Data Analytics Workflow", " Chapter 2 Origin Semester Date: Spring 2021 Class: Capstone Portfolio (STAT 6382) Program: Master of Science in Data Analytics School: University of Houston Downtown 2.1 Starting Point House Prices: Advanced Regression Techniques is an ongoing Kaggle competition (Kaggle, n.d.). This “getting started” competition calls for extensive data pre-processing, feature engineering, and implementation of advanced regression techniques like random forest and gradient boosting. The data-set was made available through the Kaggle website at kaggle.com (De Cock 2011b). 2.2 Technical Skills Techniques: Data Cleaning, Exploratory Data Analysis, Feature Engineering, Advanced Regression Techniques, Machine Learning Program: R 2.3 Problem Definition Buying a house is a complicated decision making process for a lot of people. As Kaggle.com puts it: “Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad” (Kaggle, n.d.). However, it is in fact, characteristics like these that influence the price of homes. Throughout this project we will explore a plethora of house features from kitchen quality to land slope, and evaluate their contribution to the sale price of homes. With some luck, or rather some data analytics skills, we will gain some insight into the real estate world of pricing residential homes. By understanding the relationship between key features and price, both buyers and sellers can optimize their price negotiations game, and make smarter financial decisions in the process of buying or selling a home. This project has three main goals. The first, as defined by Kaggle, is to predict the sales price value for each ID in the test set using advanced regression techniques. The second goal is to identify which features affect sale price the most. Lastly, I would like to explore which data analytics techniques lead to a better prediction in this case. Which are the most relevant features that impact sales price? Which data pre-processing techniques improve the predictive value of the data? Which model performs best at predicting price? 2.4 Data Analytics Workflow Here 2.1 is an overview of the data analytics workflow for this project done with packages ‘DiagrameR’ (Iannone 2020) and ‘webshot’ (Chang 2019). Figure 2.1: Workflow References "],["data.html", "Chapter 3 Data", " Chapter 3 Data The data is the “Ames Housing Dataset”, provided at kaggle.com, which contains information about the sale of individual residential properties in Ames, Iowa from 2006 to 2010. The original dataset was compiled by Dean DeCock for use in data science education, and and is available online here: Ames Housing Dataset. The data set contains 2919 observations and 80 explanatory variables, plus the variable of interest: Sales Price. Kaggle provided the data split into train and test sets. # Import Train and Tests Datasets train_raw&lt;-read.csv(&quot;train.csv&quot;,stringsAsFactors = TRUE) # 1460 rows, 81 columns test_raw&lt;-read.csv(&quot;test.csv&quot;, stringsAsFactors = TRUE) #Merge Datasets for preprocessing steps. Separate into train and test sets later for modeling and prediction. test_raw$&quot;SalePrice&quot;&lt;-0 # Create &quot;SalePrice&quot; feature for test set data_full&lt;- rbind(train_raw, test_raw) # Combine hdb_train and hdb_test #Save ID column for submissiom test_ID &lt;-test_raw$Id ## There are 1460 rows and 81 columns in the Train data-set. ## There are 1459 rows and 81 columns in the Test data-set. ## There are 2919 rows and 81 columns in the combined data-set. Table 3.1 shows a snippet of the raw data including the first 10 rows and 10 columns plus SalesPrice. Table 3.1: Raw Data Snippet Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities SalePrice 1 60 RL 65 8450 Pave NA Reg Lvl AllPub 208500 2 20 RL 80 9600 Pave NA Reg Lvl AllPub 181500 3 60 RL 68 11250 Pave NA IR1 Lvl AllPub 223500 4 70 RL 60 9550 Pave NA IR1 Lvl AllPub 140000 5 60 RL 84 14260 Pave NA IR1 Lvl AllPub 250000 6 50 RL 85 14115 Pave NA IR1 Lvl AllPub 143000 7 20 RL 75 10084 Pave NA Reg Lvl AllPub 307000 8 60 RL NA 10382 Pave NA IR1 Lvl AllPub 200000 9 50 RM 51 6120 Pave NA Reg Lvl AllPub 129900 10 190 RL 50 7420 Pave NA Reg Lvl AllPub 118000 "],["data-pre-processing.html", "Chapter 4 Data Pre-processing 4.1 Identifying and Correcting Missing Values 4.2 Correct Data Types 4.3 Feature Transformation 4.4 Dimensionality and Numerosity Reduction", " Chapter 4 Data Pre-processing The goal of this project is to predict the sale price of houses using a large set of house features by applying advanced regression techniques. However, before we jump into building a highly accurate prediction model, an extensive amount of data pre-processing has to be done. The major pre-processing tasks employed in this project are: data cleaning, to address missing values and noisy data; data transformation via smoothing, attribute construction, aggregation, discretization and hierarchy generation in order to transform the data into appropriate forms for mining; and feature selection by removing redundant features and features with near zero variability. The first step is to explore the data and identify any discrepancies or opportunities for improvement by examining the following: Missing Values Attribute types Distribution, Skewness and Relationships Dimensionality 4.1 Identifying and Correcting Missing Values 4.1.1 Detect Missing Values col_na&lt;-which(colSums(is.na(data_full)) &gt; 0) col_na_sum&lt;-sum(colSums(is.na(data_full)) &gt; 0) cat(&quot;The total number of missing values in the combined data-set is:&quot;, sum(is.na(data_full)),&quot;\\nThe number of columns containing missing values is:&quot;, col_na_sum) *** The total number of missing values in the combined data-set is: 13965 *** The number of columns containing missing values is: 34 We can see from the heatmap in 4.1, that about 6% of the data is “missing”. A simple strategy to deal with missing values is to eliminate the features or examples containing missing values. However, given that there are a total of 13965 missing values spread across 34 features, dropping these data points could mean loss of data critical to the analysis. Instead, we will deal with missing values by imputation using a variety of methods. # Heat map for missing values of the housing dataset with function missmap from library(Rcpp) missmap(data_full,col = c(&quot;blue&quot;,&quot;gray&quot;), main =&quot;Heatmap showing Missing values&quot;) Figure 4.1: Missing Values Heatmap Figure 4.2 shows that the features “PoolQC”, “MiscFeature”, “Alley”, and “Fence” have a high number of missing values. It might be tempting to drop these features, but with one quick look at the data description provided with the data (De Cock 2011a) we learn that “NA” in these cases means that the feature is not applicable, so it should be either “0” or “None”. NA_col_list &lt;- sort(col_na, decreasing = T) # arrange the named list with descending order gg_miss_var(data_full[,NA_col_list], show_pct = FALSE )+theme(text = element_text(size = 8,)) Figure 4.2: Number of Missing Values per Column Table 4.1 shows the number and percentge of missing values per column in the combined dataset. # Summary of Missing Values per Column/Variable kable(miss_var_summary(data_full[,NA_col_list]), caption=&#39;Summary of Missing Values per Column&#39;, booktabs = TRUE) Table 4.1: Summary of Missing Values per Column variable n_miss pct_miss PoolQC 2909 99.6574169 MiscFeature 2814 96.4028777 Alley 2721 93.2168551 Fence 2348 80.4385063 FireplaceQu 1420 48.6467968 LotFrontage 486 16.6495375 GarageCond 159 5.4470709 GarageQual 159 5.4470709 GarageFinish 159 5.4470709 GarageYrBlt 159 5.4470709 GarageType 157 5.3785543 BsmtExposure 82 2.8091812 BsmtCond 82 2.8091812 BsmtQual 81 2.7749229 BsmtFinType2 80 2.7406646 BsmtFinType1 79 2.7064063 MasVnrType 24 0.8221994 MasVnrArea 23 0.7879411 MSZoning 4 0.1370332 Functional 2 0.0685166 BsmtHalfBath 2 0.0685166 BsmtFullBath 2 0.0685166 Utilities 2 0.0685166 SaleType 1 0.0342583 GarageArea 1 0.0342583 GarageCars 1 0.0342583 KitchenQual 1 0.0342583 Electrical 1 0.0342583 TotalBsmtSF 1 0.0342583 BsmtUnfSF 1 0.0342583 BsmtFinSF2 1 0.0342583 BsmtFinSF1 1 0.0342583 Exterior2nd 1 0.0342583 Exterior1st 1 0.0342583 4.1.2 Dealing With Missing Values There are two types of missing values in this data-set, some values classified as “NA” are truly missing from the data, the information was not collected. The second type of “NA” means that the feature is not present, so “NA” in this case means either ‘Zero’ for numerical features, ‘None’ for categorical features, or ‘No’ for binary features. By examining the data and reading the data description provided (De Cock 2011a), we are able to determine each case. 4.1.2.1 Manual Imputation of NAs due to Inconsistent Values Starting with groups of related features for garage, basement, pool and masonry attributes to explore the columns containing missing values, we discover that there are a few inconsistencies in the raw data, for example for the feature MasVnrType, there is a missing value for row ID=2611, but the column “MasVnrArea” shows a value, this obviously indicates that there is a MasVnrType associated with this instance, so instead of replacing it with “None”, we will impute the missing value with the most common value for this feature. masonry_features &lt;- names(data_full)[sapply(names(data_full), function(x) str_detect(x, &quot;Mas&quot;))] mas_na&lt;-which(is.na(data_full$MasVnrType) &amp; data_full$MasVnrArea &gt;0) data_full[mas_na,masonry_features] ## MasVnrType MasVnrArea ## 2611 &lt;NA&gt; 198 For features containing inconsistent values, listed in table 4.2, we will impute the missing values mannually according to each case. Inconsitent_Values&lt;- c(&quot;BsmtQual&quot;, &quot;BsmtCond&quot;, &quot;GarageQual&quot;,&quot;GarageFinish&quot;, &quot;GarageCond&quot;, &quot;GarageYrBlt&quot;,&quot;BsmtCond&quot;, &quot; PoolQC&quot;, &quot;MasVnrArea&quot;) Inconsitent_Values&lt;-tibble(&quot;Feature&quot;=Inconsitent_Values, &quot;Method&quot;=rep(&quot;Manual Imputation&quot;, length(Inconsitent_Values))) kable(Inconsitent_Values, caption = &quot;Inconsistent Values&quot;) Table 4.2: Inconsistent Values Feature Method BsmtQual Manual Imputation BsmtCond Manual Imputation GarageQual Manual Imputation GarageFinish Manual Imputation GarageCond Manual Imputation GarageYrBlt Manual Imputation BsmtCond Manual Imputation PoolQC Manual Imputation MasVnrArea Manual Imputation Garage Features Row 2127 has a garage, given that it has values for “GarageArea” (360), “GarageType” (Detchd), and “GarageCars” (1), so fill in the ‘GarageQual’,‘GarageFinish’, and ‘GarageCond’ with most the common for those features values. garage_features &lt;- names(data_full)[sapply(names(data_full), function(x) str_detect(x, &quot;Garage&quot;))] #View(data_full[which(is.na(data_full$GarageCond)), garage_features]) # To look at all garage features containing missing values #count(data_full[which(is.na(data_full$GarageCond)), garage_features]) #159 kable(data_full[2127,garage_features]) GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond 2127 Detchd NA NA 1 360 NA NA #Get most commom values kable(names(sapply(data_full[which( data_full$GarageCars == 1 &amp; data_full$GarageType==&quot;Detchd&quot; ) ,garage_features], function(x) sort(table(x), decreasing=TRUE)[1])), col.names = &quot;Most Common Value per Feature&quot;) Most Common Value per Feature GarageType.Detchd GarageYrBlt.1920 GarageFinish.Unf GarageCars.1 GarageArea.240 GarageQual.TA GarageCond.TA # Replace Values Manually data_full[2127,&#39;GarageQual&#39;] = &#39;TA&#39; data_full[2127, &#39;GarageFinish&#39;] = &#39;Unf&#39; data_full[2127, &#39;GarageCond&#39;] = &#39;TA&#39; #Check changes data_full[2127,garage_features] *** GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual *** 2127 Detchd NA Unf 1 360 TA *** GarageCond *** 2127 TA Likewise, we can see that row 2577 has no garage, so we can fill in garage type with none. data_full[2577,garage_features] ## GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual ## 2577 Detchd NA &lt;NA&gt; NA NA &lt;NA&gt; ## GarageCond ## 2577 &lt;NA&gt; #Check and correct levels in Garage Type levels(data_full$GarageType) ## [1] &quot;2Types&quot; &quot;Attchd&quot; &quot;Basment&quot; &quot;BuiltIn&quot; &quot;CarPort&quot; &quot;Detchd&quot; data_full$GarageType&lt;-factor(data_full$GarageType, levels=c(&quot;None&quot;,&quot;2Types&quot;,&quot;Attchd&quot;, &quot;Basment&quot;,&quot;BuiltIn&quot;,&quot;CarPort&quot;,&quot;Detchd&quot; ), ordered=FALSE) #Update Garage type for row 2577 data_full[2577, &#39;GarageType&#39;] = &#39;None&#39; There is an error in GarageYrBlt, from the summary we see a year 2207. We will update to 2007. summary(data_full$GarageYrBlt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1895 1960 1979 1978 2002 2207 159 subset(data_full[garage_features], GarageYrBlt &gt;= 2011) ## GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual ## 2593 Attchd 2207 RFn 2 502 TA ## GarageCond ## 2593 TA data_full$GarageYrBlt[data_full$GarageYrBlt==2207] &lt;- 2007 summary(data_full$GarageYrBlt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1895 1960 1979 1978 2002 2010 159 For the missing values in GarageYrBlt, we will fill in NAs with with the year the house was built. cat(&quot;GarageYrBlt has missing values:&quot;, sum(is.na(data_full$GarageYrBlt))) *** GarageYrBlt has missing values: 159 # Fill in year garage built in the same year when house was built. *** data_full$GarageYrBlt[is.na(data_full$GarageYrBlt)]&lt;-data_full$YearBuilt[is.na(data_full$GarageYrBlt)] cat(&quot; After inputing NA&#39;s with year YearBuilt values, GarageYrBlt has&quot;, sum(is.na(data_full$GarageYrBlt)), &quot;missing values&quot;) *** After inputing NA&#39;s with year YearBuilt values, GarageYrBlt has 0 missing values Basement Features From viewing at all the basement columns we can see that basement condition is missing in rows 2041, 2186, 2525, and will replace with most the common feature for each column. basement_features &lt;- names(data_full)[sapply(names(data_full), function(x) str_detect(x, &quot;Bsmt&quot;))] #View(data_full[is.na(data_full$BsmtCond), basement_features]) data_full[c(2041, 2186,2525),basement_features] ## BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 ## 2041 Gd &lt;NA&gt; Mn GLQ 1044 Rec ## 2186 TA &lt;NA&gt; No BLQ 1033 Unf ## 2525 TA &lt;NA&gt; Av ALQ 755 Unf ## BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath ## 2041 382 0 1426 1 0 ## 2186 0 94 1127 0 1 ## 2525 0 240 995 0 0 #names(which.max(table(data_full$BsmtCond))) data_full[c(2041,2186, 2525),&#39;BsmtCond&#39;]=names(which.max(table(data_full$BsmtCond))) #Check changes data_full[c(2041, 2186,2525), basement_features] ## BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 ## 2041 Gd TA Mn GLQ 1044 Rec ## 2186 TA TA No BLQ 1033 Unf ## 2525 TA TA Av ALQ 755 Unf ## BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath ## 2041 382 0 1426 1 0 ## 2186 0 94 1127 0 1 ## 2525 0 240 995 0 0 Pool Features There are three pools where values for the ‘quality’ are missing, so we will fill in with the most common value based on the area of the pool. pool_features &lt;- names(data_full)[sapply(names(data_full), function(x) str_detect(x, &quot;Pool&quot;))] data_full[c(2421,2504,2600),pool_features] ## PoolArea PoolQC ## 2421 368 &lt;NA&gt; ## 2504 444 &lt;NA&gt; ## 2600 561 &lt;NA&gt; pool_na&lt;-which(is.na(data_full$PoolQC) &amp; data_full$PoolArea &gt;0) aggregate(data=data_full, PoolArea~PoolQC, mean, na.rm=TRUE) ## PoolQC PoolArea ## 1 Ex 359.75 ## 2 Fa 583.50 ## 3 Gd 648.50 data_full$PoolArea[which(is.na(data_full$PoolQC) &amp; data_full$PoolArea &gt;0)] ## [1] 368 444 561 #Replace NA with most common values data_full$PoolQC[data_full$Id == 2421] &lt;- &quot;Ex&quot; data_full$PoolQC[data_full$Id == 2504] &lt;- &quot;Ex&quot; data_full$PoolQC[data_full$Id == 2600] &lt;- &quot;Fa&quot; #Check changes data_full[c(2421,2504,2600),pool_features] ## PoolArea PoolQC ## 2421 368 Ex ## 2504 444 Ex ## 2600 561 Fa MasVnrType Features There is a missing value for row ID=2611 in the MasVnrType column, we will replace with most common “MasVnrType”. masonry_features &lt;- names(data_full)[sapply(names(data_full), function(x) str_detect(x, &quot;Mas&quot;))] mas_na&lt;-which(is.na(data_full$MasVnrType) &amp; data_full$MasVnrArea &gt;0) mas_na #ID=2611 ## [1] 2611 data_full[2611,masonry_features] ## MasVnrType MasVnrArea ## 2611 &lt;NA&gt; 198 data_full$MasVnrArea[which(is.na(data_full$MasVnrType) &amp; data_full$MasVnrArea &gt;0)] #198 ## [1] 198 aggregate(data=data_full, MasVnrArea~MasVnrType, mean, na.rm=TRUE) ## MasVnrType MasVnrArea ## 1 BrkCmn 195.4800000 ## 2 BrkFace 261.6723549 ## 3 None 0.7072331 ## 4 Stone 239.5502008 names(which.max(table(data_full$MasVnrType))) ## [1] &quot;None&quot; summary(data_full$MasVnrType) #most common is BrkFace ## BrkCmn BrkFace None Stone NA&#39;s ## 25 879 1742 249 24 data_full$MasVnrType[data_full$Id == 2611] &lt;- &quot;BrkFace&quot; #Check Changes data_full[2611,masonry_features] ## MasVnrType MasVnrArea ## 2611 BrkFace 198 4.1.2.2 Imputation with Mode for Categorical Features with a Few NAs For some of the categorical features only missing a few values, we filled in the missing values with the most commonly occurring attribute value. Specially because for many of these the most frequent category was already over-represented, so it is pretty safe to assume that the missing values are more likely to be in the most common category. 1 Figure 4.3: Categorical Features with Few NAs Table 4.3: Few NAs Feature Method Utilities Mode Functional Mode Exterior1st Mode Exterior2nd Mode Electrical Mode KitchenQual Mode SaleType Mode #Replace with most common value since these are missing very few values: na_m &lt;- c( &quot;Utilities&quot;, &quot;Functional&quot;, &quot;Exterior1st&quot;, &quot;Exterior2nd&quot;, &quot;Electrical&quot;, &quot;KitchenQual&quot;, &quot;SaleType&quot;) data_full[,na_m] &lt;- apply(data_full[,na_m], 2, function(x) {replace(x, is.na(x), names(which.max(table(x))))}) 4.1.2.3 Imputation with “None” for Categorical Features According to the data description file (De Cock 2011a) for the categorical features listed in MissValsSumm, NA means that the feature is not present in the house. For example, the house doesn’t have a garage. For these features the NA values were replaced with “None”. Table 4.4: Categorical Features where NA means ‘None’ Feature Method GarageFinish None GarageQual None GarageType None GarageCond None BsmtCond None BsmtExposure None BsmtQual None BsmtFinType1 None BsmtFinType2 None FireplaceQu None EnclosedPorch None MiscFeature None Alley None Fence None MasVnrType None na_none &lt;- c(&quot;GarageFinish&quot;, &quot;GarageQual&quot;, &quot;GarageType&quot;, &quot;GarageCond&quot;, &quot;BsmtCond&quot;, &quot;BsmtExposure&quot;, &quot;BsmtQual&quot;, &quot;BsmtFinType1&quot;, &quot;BsmtFinType2&quot;, &quot;FireplaceQu&quot;, &quot;EnclosedPorch&quot;,&quot;PoolQC&quot;,&quot;MiscFeature&quot;,&quot;Alley&quot;, &quot;Fence&quot;, &quot;MasVnrType&quot;) data_full[,na_none] &lt;- apply(data_full[,na_none], 2, function(x) {replace(x, is.na(x), &quot;None&quot;)}) 4.1.2.4 Imputation with Zero (0) for Numerical Features where NA means Feature is not Present According to the data description file (De Cock 2011a) for the numerical features listed in table 4.5. NA means that the feature is not present in the house, so we replaced NA with the number zero for these features. Continous &lt;- c(&quot;BsmtFinSF1&quot;,&quot;BsmtFinSF2&quot;, &quot;BsmtUnfSF&quot;, &quot;TotalBsmtSF&quot;, &quot;BsmtFullBath&quot;, &quot;BsmtHalfBath&quot;,&quot;GarageCars&quot;,&quot;GarageArea&quot;, &quot;MasVnrArea&quot;) Continous &lt;- tibble(&quot;Feature&quot;=Continous, &quot;Method&quot;=rep(&quot;0&quot;,length(Continous))) kable(Continous, caption = &quot;NA means &#39;0&#39;&quot;) Table 4.5: NA means ‘0’ Feature Method BsmtFinSF1 0 BsmtFinSF2 0 BsmtUnfSF 0 TotalBsmtSF 0 BsmtFullBath 0 BsmtHalfBath 0 GarageCars 0 GarageArea 0 MasVnrArea 0 na_z &lt;- c(&quot;BsmtFinSF1&quot;,&quot;BsmtFinSF2&quot;, &quot;BsmtUnfSF&quot;, &quot;TotalBsmtSF&quot;, &quot;BsmtFullBath&quot;, &quot;BsmtHalfBath&quot;,&quot;GarageCars&quot;,&quot;GarageArea&quot;, &quot;MasVnrArea&quot;) data_full[,na_z] &lt;- apply(data_full[,na_z], 2, function(x) {replace(x, is.na(x), 0)}) 4.1.2.5 Imputation with estimation for Features with Large Number of Missing Values Lastly, we were left with two features as seen in 4.4. LotFrontage, which captures the linear feet of street-connected to the property; and MSZoning, which indicates the different zoning classifications ranging from agricultural to residential. Because these features had a large percentage of missing values, it might be better to estimate their value based on other attributes. I used the mice package(van Buuren and Groothuis-Oudshoorn 2011), which uses a Random Forest algorithm to estimate the missing values. cols_missing_values2&lt;-data_full[which(colSums(is.na(data_full))&gt;0)] plot_missing(cols_missing_values2) Figure 4.4: Remaining Missing Values 2 Table 4.6: Large Number of Missing Values Feature Method LotFrontage Prediction MSZoning Prediction #Impute LotFrontage (This takes a long time to run) library(mice) set.seed(123) mice_rf_mod&lt;- mice(data_full[, !names(data_full) %in% c(&#39;Id&#39;, &#39;SalePrice&#39;)], method =&#39;rf&#39;, printFlag = FALSE) ## Warning: Number of logged events: 73 mice_output &lt;- complete(mice_rf_mod) #Inpute LotFrontage sum(is.na(data_full$LotFrontage)) ## [1] 486 data_full$LotFrontage[is.na(data_full$LotFrontage)] &lt;- mice_output$LotFrontage[is.na(data_full$LotFrontage)] sum(is.na(data_full$LotFrontage)) ## [1] 0 #Inpute MSZoning set.seed(123) sum(is.na(data_full$MSZoning)) ## [1] 4 data_full$MSZoning[is.na(data_full$MSZoning)] &lt;- mice_output$MSZoning[is.na(data_full$MSZoning)] sum(is.na(data_full$MSZoning)) ## [1] 0 After imputing values for all NA’s, confirm all missing values are now cleared. sum(is.na(data_full)) ## [1] 0 missmap(data_full,col = c(&quot;red&quot;,&quot;gray&quot;), main =&quot;Heatmap showing Missing values&quot;) Figure 4.5: Final Missing Values HeatMap 4.2 Correct Data Types The next step in pre-processing, after all the NA values have been cleared, is to identify and correct data type inconsistencies. The raw data shows that there are 43 factors, 37 integers, and 1 numeric data types. ## ## factor integer ## 43 38 ## ## factor integer numeric ## 43 37 1 Our current dataset looks like this: ## Integer Features (int): Id MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd X1stFlrSF X2ndFlrSF LowQualFinSF GrLivArea FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces WoodDeckSF OpenPorchSF X3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold ## Categorical Features: MSZoning Street LotShape LandContour LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle RoofStyle RoofMatl ExterQual ExterCond Foundation Heating HeatingQC CentralAir PavedDrive SaleCondition ## Character features: Alley Utilities Exterior1st Exterior2nd MasVnrType BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 Electrical KitchenQual Functional FireplaceQu GarageType GarageFinish GarageQual GarageCond EnclosedPorch PoolQC Fence MiscFeature SaleType ## Numerical Features(num): MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath BsmtHalfBath GarageYrBlt GarageCars GarageArea SalePrice However, the documentation on the data (De Cock 2011a) says that the data-set consist of 20 continuous features that refer to area dimensions, 14 discrete features that quantify the number of items in the house, 23 nominal categorical features that refer to types of dwellings, materials and conditions, and 23 ordinal categorical features that rate various property related items. After correcting the data types according to the documentstion, it should look as shown in table 4.7. Table 4.7: How the Data Should Look Cont_Features_20 Disc_Features_14 Nom_Categorical_23 Ord_Categorical_23 LotFrontage BsmtFullBath Neighborhood Utilities MasVnrArea BsmtHalfBath SaleCondition LandSlope BsmtFinSF1 FullBath HouseStyle ExterQual BsmtFinSF2 HalfBath Street ExterCond BsmtUnfSF BedroomAbvGr Alley BsmtQual TotalBsmtSF KitchenAbvGr CentralAir BsmtCond GarageArea TotRmsAbvGrd LandContour BsmtExposure X1stFlrSF Fireplaces Condition1 BsmtFinType1 X2ndFlrSF GarageCars Condition2 BsmtFinType2 LowQualFinSF GarageYrBlt BldgType HeatingQC GrLivArea YearBuilt RoofStyle Electrical X3SsnPorch YearRemodAdd RoofMatl KitchenQual PoolArea MoSold Exterior1st Functional WoodDeckSF YrSold Exterior2nd FireplaceQu SalePrice Foundation GarageFinish EnclosedPorch Heating GarageQual ScreenPorch GarageType GarageCond LotArea MiscFeature PavedDrive MiscVal SaleType PoolQC OpenPorchSF MSSubClass Fence MSZoning OverallQual MasVnrType OverallCond LotConfig LotShape The first step is to convert the numeric features to the appropriate data type. to_num &lt;-c( &quot;LotFrontage&quot;, &quot;MasVnrArea&quot;, &quot;BsmtFinSF1&quot;, &quot;BsmtFinSF2&quot;, &quot;BsmtUnfSF&quot;, &quot;TotalBsmtSF&quot;, &quot;GarageArea&quot;,&quot;X1stFlrSF&quot;, &quot;X2ndFlrSF&quot;, &quot;LowQualFinSF&quot;, &quot;GrLivArea&quot;, &quot;X3SsnPorch&quot;, &quot;PoolArea&quot;, &quot;WoodDeckSF&quot;, &quot;SalePrice&quot; ,&quot;EnclosedPorch&quot;, &quot;ScreenPorch&quot;, &quot;LotArea&quot;, &quot;MiscVal&quot;, &quot;OpenPorchSF&quot;) data_full[,to_num] &lt;- lapply(data_full[,to_num], as.numeric) to_int&lt;-c(&quot;BsmtFullBath&quot;,&quot;BsmtHalfBath&quot;,&quot;FullBath&quot;, &quot;HalfBath&quot;, &quot;BedroomAbvGr&quot;, &quot;KitchenAbvGr&quot;, &quot;TotRmsAbvGrd&quot;, &quot;Fireplaces&quot;, &quot;GarageCars&quot;, &quot;MoSold&quot;, &quot;YrSold&quot;,&quot;YearRemodAdd&quot;,&quot;GarageYrBlt&quot;,&quot;YearBuilt&quot;) data_full[,to_int] &lt;- lapply(data_full[,to_int], as.integer) The second step, is to convert missclassified numeric features to categorical. nom_to_cat &lt;-c(&quot;MasVnrType&quot;,&quot;MSSubClass&quot;, &quot;Neighborhood&quot;,&quot;CentralAir&quot;, &quot;SaleCondition&quot;, &quot;HouseStyle&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;LandContour&quot;, &quot;Condition1&quot;, &quot;Condition2&quot;, &quot;BldgType&quot;, &quot;RoofStyle&quot;, &quot;RoofMatl&quot;, &quot;Exterior1st&quot;, &quot;Exterior2nd&quot;, &quot;Foundation&quot;, &quot;BsmtExposure&quot;, &quot;Heating&quot;, &quot;GarageType&quot;, &quot;PavedDrive&quot;, &quot;MiscFeature&quot;, &quot;SaleType&quot;) data_full[,nom_to_cat] &lt;- lapply(data_full[,nom_to_cat], factor) The third step is to add levels to ordinal categorical features. data_full$Utilities&lt;-factor(data_full$Utilities,levels=c(&quot;ELO&quot;,&quot;NoSeWa&quot;,&quot;NoSewr&quot;,&quot;AllPub&quot;), ordered=TRUE) data_full$ExterQual&lt;-factor(data_full$ExterQual, levels=c(&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$ExterCond&lt;-factor(data_full$ExterCond, levels=c(&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$FireplaceQu&lt;-factor(data_full$FireplaceQu, levels=c(&quot;None&quot;,&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$Functional&lt;-factor(data_full$Functional,levels=c(&quot;Sal&quot;, &quot;Sev&quot;, &quot;Maj2&quot;, &quot;Maj1&quot;,&quot;Mod&quot;,&quot;Min2&quot;,&quot;Min1&quot;,&quot;Typ&quot;), ordered=TRUE) data_full$PoolQC&lt;-factor(data_full$PoolQC,levels=c(&quot;None&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$BsmtCond&lt;-factor(data_full$BsmtCond, levels=c(&quot;None&quot;,&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$BsmtQual&lt;-factor(data_full$BsmtQual,levels=c(&quot;None&quot;,&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$BsmtExposure&lt;-factor(data_full$BsmtExposure, levels=c(&quot;None&quot;, &quot;No&quot;, &quot;Mn&quot;, &quot;Av&quot;, &quot;Gd&quot;), ordered=TRUE) data_full$BsmtFinType1&lt;-factor(data_full$BsmtFinType1, levels=c(&quot;None&quot;,&quot;Unf&quot;,&quot;LwQ&quot;,&quot;Rec&quot;,&quot;BLQ&quot;,&quot;ALQ&quot;,&quot;GLQ&quot;), ordered=TRUE) data_full$BsmtFinType2&lt;-factor(data_full$BsmtFinType2, levels=c(&quot;None&quot;,&quot;Unf&quot;,&quot;LwQ&quot;,&quot;Rec&quot;,&quot;BLQ&quot;,&quot;ALQ&quot;,&quot;GLQ&quot;), ordered=TRUE) data_full$HeatingQC&lt;-factor(data_full$HeatingQC, levels=c(&quot;Po&quot;, &quot;Fa&quot;, &quot;TA&quot;, &quot;Gd&quot;, &quot;Ex&quot;), ordered=TRUE) data_full$KitchenQual&lt;-factor(data_full$KitchenQual, levels=c(&quot;Po&quot;, &quot;Fa&quot;, &quot;TA&quot;, &quot;Gd&quot;, &quot;Ex&quot;), ordered=TRUE) data_full$GarageQual&lt;-factor(data_full$GarageQual, levels=c(&quot;None&quot;,&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$GarageCond&lt;-factor(data_full$GarageCond, levels=c(&quot;None&quot;,&quot;Po&quot;,&quot;Fa&quot;,&quot;TA&quot;,&quot;Gd&quot;,&quot;Ex&quot;), ordered=TRUE) data_full$Electrical&lt;-factor(data_full$Electrical, levels=c(&quot;Mix&quot;,&quot;FuseP&quot;,&quot;FuseF&quot;,&quot;FuseA&quot;,&quot;SBrkr&quot;), ordered=TRUE) data_full$GarageFinish&lt;-factor(data_full$GarageFinish, levels=c(&quot;None&quot;,&quot;Unf&quot;,&quot;RFn&quot;,&quot;Fin&quot;), ordered=TRUE) data_full$PavedDrive&lt;-factor(data_full$PavedDrive, levels=c(&quot;N&quot;,&quot;P&quot;,&quot;Y&quot;), ordered=TRUE) data_full$Fence&lt;-factor(data_full$Fence, levels=c(&quot;None&quot;,&quot;MnWw&quot;,&quot;GdWo&quot;, &quot;MnPrv&quot;, &quot;GdPrv&quot;), ordered=TRUE) data_full$OverallQual&lt;-factor(data_full$OverallQual, levels=c(&quot;1&quot;, &quot;2&quot;,&quot;3&quot;,&quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;), ordered=TRUE) data_full$OverallCond&lt;-factor(data_full$OverallCond, levels=c(&quot;1&quot;, &quot;2&quot;,&quot;3&quot;,&quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;), ordered=TRUE) data_full$GarageType&lt;-factor(data_full$GarageType, levels=c(&quot;None&quot;,&quot;2Types&quot;,&quot;Attchd&quot;, &quot;Basment&quot;,&quot;BuiltIn&quot;,&quot;CarPort&quot;,&quot;Detchd&quot; ), ordered=FALSE) data_full$LandSlope&lt;-factor(data_full$LandSlope, levels=c(&quot;Gtl&quot;,&quot;Mod&quot;,&quot;Sev&quot; ), ordered=TRUE) data_full$LotShape&lt;-factor(data_full$LotShape, levels=c(&quot;IR1&quot;,&quot;IR2&quot;, &quot;IR3&quot;, &quot;Reg&quot; ), ordered=TRUE) Confirm all features are now approrpiately classified by their data type. *** 15 Discrete Features: Id YearBuilt YearRemodAdd BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars MoSold YrSold *** 20 Continous Feature: LotFrontage LotArea MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF X1stFlrSF X2ndFlrSF LowQualFinSF GrLivArea GarageArea WoodDeckSF OpenPorchSF EnclosedPorch X3SsnPorch ScreenPorch PoolArea MiscVal SalePrice *** 23 Nominal Categorical Features: MSSubClass MSZoning Street Alley LandContour LotConfig Neighborhood Condition1 Condition2 BldgType HouseStyle RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType Foundation Heating CentralAir GarageType MiscFeature SaleType SaleCondition *** 23 Ordered Categorical Features: LotShape Utilities LandSlope OverallQual OverallCond ExterQual ExterCond BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 HeatingQC Electrical KitchenQual Functional FireplaceQu GarageFinish GarageQual GarageCond PavedDrive PoolQC Fence 4.3 Feature Transformation With clean data in the correct form, it is time to start the Exploratory Data Analysis (EDA). By visualizing the data we hope to uncover interesting patterns and relationships which we could use to enhance the predictive value of the features via transformation or new feature creation. 4.3.1 Visualizing the Distribution and Spread of Target Feature: SalesPrice The histogram of the “SalesPrice” feature in figure 4.6 shows a few very large values on the right, making the distribution of the data right skewed. Since the goal is to predict the continuous numerical variable “SalesPrice” with regression models, it might be useful to transform it, since one of the assumptions of regression analysis is that the error between the observed and expected values (the residuals) should be normally distributed, and violations of this assumption often stem from a skewed response variable. We can make the distribution more normal by taking the natural logarithm, since in a right-skewed distribution where there are a few very large values, the log transformation helps bring these values into the center. After applying the log transformation, as seen in 4.7, the distribution looks more symmetrical. #Create a copy of the whole dataset to work from here on out data_2&lt;-data_full #Create a set that includes only the training examples data_2_train&lt;-data_2[1:1460,] par(mfrow=c(1,1)) ggplot(data_2_train, aes(SalePrice)) + geom_histogram(fill=&quot;#053061&quot;, alpha=.5, color=&quot;#F5F5F5&quot; , bins = 30)+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE))+ geom_vline(aes(xintercept = mean(SalePrice), color = &quot;mean&quot;), linetype = &quot;dashed&quot;, size = .7) + geom_vline(aes(xintercept = median(SalePrice), color = &quot;median&quot;), linetype = &quot;dashed&quot;, size = .7) + scale_color_manual(name = &quot;Central Tendency&quot;, values = c(mean = &quot;#67001F&quot;, median = &quot;#01665E&quot;)) Figure 4.6: Target Feature: SalesPrice #Log transform the Target Feature data_2_log&lt;-data_2_train data_2_log$SalePrice&lt;-log(data_2_log$SalePrice) ggplot(data_2_log, aes((SalePrice))) + geom_histogram(fill=&quot;#053061&quot;, alpha=.5, color=&quot;#F5F5F5&quot;, bins = 30)+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + geom_vline(aes(xintercept = mean(SalePrice), color = &quot;mean&quot;), linetype = &quot;dashed&quot;, size = .7) + geom_vline(aes(xintercept = median(SalePrice), color = &quot;median&quot;), linetype = &quot;dashed&quot;, size = .7) + scale_color_manual(name = &quot;Central Tendency&quot;, values = c(mean = &quot;#67001F&quot;, median = &quot;#01665E&quot;)) Figure 4.7: Log-Transformed SalesPrice 4.3.2 Features Highly Correlated with Sales Price Given the large number of features in the data-set, we will focus our data engineering efforts on those features which are highly correlated with our variable of interest. For continuous features, the correlation plot shows Garage Area, Great Living Room Area, First Floor SF, Total Basement SF, and Masonry Veneer Area have a strong positive correlation with Sale Price. Figure 4.8: Continous Features and Sale Price Correlation Plot 3 From the histograms we can see that many of the continuous independent variables are right skewed, similar to the response, so it might be a good idea to normalize these prior to modeling, since many algorithms perform better with normalized data because it improves the numerical stability of the model and reduces training time (Zhang_2019). Figure 4.9: Continous Features Distribution Plots 4 The correlation plot for the discrete features in 4.10 shows that YearBuilt, YearRemodAdd, GarageYearBuilt, GarageCars, FullBath, FirePlaces, and TotRmsAbvGrd are strongly and positively correlated with SalePrice: Figure 4.10: Discrete Features and Sale Price Correlation Plot 5 4.3.3 Attribute Construction from Year Features The barplot of Year Built shows a clear distinction between the number of houses built and sold in the 2000s vs the number built and sold in before that time. The distribution is left skewed, as we can see that the mean of the distribution is less than the median. Most importantly, since there are so many unique years, it might be helpful to create a new “age” variable by subtracting the year built from 2010, which is the upper limit of year sold in the data-set. And we can further refine this new age variable later on by binning age groups into buckets from old to new, thereby reducing the number of levels. Figure 4.11: Year Built vs Sales Price 6 Curiously, in the scatter plot shown in 4.12, where the dots have been colored in light blue by the YearRemodAdd feature, newer houses are displaying as having been remodeled, apparently if the house has not been remodeled the year remodeled defaults to the year built. To improve the value of this metric we can create a new binary feature for Remodeled: Yes or No. Also, it might be useful to create a feature to show how recently the house was remodeled, and bin these into categories from most recently remodeled. ggplot(data_2_train, aes(x=YearBuilt, y=SalePrice, color=(YearRemodAdd))) + geom_jitter(height = 1) Figure 4.12: Year Bult vs Sales Prie Colored by Year Remodeled 7 In order to make the year features more meaningful, we created new attributes as indicated in the table below. In addition to the building age and time since remodeled features, we added a feature to indicate if the home is a new build, since it is likely that being a new home will have an impact on the sale’s price. We also added a feature for the year the house was sold, since macro economic events, like the economic depression in 2008, could also impact the sale price. Later, we simplify some of these newly created features by binning them into categories with fewer levels. Since all of the categorical features will have to be converted into numeric via dummy coding for modeling, having fewer levels will help with model performance and dimensionality reduction. Attribute Construction from Year Features New Feature Method BldgAge 2010 - YearBuilt NewBuild If YearBuilt or YearBuilt + 1 = YrSold then NewBuild = 1 Remod If YearBuilt = YearRemodAdd then Remod = 0 TimeSinceRemod 2010 - YearRemodAdd LastSold 2010 - YrSold # Remodeled data_2[&#39;Remod&#39;] &lt;- ifelse(data_2$YearBuilt==data_2$YearRemodAdd, 0, 1) #0=No Remodeling, 1=Remodeling #New Build data_2[&#39;NewBuild&#39;] &lt;- ifelse(data_2$YearBuilt == data_2$YrSold|data_2$YearBuilt+1 == data_2$YrSold, 1,0) # Age of property based on last year of dataset data_2[&#39;BldgAge&#39;]&lt;- max(data_2$YearBuilt) - data_2$YearBuilt head(data_2[,c(&quot;YearBuilt&quot;,&quot;YrSold&quot;, &quot;BldgAge&quot;,&quot;YearRemodAdd&quot;, &quot;Remod&quot;, &quot;NewBuild&quot;)]) ## YearBuilt YrSold BldgAge YearRemodAdd Remod NewBuild ## 1 2003 2008 7 2003 0 0 ## 2 1976 2007 34 1976 0 0 ## 3 2001 2008 9 2002 1 0 ## 4 1915 2006 95 1970 1 0 ## 5 2000 2008 10 2000 0 0 ## 6 1993 2009 17 1995 1 0 4.3.4 Reducing Levels by Grouping Unrepresented Categories Another way that we can improve the data for mining is to reduce the levels in categorical variables by merging under-represented categories. For example, in the case of the fireplaces, we can see in scatterplot 4.13 that there are only a few houses that have 3 fireplaces, and the scatter plot shows that the relationship between three fireplaces and price is not much different than between two fireplaces and price, so we can merge these into one level. ggplot(data_2_train, aes(x=Fireplaces, y=SalePrice, color=(Fireplaces))) + geom_jitter(height = 1) Figure 4.13: Fireplaces Scatter Plot Grouping Fireplace Features #FirePlaces data_2$Fireplaces[data_2$Fireplaces==3]&lt;-2 data_2$Fireplaces[data_2$Fireplaces==4]&lt;-2 ggplot(data_2[1:1460,], aes(x=as.factor(Fireplaces), y=SalePrice, color=Fireplaces)) + geom_jitter(height = 1) Figure 4.14: Fireplaces After Groupping Similarly, the scatter plot in 4.15 of the “GarageCars” variable shows that few houses have more than three garages, so we could simplify the levels in this feature by keeping only three levels: 1,2, and 3+. ggplot(data_2_train, aes(x=GarageCars, y=SalePrice, color=(GarageCars))) + geom_jitter(height = 1) + labs(title=&quot;Garage vs Sale Price&quot;, subtitle=&quot;Training Data&quot;) Figure 4.15: Discrete Features Scatter Plots data_2$GarageCars[data_2$GarageCars==4]&lt;-3 data_2$GarageCars[data_2$GarageCars==5]&lt;-3 ggplot(data_2[1:1460,], aes(x=GarageCars, y=SalePrice, color=(GarageCars))) + geom_jitter(height = 1) + labs(title=&quot;GarageCars vs Sale Price&quot;, subtitle=&quot;Training Data&quot;) Figure 4.16: Garage Cars After Groupping 8 We applied this treatment to the features listed in the table below. This will make these features more robust for modeling. Reducing Levels by Grouping New Feature Method GarageCars Merge 4 and 3 Fireplaces Merge 3 and 2 Electrical Merge FuseF and FuseA Function Merge Maj1 and Mod, Min1 and Min2 Grouping Electrical Features data_full$Electrical&lt;-factor(data_full$Electrical, levels=c(&quot;Mix&quot;,&quot;FuseP&quot;,&quot;FuseF&quot;,&quot;FuseA&quot;,&quot;SBrkr&quot;), ordered=TRUE) data_2$Electrical&lt;-recode_factor(data_2$Electrical, &quot;Mix&quot;=1, &quot;FuseP&quot;=2, &quot;FuseF&quot;=3, &quot;FuseA&quot;=3, &quot;SBrkr&quot;=4) ggplot(data_2[1:1460,], aes(x=Electrical, y=SalePrice, color=Electrical)) + geom_jitter(height = 1) (#fig:GrouppingElectrical )Electrical vs Sales Price Scatterplot Grouping Functional Features data_2$Functional&lt;-recode_factor(data_2$Functional, &quot;Sal&quot;=1 , &quot;Sev&quot;=1 , &quot;Maj2&quot;=2 ,&quot;Maj1&quot;=3 ,&quot;Mod&quot;=3, &quot;Min2&quot;=4 ,&quot;Min1&quot;=4 ,&quot;Typ&quot;=5) ggplot(data_2[1:1460,], aes(x=Functional, y=SalePrice, color=(Functional))) + geom_jitter(height = 1) Figure 4.17: Functional vs Sales Price Scatterplot 4.3.5 Attribute Construction by Combining some Features to make New Features. There are four different columns related to the number of bathrooms in different areas of the home. Individually these features may not carry as mush weight as combined into a single feature for total number baths. bsmtFB&lt;-ggplot(data_2_train)+aes(BsmtFullBath)+ labs(title=&quot;Basement Full Bath&quot;, subtitle=&quot;Training Data&quot;)+ geom_bar(fill=&quot;#053061&quot;, color=&quot;#F5F5F5&quot;)+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) BsmtHB&lt;-ggplot(data_2_train)+aes(BsmtHalfBath)+ labs(title=&quot;Basement Half Bath&quot;, subtitle=&quot;Training Data&quot;)+ geom_bar(fill=&quot;#053061&quot;, color=&quot;#F5F5F5&quot;)+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) hFB&lt;-ggplot(data_2_train)+aes(FullBath)+ labs(title=&quot;Number of Full Baths in the Home&quot;, subtitle=&quot;Training Data&quot;)+ geom_bar(fill=&quot;#053061&quot;, color=&quot;#F5F5F5&quot;)+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) hhB&lt;-ggplot(data_2_train)+aes(HalfBath)+ labs(title=&quot;Number of Half Baths in the Home&quot;, subtitle=&quot;Training Data&quot;)+ geom_bar(fill=&quot;#053061&quot;, color=&quot;#F5F5F5&quot;)+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) grid.arrange(bsmtFB, BsmtHB, hFB, hhB, ncol = 2, nrow = 2) Figure 4.18: Bath Features 9 Close look at fullbath = 0 Some homes are showing as having zero baths, this might be a mistake, so it will be usefull to look closer are these examples. The nine rows which had zero full bath, did have other bathrooms in either the basement, or half bath, it is ok to leave them for now, since it will be better to combine all baths together and have one feature for the total baths. missing_bath &lt;- filter(data_2_train, FullBath ==0) bath_features &lt;- names(data_2_train)[sapply(names(data_full), function(x) str_detect(x, &quot;Bath&quot;))] data_2_train[c(54, 189, 376, 598, 635, 917, 1164,1214, 1271),c(&quot;BsmtFullBath&quot;,&quot;BsmtHalfBath&quot;,&quot;FullBath&quot;,&quot;HalfBath&quot;)] We will also combine the basement finished squared feet and the porch features. Furthermore, we created a new feature for total area by adding the living area square feet and the total basement square feet. The goal here is to create new features that might have a stronger predictive power than the individual original features. Prior to modeling, we will be removing all of the features rendered redundant by the feature creation process. The table below summarizes the features we constructed by adding the same type of feature together to form a total. New Feature Construction by Combining Existing Features New Feature Method BSmtFinSFComb BsmtFinSF1 + BsmtFinSF2 TotalArea GrLivArea + TotalBsmtSF TotalBaths FullBath + HalfBath + BsmtFullBath + BsmtHalfBath TotalPorchSF OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch data_2 &lt;- mutate(data_2, BSmtFinSFComb = BsmtFinSF1 + BsmtFinSF2, TotalArea= GrLivArea + TotalBsmtSF, TotalBaths = BsmtFullBath + (0.5*BsmtHalfBath) + FullBath+ (0.5*HalfBath), PorchSF = OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch) #To view cobined fatures #data_2[,c(&quot;X1stFlrSF&quot;,&quot;X2ndFlrSF&quot;, &quot;BSmtFinSFComb&quot;, &quot;TotalBsmtSF&quot;, &quot;GrLivArea&quot;, &quot;TotalArea&quot;)] 4.3.6 Attribute Construction by Binning Some of the categorical variables have a large number of levels. For example, “Neighborhood”, one of the features most highly correlated with sale price, has 25 different levels. ggplot(data_2_train)+ geom_boxplot(mapping=aes(x=reorder(Neighborhood, SalePrice, FUN=median), y=SalePrice), fill = &quot;#053061&quot;, color=&quot;#053061&quot;, alpha=.3,)+theme(axis.text.x = element_text(angle = 45,hjust = 1)) Figure 4.19: Neighborhood vs Sale Price Descretization and Binning-Neighboorhood Concept hirearchy discretization and binning is a way to reduce the the number of distinc values per attribute, we will group the neighborhoods together to create fewer categories. We can reference the SalePrice to find similar neighboorhoods. # Summary statistics ames_train&lt;-as.data.frame(data_2[1:1460,]) log_price&lt;-log(ames_train$SalePrice) neigh_brk&lt;-setDT(ames_train)[ , list(mean_1 = mean(SalePrice), median_1 = median(SalePrice), st_dev = sd(SalePrice), sum_gr = sum(SalePrice), range=range(SalePrice)[2] - range(SalePrice)[1]) , by = .(Neighborhood)] # Sort by median and show top, bottom top_3&lt;-kable(neigh_brk %&gt;% arrange(desc(median_1)) %&gt;% head(3), caption = &quot;Most Expensive Neighborhoods&quot;) bottom_3&lt;-kable(neigh_brk %&gt;% arrange(desc(median_1)) %&gt;% tail(3), caption = &quot;Least Expensive Neighborhoods&quot;) Table 4.8: All Neighborhoods by Median Sales Price Neighborhood mean_1 median_1 st_dev sum_gr range NridgHt 316270.62 315000 96392.545 24352838 457657 NoRidge 335295.32 301500 121412.659 13747108 565000 StoneBr 310499.00 278000 112969.677 7762475 386581 Timber 242247.45 228475 64845.652 9205403 241000 Somerst 225379.84 225500 56177.556 19382666 278848 Veenker 238772.73 218000 72369.318 2626500 222500 Crawfor 210624.73 200624 68866.395 10741861 302150 ClearCr 212565.43 200250 50231.539 5951832 198000 CollgCr 197965.77 197200 51403.666 29694866 314870 Blmngtn 194870.88 191000 30393.229 3312805 104666 NWAmes 189050.07 182900 37172.218 13800655 217300 Gilbert 192854.51 181000 35986.779 15235506 236500 SawyerW 186555.80 179900 55651.998 11006792 244000 Mitchel 156270.12 153500 36486.625 7657236 186500 NPkVill 142694.44 146000 9377.315 1284250 27500 NAmes 145847.08 140000 33075.345 32815593 257500 SWISU 142591.36 139500 32622.918 3564784 140000 Blueste 137500.00 137500 19091.883 275000 27000 Sawyer 136793.14 135000 22345.129 10122692 127617 BrkSide 124834.05 124300 40348.689 7240375 184200 Edwards 128219.70 121750 43208.616 12821970 261500 OldTown 128225.30 119000 52650.583 14489459 437100 BrDale 104493.75 106000 14330.176 1671900 42000 IDOTRR 100123.78 103000 33376.710 3704580 134600 MeadowV 98576.47 88000 23491.050 1675800 76400 Table 4.9: The Most Heterogeneous Neighborhoods Neighborhood mean_1 median_1 st_dev sum_gr range Blmngtn 194870.88 191000 30393.229 3312805 104666 MeadowV 98576.47 88000 23491.050 1675800 76400 Sawyer 136793.14 135000 22345.129 10122692 127617 Blueste 137500.00 137500 19091.883 275000 27000 BrDale 104493.75 106000 14330.176 1671900 42000 NPkVill 142694.44 146000 9377.315 1284250 27500 Grouping Neighborhoods ## Picking joint bandwidth of 17200 Figure 4.20: Spread Based on Sale Price In order to avoid data leakage we could use the “Overall Quality” feature, which shows a similar relationship with the neighborhoods, to create 5 bins for neighborhood categories ranging from low (1) to high (5). As we can see below grouping neighborhoods into 5 categorires looks similar whether we look at it by the “Overall Quality” feature or by SalePrice. Figure 4.21: Neighborhood Category by Overal Quality Figure 4.22: Neighborhood Category by SalePrice 10 In addition to the “Neighborhood” feature, we also used binning as a means to reduce the number of unique levels for the “Month” feature. For the Month feature, we ploted the months, and put them into bins according to low, medium, or high season, based on the number of houses sold. The AgeCat and RemodelFromCat are new features based of other features we contructed previously from the year features. These features contain fewer levels, which is one of our goals here, to reduce the nuber of levels in categorical features, so that the prediction model is simpler, which as mentioned before reduces the error. The table below shows the attributes constructed by binning. Attribute Construction by Binning New Feature Method AgeCat BldgAge into 4 age categories: Antique, Old, Mid, New RemodelFromCat Max year in Data (2010) - YearRemodAdd into 4 categories SeasonSale MoSold into 3 categories (Low, Mid &amp; High) NeighCat Neighborhood into 5 categories Bin the age of the house into 4 categories min(data_2$BldgAge) ## [1] 0 max(data_2$BldgAge) ## [1] 138 head(data_2[,c(&quot;BldgAge&quot;,&quot;YearBuilt&quot;)]) ## BldgAge YearBuilt ## 1 7 2003 ## 2 34 1976 ## 3 9 2001 ## 4 95 1915 ## 5 10 2000 ## 6 17 1993 age&lt;-data_2$BldgAge AgeCat&lt;- case_when(age&lt;= 9 ~ &#39;New&#39;, between(age, 10, 40) ~ &#39;Mid&#39;, between(age, 41, 70) ~ &#39;Old&#39;, age &gt;= 71 ~ &#39;Antique&#39;) data_2$AgeCat&lt;-as.numeric(factor(AgeCat, levels=c(&#39;Antique&#39;,&#39;Old&#39;, &#39;Mid&#39;, &#39;New&#39;), ordered=TRUE)) head(data_2[,c(&quot;BldgAge&quot;, &quot;AgeCat&quot;,&quot;YearBuilt&quot;)]) ## BldgAge AgeCat YearBuilt ## 1 7 4 2003 ## 2 34 3 1976 ## 3 9 4 2001 ## 4 95 1 1915 ## 5 10 3 2000 ## 6 17 3 1993 #Time when property sold based on max year of dataset data_2$LastSold &lt;- 2010 - data_2$YrSold #This might be interesting to look at from economic boom and bust effects Bin the year-since-remodeled of the house into 4 categories ##Years Since Remodeled data_2[&#39;YearRemodAdd2&#39;] &lt;- ifelse(data_2$YearBuilt == data_2$YearRemodAdd, 0, data_2$YearRemodAdd) #fix the year remodeled column head(data_2[,c(&quot;YearBuilt&quot;,&quot;YearRemodAdd&quot;, &#39;YearRemodAdd2&#39;,&#39;Remod&#39; )]) ## YearBuilt YearRemodAdd YearRemodAdd2 Remod ## 1 2003 2003 0 0 ## 2 1976 1976 0 0 ## 3 2001 2002 2002 1 ## 4 1915 1970 1970 1 ## 5 2000 2000 0 0 ## 6 1993 1995 1995 1 data_2[&#39;TimeSinceRemod&#39;] &lt;- ifelse(data_2$Remod == 1, 2010 - data_2$YearRemodAdd2,0) data_2$RemodelFromCat&lt;- case_when(data_2$TimeSinceRemod &lt;= 0 &amp; data_2$NewBuild == 1 ~ &#39;New&#39;, data_2$TimeSinceRemod &lt;= 0 &amp; data_2$YearRemodAdd2 == 0 ~ &#39;None&#39;, between(data_2$TimeSinceRemod, 0, 5) ~ &#39;Recent&#39;, between(data_2$TimeSinceRemod, 6, 10) ~ &#39;Mid&#39;, between(data_2$TimeSinceRemod, 11, 20) ~ &#39;Old&#39;, data_2$TimeSinceRemod &gt;= 20 ~ &#39;Outdated&#39;) data_2$RemodelFromCat&lt;-factor(data_2$RemodelFromCat, levels=c(&#39;None&#39;,&#39;Outdated&#39;,&#39;Old&#39;, &#39;Mid&#39;,&#39;Recent&#39;, &#39;New&#39;), ordered=TRUE) ggplot(data_2, aes(x=RemodelFromCat)) + geom_bar(fill = &#39;#053061&#39;) + geom_text(aes(label=..count..), stat=&#39;count&#39;, vjust = -.5) + theme_minimal() data_2$RemodelFromCat&lt;-as.integer(data_2$RemodelFromCat) head(data_2[,c(&quot;YrSold&quot;,&quot;YearBuilt&quot;,&quot;YearRemodAdd&quot;, &quot;YearRemodAdd2&quot;,&quot;Remod&quot;, &#39;TimeSinceRemod&#39;, &#39;BldgAge&#39;, &#39;NewBuild&#39;)]) ## YrSold YearBuilt YearRemodAdd YearRemodAdd2 Remod TimeSinceRemod BldgAge ## 1 2008 2003 2003 0 0 0 7 ## 2 2007 1976 1976 0 0 0 34 ## 3 2008 2001 2002 2002 1 8 9 ## 4 2006 1915 1970 1970 1 40 95 ## 5 2008 2000 2000 0 0 0 10 ## 6 2009 1993 1995 1995 1 15 17 ## NewBuild ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 Bin the sale month into seasons from low season to high season based on sales ggplot(data_2, aes(x=MoSold)) + geom_bar(fill = &#39;#053061&#39;) + geom_text(aes(label=..count..), stat=&#39;count&#39;, vjust = -.5) + theme_minimal() data_2$SeasonSale &lt;- mapvalues(data_2$MoSold, from = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;), to = c(&quot;LowSeason&quot;, &quot;LowSeason&quot;, &quot;MidSeason&quot;,&#39;MidSeason&#39;, &#39;HighSeason&#39;,&#39;HighSeason&#39;,&#39;HighSeason&#39;,&#39;MidSeason&#39;, &#39;MidSeason&#39;,&#39;MidSeason&#39;,&#39;LowSeason&#39;,&#39;LowSeason&#39;)) head(data_2[,c(&quot;MoSold&quot;,&quot;SeasonSale&quot;)]) ## MoSold SeasonSale ## 1 2 LowSeason ## 2 5 HighSeason ## 3 9 MidSeason ## 4 2 LowSeason ## 5 12 LowSeason ## 6 10 MidSeason data_2$SeasonSale&lt;-as.numeric(factor(data_2$SeasonSale,levels=c(&#39;LowSeason&#39;,&#39;MidSeason&#39;, &#39;HighSeason&#39;),ordered=TRUE)) head(data_2[,c(&quot;MoSold&quot;,&quot;SeasonSale&quot;)]) ## MoSold SeasonSale ## 1 2 1 ## 2 5 3 ## 3 9 2 ## 4 2 1 ## 5 12 1 ## 6 10 2 Update Categorical Features to have numerical value data_2$Utilities &lt;-as.numeric(data_2$Utilities) data_2$LandSlope&lt;-as.numeric(data_2$LandSlope) data_2$ExterQual &lt;-as.numeric(data_2$ExterQual) data_2$ExterCond &lt;-as.numeric(data_2$ExterCond) data_2$BsmtQual &lt;-as.numeric(data_2$BsmtQual) data_2$BsmtCond &lt;-as.numeric(data_2$BsmtCond) data_2$BsmtExposure &lt;-as.numeric(data_2$BsmtExposure) data_2$BsmtFinType1 &lt;-as.numeric(data_2$BsmtFinType1) data_2$BsmtFinType2 &lt;-as.numeric(data_2$BsmtFinType2) data_2$HeatingQC &lt;-as.numeric(data_2$HeatingQC) data_2$CentralAir &lt;-as.numeric(data_2$CentralAir) data_2$Electrical &lt;-as.numeric(data_2$Electrical) data_2$KitchenQual &lt;-as.numeric(data_2$KitchenQual) data_2$Functional &lt;-as.numeric(data_2$Functional) data_2$FireplaceQu &lt;-as.numeric(data_2$FireplaceQu) data_2$GarageFinish &lt;-as.numeric(data_2$GarageFinish) data_2$GarageQual &lt;-as.numeric(data_2$GarageQual) data_2$GarageCond &lt;-as.numeric(data_2$GarageCond) data_2$PavedDrive &lt;-as.numeric(data_2$PavedDrive) data_2$PoolQC &lt;-as.numeric(data_2$PoolQC) data_2$Fence &lt;-as.numeric(data_2$Fence) data_2$OverallQual&lt;-as.numeric(data_2$OverallQual) data_2$OverallCond &lt;-as.numeric(data_2$OverallCond) 4.3.7 New Variables by Interactions The last type of feature engineering we attempted is attribute construction from interactions by multiplying certain features listed in the table below. New Variables from Interactions New Feature Method OverallScore GarageQual * GarageCond GarageScore OverallQual * OverallCond ExterScore ExterQual * ExterCond KitchenScore KitchenAbvGr * KitchenQual GarageGrade GarageArea * GarageQual data_2 &lt;- mutate(data_2,GarageScore = GarageQual * GarageCond, OverallScore= OverallQual * OverallCond, ExterScore = ExterQual * ExterCond, KitchenScore = KitchenAbvGr * KitchenQual, GarageGrade = GarageArea * GarageQual) 4.4 Dimensionality and Numerosity Reduction The last major step left in pre-processing is dimensionality and numerosity reduction through the following steps: Delete Redundant Features Due to Data Engineering Delete Outliers Delete Features with Near Zero Variability data_2 &lt;- subset(data_2, select = -c(BsmtFinSF1,BsmtFinSF2,BsmtFullBath,GarageArea,GarageQual, BsmtHalfBath,FullBath,HalfBath,X1stFlrSF, X2ndFlrSF, OpenPorchSF,EnclosedPorch,X3SsnPorch,ScreenPorch, GarageQual,GarageCond,ExterCond,KitchenAbvGr,KitchenQual, BsmtFinType1,BsmtFinType2,BsmtCond,BsmtQual, Neighborhood,YearBuilt,YrSold, BldgAge,YearRemodAdd, MoSold, GarageYrBlt,YearRemodAdd2, Remod, TimeSinceRemod,PoolQC)) 4.4.1 Outliers According to the data collector: “There are 5 observations that an instructor may wish to remove from the data set before giving it to students. Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations)” As we can see from the scatterplot 4 of the outlier are in the training set, which means that one is in the test set, because we are using the test set to submit to Kaggle.com for evalueation, we can not remove any rows from the test set (or it would be incomplete). Here, we will only remove the outliers indicated by the data collector in the training set only. Additional outlier detection could be useful in the feature. highlight_df &lt;- data_2_train %&gt;% filter(GrLivArea&gt;=4000) outlier_plot2&lt;-ggplot(data_2_train, aes(x =GrLivArea , y = SalePrice, color=&quot;red&quot;)) + ggtitle(&quot;Outliers: Living Area vs SalesPrice&quot;)+ geom_point(color=&quot;#053061&quot;, alpha=.5)+ geom_point(data=highlight_df, aes(x=GrLivArea,y=SalePrice, color=&quot;red&quot;))+ theme( panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;#053061&quot;))+ scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) outlier_plot2$labels$colour &lt;- &quot;Outliers&quot; outlier_plot2 For now, we are only deleting the outliers in the training set. highlight_df$SalePrice ## [1] 184750 755000 745000 160000 data_2 &lt;- data_2[-c(524, 692, 1183, 1299),] View data shape after feature engineering length(data_2) ## [1] 67 #sapply(data_2 , class) names(data_2) ## [1] &quot;Id&quot; &quot;MSSubClass&quot; &quot;MSZoning&quot; &quot;LotFrontage&quot; ## [5] &quot;LotArea&quot; &quot;Street&quot; &quot;Alley&quot; &quot;LotShape&quot; ## [9] &quot;LandContour&quot; &quot;Utilities&quot; &quot;LotConfig&quot; &quot;LandSlope&quot; ## [13] &quot;Condition1&quot; &quot;Condition2&quot; &quot;BldgType&quot; &quot;HouseStyle&quot; ## [17] &quot;OverallQual&quot; &quot;OverallCond&quot; &quot;RoofStyle&quot; &quot;RoofMatl&quot; ## [21] &quot;Exterior1st&quot; &quot;Exterior2nd&quot; &quot;MasVnrType&quot; &quot;MasVnrArea&quot; ## [25] &quot;ExterQual&quot; &quot;Foundation&quot; &quot;BsmtExposure&quot; &quot;BsmtUnfSF&quot; ## [29] &quot;TotalBsmtSF&quot; &quot;Heating&quot; &quot;HeatingQC&quot; &quot;CentralAir&quot; ## [33] &quot;Electrical&quot; &quot;LowQualFinSF&quot; &quot;GrLivArea&quot; &quot;BedroomAbvGr&quot; ## [37] &quot;TotRmsAbvGrd&quot; &quot;Functional&quot; &quot;Fireplaces&quot; &quot;FireplaceQu&quot; ## [41] &quot;GarageType&quot; &quot;GarageFinish&quot; &quot;GarageCars&quot; &quot;PavedDrive&quot; ## [45] &quot;WoodDeckSF&quot; &quot;PoolArea&quot; &quot;Fence&quot; &quot;MiscFeature&quot; ## [49] &quot;MiscVal&quot; &quot;SaleType&quot; &quot;SaleCondition&quot; &quot;SalePrice&quot; ## [53] &quot;NewBuild&quot; &quot;BSmtFinSFComb&quot; &quot;TotalArea&quot; &quot;TotalBaths&quot; ## [57] &quot;PorchSF&quot; &quot;Neigh_Cat&quot; &quot;AgeCat&quot; &quot;LastSold&quot; ## [61] &quot;RemodelFromCat&quot; &quot;SeasonSale&quot; &quot;GarageScore&quot; &quot;OverallScore&quot; ## [65] &quot;ExterScore&quot; &quot;KitchenScore&quot; &quot;GarageGrade&quot; Export Pre-processed Data write.csv(data_2,&quot;AmesDataClean.csv&quot;, row.names = FALSE) References "],["modeling.html", "Chapter 5 Modeling 5.1 Multivariate Linear Regression Model 5.2 Model Selection 5.3 Regularized Regression Models 5.4 Training the Regularized Regression Models 5.5 Evaluating the Regularized Regression Models’ Performance 5.6 Training XGBoost Model with Feature Selection 5.7 Evaluating XGBoost Model’s Performance", " Chapter 5 Modeling Clean Data The original data, the “Ames Housing Data-set”, provided at Kaggle.com, contains 2919 observations and 80 explanatory variables. However, from this point on we are starting with an already cleaned and pre-processed data-set based on previous work, which consists of 2919 observations and 67 variables. After transforming the categorical variables to numerical with dummy coding, the columns increase to 167 columns. ## [1] 2915 67 ## Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour ## 1 1 60 RL 65 8450 Pave None Reg Lvl ## 2 2 20 RL 80 9600 Pave None Reg Lvl ## 3 3 60 RL 68 11250 Pave None IR1 Lvl ## 4 4 70 RL 60 9550 Pave None IR1 Lvl ## 5 5 60 RL 84 14260 Pave None IR1 Lvl ## 6 6 50 RL 85 14115 Pave None IR1 Lvl ## Utilities LotConfig LandSlope Condition1 Condition2 BldgType ## 1 4 Inside 1 Norm Norm 1Fam ## 2 4 FR2 1 Feedr Norm 1Fam ## 3 4 Inside 1 Norm Norm 1Fam ## 4 4 Corner 1 Norm Norm 1Fam ## 5 4 FR2 1 Norm Norm 1Fam ## 6 4 Inside 1 Norm Norm 1Fam Dummy Codding: One of the last pre-processing steps before modeling #Convert Categorical Features to numeric with dummy codding using function dummyVars() from Caret package. factor_var &lt;- which(lapply(ames_clean, class) == &quot;factor&quot;) data_temp&lt;-ames_clean dummy&lt;- dummyVars(&quot; ~ MSSubClass + MSZoning +Street + Alley+ LotShape + LandContour+ LotConfig + Condition1 + Condition2 + BldgType + HouseStyle + RoofStyle + RoofMatl+ Heating + Exterior1st + Exterior2nd + MasVnrType + Foundation + GarageType + SaleType + SaleCondition + MiscFeature&quot; , data = data_temp, fullRank = TRUE) pred&lt;- data.frame (predict(dummy, data_temp)) data_final&lt;-cbind(ames_clean[,-factor_var], pred) Split pre-processed dataset into Train and Test sets for modeling* train_1&lt;-data_final[1:1456,-1] train_x&lt;-select(train_1, -SalePrice) train_y&lt;-train_1$SalePrice test_1&lt;-data_final[1457:2915,-1] test_1&lt;-select(test_1, -SalePrice) train_1$SalePrice&lt;- log(train_1$SalePrice) test_ID&lt;-data_final[1457:2915,1] 5.1 Multivariate Linear Regression Model We start by fitting a multivariate linear regression model to illustrate the issue of multicollinearity present in our data. The model returns a warning: “Coefficients: (4 not defined because of singularities)”. This is due to variables that have perfect multicollinearity, so we find those variables and remove them. #Fit Linear Model ml_model&lt;- lm(SalePrice ~ ., train_1) summary(ml_model) ## ## Call: ## lm(formula = SalePrice ~ ., data = train_1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69436 -0.05306 0.00131 0.05573 0.50403 ## ## Coefficients: (4 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.495e+00 5.306e-01 16.010 &lt; 2e-16 *** ## MSSubClass -5.914e-04 3.658e-04 -1.617 0.106167 ## LotFrontage 3.249e-04 1.870e-04 1.737 0.082545 . ## LotArea 1.340e-06 4.116e-07 3.256 0.001159 ** ## Utilities 1.214e-01 5.864e-02 2.071 0.038547 * ## LandSlope -2.158e-03 1.565e-02 -0.138 0.890348 ## OverallQual 2.964e-02 1.235e-02 2.400 0.016534 * ## OverallCond 1.903e-02 1.236e-02 1.540 0.123864 ## MasVnrArea 2.959e-05 2.518e-05 1.175 0.240064 ## ExterQual 2.697e-02 1.321e-02 2.043 0.041303 * ## BsmtExposure 1.136e-02 3.740e-03 3.037 0.002435 ** ## BsmtUnfSF -7.757e-05 9.950e-06 -7.796 1.30e-14 *** ## TotalBsmtSF 1.690e-04 1.645e-05 10.275 &lt; 2e-16 *** ## HeatingQC 1.582e-02 4.236e-03 3.734 0.000197 *** ## CentralAir 7.256e-02 1.670e-02 4.346 1.49e-05 *** ## Electrical -5.148e-03 1.162e-02 -0.443 0.657835 ## LowQualFinSF -3.450e-05 7.883e-05 -0.438 0.661748 ## GrLivArea 2.430e-04 1.816e-05 13.381 &lt; 2e-16 *** ## BedroomAbvGr -7.802e-03 5.881e-03 -1.327 0.184878 ## TotRmsAbvGrd 6.069e-03 4.117e-03 1.474 0.140635 ## Functional 6.326e-02 8.454e-03 7.482 1.34e-13 *** ## Fireplaces 1.334e-02 1.039e-02 1.284 0.199221 ## FireplaceQu 4.493e-03 3.714e-03 1.210 0.226548 ## GarageFinish 6.688e-03 5.304e-03 1.261 0.207606 ## GarageCars 2.726e-02 1.003e-02 2.717 0.006672 ** ## PavedDrive 1.629e-02 7.232e-03 2.252 0.024482 * ## WoodDeckSF 7.376e-05 2.624e-05 2.811 0.005019 ** ## PoolArea 1.883e-04 9.372e-05 2.009 0.044747 * ## Fence -2.641e-03 2.698e-03 -0.979 0.327872 ## MiscVal 1.259e-05 2.709e-05 0.465 0.642314 ## NewBuild -2.413e-03 1.961e-02 -0.123 0.902113 ## BSmtFinSFComb NA NA NA NA ## TotalArea NA NA NA NA ## TotalBaths 2.609e-02 6.748e-03 3.867 0.000116 *** ## PorchSF 1.649e-04 3.176e-05 5.194 2.39e-07 *** ## Neigh_Cat 3.822e-02 4.766e-03 8.019 2.37e-15 *** ## AgeCat 2.631e-02 7.014e-03 3.750 0.000184 *** ## LastSold 2.458e-03 2.287e-03 1.075 0.282674 ## RemodelFromCat 3.273e-03 2.633e-03 1.243 0.214018 ## SeasonSale 6.639e-03 4.002e-03 1.659 0.097356 . ## GarageScore 3.546e-03 2.107e-03 1.683 0.092565 . ## OverallScore 3.778e-03 2.132e-03 1.772 0.076620 . ## ExterScore -6.642e-03 2.923e-03 -2.273 0.023202 * ## KitchenScore 3.653e-03 5.379e-03 0.679 0.497210 ## GarageGrade 1.733e-05 8.111e-06 2.136 0.032849 * ## MSSubClass.1 NA NA NA NA ## MSZoning.FV 4.157e-01 4.385e-02 9.479 &lt; 2e-16 *** ## MSZoning.RH 4.071e-01 4.882e-02 8.339 &lt; 2e-16 *** ## MSZoning.RL 4.066e-01 4.074e-02 9.979 &lt; 2e-16 *** ## MSZoning.RM 3.531e-01 4.056e-02 8.707 &lt; 2e-16 *** ## Street.Pave 3.815e-02 5.210e-02 0.732 0.464122 ## Alley.None 2.037e-02 1.789e-02 1.138 0.255168 ## Alley.Pave 3.381e-02 2.662e-02 1.270 0.204237 ## LotShape.IR2 2.498e-02 1.872e-02 1.334 0.182286 ## LotShape.IR3 5.335e-05 3.947e-02 0.001 0.998922 ## LotShape.Reg -4.993e-04 7.190e-03 -0.069 0.944651 ## LandContour.HLS 2.744e-02 2.235e-02 1.227 0.219889 ## LandContour.Low -1.192e-02 2.806e-02 -0.425 0.670947 ## LandContour.Lvl -4.264e-03 1.629e-02 -0.262 0.793621 ## LotConfig.CulDSac 3.111e-02 1.530e-02 2.033 0.042280 * ## LotConfig.FR2 -2.522e-02 1.802e-02 -1.400 0.161849 ## LotConfig.FR3 -6.387e-02 5.754e-02 -1.110 0.267177 ## LotConfig.Inside -7.316e-03 8.140e-03 -0.899 0.368956 ## Condition1.Feedr 4.951e-02 2.187e-02 2.263 0.023770 * ## Condition1.Norm 9.994e-02 1.810e-02 5.521 4.07e-08 *** ## Condition1.PosA 5.640e-02 4.502e-02 1.253 0.210481 ## Condition1.PosN 8.356e-02 3.281e-02 2.547 0.010983 * ## Condition1.RRAe 5.565e-03 3.824e-02 0.146 0.884311 ## Condition1.RRAn 8.021e-02 2.999e-02 2.674 0.007581 ** ## Condition1.RRNe 4.897e-02 7.912e-02 0.619 0.536051 ## Condition1.RRNn 1.060e-01 5.692e-02 1.862 0.062770 . ## Condition2.Feedr 1.046e-01 1.024e-01 1.021 0.307270 ## Condition2.Norm 6.743e-02 8.762e-02 0.770 0.441688 ## Condition2.PosA 1.426e-01 1.524e-01 0.936 0.349701 ## Condition2.PosN -5.864e-02 1.440e-01 -0.407 0.683887 ## Condition2.RRAe -2.268e-01 2.909e-01 -0.780 0.435698 ## Condition2.RRAn -5.888e-02 1.413e-01 -0.417 0.676949 ## Condition2.RRNn 4.416e-02 1.197e-01 0.369 0.712312 ## BldgType.2fmCon 3.452e-02 5.406e-02 0.639 0.523232 ## BldgType.Duplex -1.800e-02 2.965e-02 -0.607 0.543816 ## BldgType.Twnhs -1.806e-02 4.290e-02 -0.421 0.673793 ## BldgType.TwnhsE 2.163e-02 3.905e-02 0.554 0.579707 ## HouseStyle.1.5Unf 2.946e-02 3.300e-02 0.893 0.372054 ## HouseStyle.1Story -2.047e-02 1.639e-02 -1.248 0.212096 ## HouseStyle.2.5Fin -8.517e-02 5.248e-02 -1.623 0.104827 ## HouseStyle.2.5Unf 1.737e-02 3.999e-02 0.434 0.664057 ## HouseStyle.2Story -9.424e-03 1.388e-02 -0.679 0.497180 ## HouseStyle.SFoyer 2.553e-03 2.659e-02 0.096 0.923525 ## HouseStyle.SLvl 1.744e-02 2.311e-02 0.755 0.450630 ## RoofStyle.Gable 1.405e-02 8.105e-02 0.173 0.862408 ## RoofStyle.Gambrel 1.347e-02 8.858e-02 0.152 0.879136 ## RoofStyle.Hip 1.969e-02 8.133e-02 0.242 0.808764 ## RoofStyle.Mansard 6.104e-02 9.486e-02 0.644 0.520004 ## RoofStyle.Shed 2.584e-01 1.521e-01 1.699 0.089570 . ## RoofMatl.Membran 1.996e-01 1.378e-01 1.449 0.147622 ## RoofMatl.Metal 5.291e-02 1.393e-01 0.380 0.704163 ## RoofMatl.Roll -7.097e-02 1.147e-01 -0.619 0.536230 ## RoofMatl.Tar.Grv 3.326e-02 8.162e-02 0.408 0.683676 ## RoofMatl.WdShake -9.881e-02 6.704e-02 -1.474 0.140752 ## RoofMatl.WdShngl 1.153e-01 5.264e-02 2.190 0.028723 * ## Heating.GasA 5.892e-02 1.105e-01 0.533 0.593931 ## Heating.GasW 1.278e-01 1.137e-01 1.124 0.261192 ## Heating.Grav -4.177e-02 1.178e-01 -0.355 0.722974 ## Heating.OthW 1.040e-02 1.369e-01 0.076 0.939482 ## Heating.Wall 1.247e-01 1.276e-01 0.977 0.328685 ## Exterior1st.AsphShn -2.263e-02 1.493e-01 -0.152 0.879574 ## Exterior1st.BrkComm -3.287e-01 1.107e-01 -2.969 0.003044 ** ## Exterior1st.BrkFace 4.319e-02 5.494e-02 0.786 0.432012 ## Exterior1st.CBlock -1.270e-01 1.143e-01 -1.111 0.266780 ## Exterior1st.CemntBd -4.028e-02 8.327e-02 -0.484 0.628668 ## Exterior1st.HdBoard -4.256e-02 5.534e-02 -0.769 0.442007 ## Exterior1st.ImStucc -1.009e-01 1.264e-01 -0.798 0.424812 ## Exterior1st.MetalSd -1.508e-02 6.326e-02 -0.238 0.811585 ## Exterior1st.Plywood -5.533e-02 5.467e-02 -1.012 0.311643 ## Exterior1st.Stone -3.110e-02 1.052e-01 -0.296 0.767641 ## Exterior1st.Stucco -1.502e-02 6.068e-02 -0.247 0.804565 ## Exterior1st.VinylSd -3.435e-02 5.891e-02 -0.583 0.559919 ## Exterior1st.Wd.Sdng -7.155e-02 5.309e-02 -1.348 0.177940 ## Exterior1st.WdShing -2.734e-02 5.738e-02 -0.476 0.633872 ## Exterior2nd.AsphShn 5.852e-02 9.899e-02 0.591 0.554476 ## Exterior2nd.Brk.Cmn 1.229e-01 7.388e-02 1.664 0.096441 . ## Exterior2nd.BrkFace 1.025e-03 5.786e-02 0.018 0.985863 ## Exterior2nd.CBlock NA NA NA NA ## Exterior2nd.CmentBd 7.596e-02 8.275e-02 0.918 0.358781 ## Exterior2nd.HdBoard 4.847e-02 5.396e-02 0.898 0.369303 ## Exterior2nd.ImStucc 5.815e-02 6.430e-02 0.904 0.366003 ## Exterior2nd.MetalSd 4.833e-02 6.240e-02 0.775 0.438741 ## Exterior2nd.Other 2.814e-03 1.221e-01 0.023 0.981614 ## Exterior2nd.Plywood 5.665e-02 5.234e-02 1.082 0.279266 ## Exterior2nd.Stone 1.793e-02 7.471e-02 0.240 0.810416 ## Exterior2nd.Stucco 7.762e-03 6.024e-02 0.129 0.897496 ## Exterior2nd.VinylSd 5.862e-02 5.735e-02 1.022 0.306916 ## Exterior2nd.Wd.Sdng 7.747e-02 5.208e-02 1.487 0.137145 ## Exterior2nd.Wd.Shng 3.281e-02 5.468e-02 0.600 0.548544 ## MasVnrType.BrkFace 3.287e-02 3.006e-02 1.093 0.274443 ## MasVnrType.None 3.255e-02 3.025e-02 1.076 0.282046 ## MasVnrType.Stone 6.150e-02 3.190e-02 1.928 0.054062 . ## Foundation.CBlock 2.373e-02 1.352e-02 1.755 0.079556 . ## Foundation.PConc 5.838e-02 1.486e-02 3.929 8.99e-05 *** ## Foundation.Slab 1.778e-02 3.359e-02 0.529 0.596572 ## Foundation.Stone 6.588e-02 4.885e-02 1.349 0.177674 ## Foundation.Wood -1.143e-01 6.609e-02 -1.729 0.084071 . ## GarageType.Attchd 7.806e-02 4.900e-02 1.593 0.111427 ## GarageType.Basment 8.750e-02 5.646e-02 1.550 0.121436 ## GarageType.BuiltIn 8.486e-02 5.100e-02 1.664 0.096322 . ## GarageType.CarPort 8.145e-02 6.365e-02 1.280 0.200876 ## GarageType.Detchd 8.185e-02 4.877e-02 1.678 0.093563 . ## GarageType.None 9.647e-02 5.876e-02 1.642 0.100887 ## SaleType.Con 1.265e-01 8.021e-02 1.578 0.114889 ## SaleType.ConLD 1.217e-01 4.362e-02 2.790 0.005349 ** ## SaleType.ConLI -2.227e-02 5.275e-02 -0.422 0.672925 ## SaleType.ConLw 8.338e-03 5.422e-02 0.154 0.877812 ## SaleType.CWD 1.003e-01 5.874e-02 1.708 0.087805 . ## SaleType.New 1.630e-01 6.970e-02 2.338 0.019529 * ## SaleType.Oth 9.708e-02 6.704e-02 1.448 0.147865 ## SaleType.WD -3.658e-03 1.893e-02 -0.193 0.846788 ## SaleCondition.AdjLand 7.040e-02 6.259e-02 1.125 0.260898 ## SaleCondition.Alloca 6.623e-02 3.841e-02 1.724 0.084886 . ## SaleCondition.Family 1.107e-02 2.787e-02 0.397 0.691399 ## SaleCondition.Normal 6.958e-02 1.305e-02 5.330 1.15e-07 *** ## SaleCondition.Partial -4.676e-02 6.687e-02 -0.699 0.484474 ## MiscFeature.None 1.397e-01 4.332e-01 0.322 0.747158 ## MiscFeature.Othr 8.469e-02 3.997e-01 0.212 0.832240 ## MiscFeature.Shed 1.262e-01 4.151e-01 0.304 0.761101 ## MiscFeature.TenC -1.627e-01 4.020e-01 -0.405 0.685704 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1062 on 1295 degrees of freedom ## Multiple R-squared: 0.936, Adjusted R-squared: 0.9281 ## F-statistic: 118.4 on 160 and 1295 DF, p-value: &lt; 2.2e-16 #identify the linearly dependent variables ld_vars &lt;- attributes(alias(ml_model)$Complete)$dimnames[[1]] #remove the linearly dependent variables variables train_ld&lt;-select(train_1, -all_of(ld_vars)) Then, we fit the model again, and take a closer look at the the Variance Inflation Factors (VIFs), which measure extent to which a predictor is correlated with the other predictor variables. As we can see in the output, we find high levels of multicollinearity with VIF values greater than 10 in many cases. Ideally, we would like to keep the VIF values below 5. #run model again ml_model_new &lt;-lm(SalePrice ~ ., train_ld) Inspect the Variance Inflation Factors vif(ml_model_new) ## MSSubClass LotFrontage LotArea ## 30.983647 2.470684 2.125854 ## Utilities LandSlope OverallQual ## 1.219122 2.417453 36.931049 ## OverallCond MasVnrArea ExterQual ## 24.473003 2.563339 7.317738 ## BsmtExposure BsmtUnfSF TotalBsmtSF ## 2.038810 2.498255 5.929023 ## HeatingQC CentralAir Electrical ## 2.133314 2.195581 1.561926 ## LowQualFinSF GrLivArea BedroomAbvGr ## 1.901055 10.507752 2.970499 ## TotRmsAbvGrd Functional Fireplaces ## 5.681774 1.446358 5.567577 ## FireplaceQu GarageFinish GarageCars ## 5.826712 2.885926 7.079632 ## PavedDrive WoodDeckSF PoolArea ## 1.668464 1.392981 1.419251 ## Fence MiscVal NewBuild ## 1.362146 23.381710 4.911904 ## TotalBaths PorchSF Neigh_Cat ## 3.557811 1.431181 3.325601 ## AgeCat LastSold RemodelFromCat ## 6.724464 1.193224 2.642765 ## SeasonSale GarageScore OverallScore ## 1.137311 8.133184 49.407995 ## ExterScore KitchenScore GarageGrade ## 4.853052 2.737940 6.403865 ## MSZoning.FV MSZoning.RH MSZoning.RL ## 10.592837 3.344599 35.842404 ## MSZoning.RM Street.Pave Alley.None ## 27.045870 1.438511 2.422478 ## Alley.Pave LotShape.IR2 LotShape.IR3 ## 2.503946 1.238556 1.235801 ## LotShape.Reg LandContour.HLS LandContour.Low ## 1.546704 2.139764 2.451706 ## LandContour.Lvl LotConfig.CulDSac LotConfig.FR2 ## 3.112040 1.826317 1.309673 ## LotConfig.FR3 LotConfig.Inside Condition1.Feedr ## 1.171462 1.718256 3.208643 ## Condition1.Norm Condition1.PosA Condition1.PosN ## 4.972023 1.429983 1.697261 ## Condition1.RRAe Condition1.RRAn Condition1.RRNe ## 1.415759 2.037224 1.108994 ## Condition1.RRNn Condition2.Feedr Condition2.Norm ## 1.431950 5.558168 9.441338 ## Condition2.PosA Condition2.PosN Condition2.RRAe ## 2.058543 1.837783 7.498325 ## Condition2.RRAn Condition2.RRNn BldgType.2fmCon ## 1.769292 2.539791 7.865580 ## BldgType.Duplex BldgType.Twnhs BldgType.TwnhsE ## 3.909723 6.811730 14.211555 ## HouseStyle.1.5Unf HouseStyle.1Story HouseStyle.2.5Fin ## 1.339025 8.676608 1.943176 ## HouseStyle.2.5Unf HouseStyle.2Story HouseStyle.SFoyer ## 1.548757 5.250288 2.261303 ## HouseStyle.SLvl RoofStyle.Gable RoofStyle.Gambrel ## 2.941887 144.148227 7.597002 ## RoofStyle.Hip RoofStyle.Mansard RoofStyle.Shed ## 133.758957 5.559546 4.097060 ## RoofMatl.Membran RoofMatl.Metal RoofMatl.Roll ## 1.682526 1.720093 1.166139 ## RoofMatl.Tar.Grv RoofMatl.WdShake RoofMatl.WdShngl ## 6.450158 1.986501 1.224843 ## Heating.GasA Heating.GasW Heating.Grav ## 33.888788 20.383640 8.574881 ## Heating.OthW Heating.Wall Exterior1st.AsphShn ## 3.320637 5.761418 1.976400 ## Exterior1st.BrkComm Exterior1st.BrkFace Exterior1st.CBlock ## 2.172081 12.927967 1.157706 ## Exterior1st.CemntBd Exterior1st.HdBoard Exterior1st.ImStucc ## 35.375768 50.914795 1.417138 ## Exterior1st.MetalSd Exterior1st.Plywood Exterior1st.Stone ## 66.278376 26.505523 1.961641 ## Exterior1st.Stucco Exterior1st.VinylSd Exterior1st.Wd.Sdng ## 7.709559 102.465002 44.025069 ## Exterior1st.WdShing Exterior2nd.AsphShn Exterior2nd.Brk.Cmn ## 7.457993 2.601873 3.372932 ## Exterior2nd.BrkFace Exterior2nd.CmentBd Exterior2nd.HdBoard ## 7.294757 34.379097 45.681446 ## Exterior2nd.ImStucc Exterior2nd.MetalSd Exterior2nd.Other ## 3.280025 63.046316 1.321348 ## Exterior2nd.Plywood Exterior2nd.Stone Exterior2nd.Stucco ## 31.136379 2.467118 7.909456 ## Exterior2nd.VinylSd Exterior2nd.Wd.Sdng Exterior2nd.Wd.Shng ## 96.147013 40.984547 9.813104 ## MasVnrType.BrkFace MasVnrType.None MasVnrType.Stone ## 24.733962 28.401976 10.387808 ## Foundation.CBlock Foundation.PConc Foundation.Slab ## 5.806813 7.031170 2.361712 ## Foundation.Stone Foundation.Wood GarageType.Attchd ## 1.264687 1.159983 74.698954 ## GarageType.Basment GarageType.BuiltIn GarageType.CarPort ## 5.301258 18.867666 3.214034 ## GarageType.Detchd GarageType.None SaleType.Con ## 59.952842 23.426135 1.139684 ## SaleType.ConLD SaleType.ConLI SaleType.ConLw ## 1.509614 1.229721 1.299290 ## SaleType.CWD SaleType.New SaleType.Oth ## 1.220638 47.439700 1.193552 ## SaleType.WD SaleCondition.AdjLand SaleCondition.Alloca ## 5.274011 1.385924 1.557201 ## SaleCondition.Family SaleCondition.Normal SaleCondition.Partial ## 1.359023 3.217916 44.657412 ## MiscFeature.None MiscFeature.Othr MiscFeature.Shed ## 865.674765 28.304742 723.737076 ## MiscFeature.TenC ## 14.326448 Dealing with Multicollinearity The curse of dimensionality refers to the phenomenon that many types of data analysis become significantly harder as the dimensionality of the data increases (Tan et al. (2019)). As more variables get added and the complexity of the patterns increase, the training of the model becomes more time consuming and the predictive power decreases. Multicollinearity for example, is a common problem in high‐dimensional data. Multicollinearity occurs when two or more predictor variables are highly correlated, and becomes a problem in many prediction settings. It is specially problematic in regression, since one of the assumptions of linear regression is the absence of multicollinearity and auto-correlation. As the number of features grow, regression models tend to over-fit the training data, causing the sample error to increase. Some approaches for dimensionality reduction are linear algebra based techniques such as Principal Component Analysis(PCA), which finds new attributes (principal components) that are linear combinations of the original attributes orthogonal to each other, and which capture the maximum amount of variation in the data. Another approach is a filter method, where the features are selected prior to modeling for their statistical value to the model, for example when features are selected for their correlation value with the predicted variable. Wrapper, methods are also popular, these include Forward and Backward Elimination. There are also some algorithms that have their built in functions for feature selection, like Lasso regression. This analysis deals with some of the issues that arise from high dimensionality by implementing regularized regression using the glmnet package (Friedman, Hastie, and Tibshirani 2010). We will also implement a wrapper method with a model called Boruta (Kursa and Rudnicki 2010) to do feature selection prior to fitting a gradient boosted model with the xgboost package(Chen et al. 2021). 5.2 Model Selection One of the ways to deal with high multicollinearity is through regularization. Regularization is a regression technique, which limits, regulates or shrinks the estimated coefficient towards zero, which can reduce the variance and decrease out of sample error (Boehmke and Greenwell 2020). This technique does not encourage learning of more complex models, and so it avoids the risk of over-fitting. I want to note that I referenced heavily the code in the digital book :“Hands-On Machine Learning with R” by Bradley Boehmke &amp; Brandon Greenwell (Boehmke and Greenwell 2020) for the modeling portion of the Regularized regression models. Three regularization methods that help with collinearity and over-fitting are: Lasso, penalizes the number of non-zero coefficients Ridge, penalizes the absolute magnitude of the coefficients Elastic Net, a mixed method closely linked to lasso and ridge regression In addition to the methods listed above, we will also try a different approach using feature subset selection with the Boruta package (Kursa and Rudnicki 2010) prior to fitting a gradient boosted tree, with the XGBoost package. Model Selection Model Model Type Tuning Parameters Lasso Regularization lambda Ridge Regularization lambda Elastic Regularization lambda xgboost Extreme Gradient Boosting general &amp; tree booster parameters 5.3 Regularized Regression Models The lasso model penalty is controlled by setting alpha=1, likewise the ridge penalty is controlled by alpha=0. The elastic-net penalty is controlled by alpha between 0 and 1. The tuning parameter lambda controls the overall strength of the penalty. 5.3.1 Identify the Optimal Lambda Parameter To identify the optimal lambda value we used 10-fold cross-validation (CV) with cv.glmnet() from the glmnet package (Friedman, Hastie, and Tibshirani 2010). We selected the minimum lambda value as the metric to determine the optimal lambda value. The figures show the 10-fold CV MSE across all the log lamda values. The numbers across the top of the plot are the number of features in the model. The first dashed line represents the log lamda value with the minimum MSE, and the second is the largest log lamda value within one standard error of it. train_x2&lt;-select(train_x, -SalePrice) test_x2&lt;-select(test_x, -SalePrice) train_y&lt;- log(train_final$SalePrice) Ridge Model* set.seed(123) glm_cv_ridge &lt;- cv.glmnet(as.matrix(train_x2), train_y, alpha = 0) glm_cv_ridge ## ## Call: cv.glmnet(x = as.matrix(train_x2), y = train_y, alpha = 0) ## ## Measure: Mean-Squared Error ## ## Lambda Index Measure SE Nonzero ## min 0.03246 100 0.01352 0.0009632 75 ## 1se 0.17325 82 0.01445 0.0010423 75 plot(glm_cv_ridge) Figure 5.1: Ridge Optimal Lamda ## The optimal lambda value selected for the ridge model is: 0.03246428 ## And the ridge model retains all features, that is 75 features in total. Lasso Model* set.seed(123) glm_cv_lasso &lt;- cv.glmnet(as.matrix(train_x2), train_y, alpha = 1) glm_cv_lasso ## ## Call: cv.glmnet(x = as.matrix(train_x2), y = train_y, alpha = 1) ## ## Measure: Mean-Squared Error ## ## Lambda Index Measure SE Nonzero ## min 0.001222 61 0.01346 0.0009675 51 ## 1se 0.007857 41 0.01429 0.0011390 33 plot(glm_cv_lasso) Figure 5.2: Lasso Optimal Lamda ## The optimal lambda value selected for the lasso model is: 0.001222259 ## And the number of features to be retained is: 51 Elastic Net Model * set.seed(123) glm_cv_net&lt;- cv.glmnet(data.matrix(train_x2), train_y, alpha = 0.1) glm_cv_net ## ## Call: cv.glmnet(x = data.matrix(train_x2), y = train_y, alpha = 0.1) ## ## Measure: Mean-Squared Error ## ## Lambda Index Measure SE Nonzero ## min 0.01114 62 0.01342 0.000971 55 ## 1se 0.06523 43 0.01426 0.001123 44 plot(glm_cv_net) Figure 5.3: Elastic Net Optimal Lamda ## The optimal lambda value selected for the Elastic Net model is: 0.01113677 ## And the number of features to be retained is: 55 5.4 Training the Regularized Regression Models #Select lambda that reduces error penalty_ridge &lt;- glm_cv_ridge$lambda.min penalty_lasso &lt;- glm_cv_lasso$lambda.min penalty_net &lt;- glm_cv_net$lambda.min set.seed(123) glm_ridge_mod &lt;- glmnet(x = as.matrix(train_x2), y = train_y, alpha = 0, lambda = penalty_ridge,standardize = FALSE) glm_lasso_mod &lt;- glmnet(x = as.matrix(train_x2), y = train_y, alpha = 1, lambda = penalty_lasso,standardize = FALSE) glm_net_mod &lt;- glmnet(x = as.matrix(train_x2), y = train_y, alpha = 0.1, lambda = penalty_net,standardize = FALSE) 5.5 Evaluating the Regularized Regression Models’ Performance The performance accuracy of the models was evaluated by comparing the predictions of each model with the actual sale prices in the training data. The graphs below show the predicted vs true Sale Price, and the score is the Root-Mean-Square Error(RMSE), which is the same metric Kaggle uses in its prediction evaluation for prediction submissions. The smaller the RMSE value the better, and the closer the dots are to the red dashed line, the closer the predicted values are to the actual values (this is based the on training data-set). plot(y_pred_ridge,train_y) abline(a=0, b=1, col=&quot;red&quot;, lwd=3, lty=2) Figure 5.4: Ridge Fit plot(y_pred_lasso,train_y) abline(a=0, b=1, col=&quot;red&quot;, lwd=3, lty=2) Figure 5.5: Lasso Fit plot(y_pred_net,train_y) abline(a=0, b=1, col=&quot;red&quot;, lwd=3, lty=2) Figure 5.6: Elastic Net Fit The three regression models Lasso, Ridge, and Elastic Net performed similarly during training, with RMSE of about 0.11. The choice for best model is the Elastic Net model, because it includes fewer features, so it is a simpler model. Table 5.1: Regression Models Comparison Models Parameters Features Rsquared Corr RMSE Ridge 0.0324643 75 0.9222852 0.9604194 0.1103787 Lasso 0.0012223 51 0.9226179 0.9605517 0.1101422 Elastic Net 0.0111368 55 0.9223046 0.9604168 0.1103650 Among the top features selected by the three models are several of the features created during the feature engineering phase. For example, the Total Area, Neighborhodd Category, Overall Score, Total Baths, and Age Category are in the top most important features, and all of these were the result of feature engineering. lassoVarImp &lt;- varImp(glm_lasso_mod,scale=F, lambda = penalty_lasso) lassoImportance &lt;- lassoVarImp$importance varsSelected &lt;- length(which(lassoImportance$Overall!=0)) varsNotSelected &lt;- length(which(lassoImportance$Overall==0)) vip(glm_ridge_mod, num_features = 20, geom = &quot;point&quot;, aesthetics = list(color = &quot;Blue&quot;))+ theme_light() Figure 5.7: Top 20 Features Ordered by Importance Ridge Model vip(glm_lasso_mod, num_features = 20, geom = &quot;point&quot;,aesthetics = list(color = &quot;Blue&quot;))+ theme_light() Figure 5.8: Top 20 Features Ordered by Importance Lasso Model vip(glm_net_mod, num_features = 20, geom = &quot;point&quot;,aesthetics = list(color = &quot;Blue&quot;))+ theme_light() Figure 5.9: Top 20 Features Ordered by Importance Elastic Net Model 5.6 Training XGBoost Model with Feature Selection Next we will implement an advanced machine learning model with a gradient boosted tree model using the xgboost package (Chen et al. 2021). First, we will do some feature selection using the Boruta algorithm from the Boruta package (Kursa and Rudnicki 2010). The Boruta algorithm is a wrapper built around the random forest classification algorithm. It selects the important features with respect to the outcome variable. Feature Selection Boruta selected 65 attributes confirmed important, and 88 attributes confirmed unimportant. After the boruta feature selection, 88 features were removed, living the data-set with 75 columns, the selected features are listed in the next page. # The Boruta algorithm for feature selection uses Random Forest set.seed(123) # to get same results train_boruta &lt;- Boruta(SalePrice~., data=train_1, doTrace=2, maxRuns=125) # printing the results #print(train_boruta) Selected Features: ## [1] &quot;MSSubClass&quot; &quot;LotFrontage&quot; &quot;LotArea&quot; ## [4] &quot;OverallQual&quot; &quot;OverallCond&quot; &quot;MasVnrArea&quot; ## [7] &quot;ExterQual&quot; &quot;BsmtExposure&quot; &quot;BsmtUnfSF&quot; ## [10] &quot;TotalBsmtSF&quot; &quot;HeatingQC&quot; &quot;CentralAir&quot; ## [13] &quot;Electrical&quot; &quot;GrLivArea&quot; &quot;BedroomAbvGr&quot; ## [16] &quot;TotRmsAbvGrd&quot; &quot;Functional&quot; &quot;Fireplaces&quot; ## [19] &quot;FireplaceQu&quot; &quot;GarageFinish&quot; &quot;GarageCars&quot; ## [22] &quot;PavedDrive&quot; &quot;WoodDeckSF&quot; &quot;NewBuild&quot; ## [25] &quot;BSmtFinSFComb&quot; &quot;TotalArea&quot; &quot;TotalBaths&quot; ## [28] &quot;PorchSF&quot; &quot;Neigh_Cat&quot; &quot;AgeCat&quot; ## [31] &quot;RemodelFromCat&quot; &quot;GarageScore&quot; &quot;OverallScore&quot; ## [34] &quot;ExterScore&quot; &quot;KitchenScore&quot; &quot;GarageGrade&quot; ## [37] &quot;MSSubClass.1&quot; &quot;MSZoning.FV&quot; &quot;MSZoning.RL&quot; ## [40] &quot;MSZoning.RM&quot; &quot;LotShape.Reg&quot; &quot;LandContour.Lvl&quot; ## [43] &quot;LotConfig.CulDSac&quot; &quot;BldgType.Duplex&quot; &quot;BldgType.Twnhs&quot; ## [46] &quot;BldgType.TwnhsE&quot; &quot;HouseStyle.1Story&quot; &quot;HouseStyle.2Story&quot; ## [49] &quot;HouseStyle.SLvl&quot; &quot;RoofStyle.Gable&quot; &quot;RoofStyle.Hip&quot; ## [52] &quot;Exterior1st.CemntBd&quot; &quot;Exterior1st.HdBoard&quot; &quot;Exterior1st.VinylSd&quot; ## [55] &quot;Exterior2nd.CmentBd&quot; &quot;Exterior2nd.MetalSd&quot; &quot;Exterior2nd.Plywood&quot; ## [58] &quot;Exterior2nd.VinylSd&quot; &quot;MasVnrType.BrkFace&quot; &quot;MasVnrType.None&quot; ## [61] &quot;MasVnrType.Stone&quot; &quot;Foundation.CBlock&quot; &quot;Foundation.PConc&quot; ## [64] &quot;GarageType.Attchd&quot; &quot;GarageType.BuiltIn&quot; &quot;GarageType.Detchd&quot; ## [67] &quot;GarageType.None&quot; &quot;SaleType.New&quot; &quot;SaleType.WD&quot; ## [70] &quot;SaleCondition.Normal&quot; &quot;SaleCondition.Partial&quot; This plot shows the selected features in green, and the rejected features in red. The yellow, represent tentative attributes. Figure 5.10: Boruta Feature Selection 5.6.1 Training the XGboost Model The XGBoost model has several sets of parameters that can be customized: we focused on only two: general parameters, which relate to which booster is used, in this case we selected a ‘gbtree’; and the second parameters we tuned were the booster parameters. For the booster parameter, we used 5-fold cross validation to determine the optimal number of rounds. For the rest of the booster parameters, the best way to find the optimal parameters is to set a search grid, however this was taking too long (more than an hour to run). So, instead of using this approach I started with the default parameters, and manually changed the min_child_weight(default=1), and eta(default = 0.3), about three times, until I got a better RMSE on the training data. This is not the ideal way to do it, but I am not sure my computer was ever going to get through it. The final parameters chosen, which resulted in training RMSE:0.125043, were: 5.6.1.1 Tuning Paraments ## [1] train-rmse:10.953498+0.002662 test-rmse:10.953500+0.011326 ## Multiple eval metrics are present. Will use test_rmse for early stopping. ## Will train until test_rmse hasn&#39;t improved in 10 rounds. ## ## [2] train-rmse:10.407027+0.002537 test-rmse:10.407029+0.011470 ## [3] train-rmse:9.887894+0.002419 test-rmse:9.887896+0.011608 ## [4] train-rmse:9.394737+0.002307 test-rmse:9.394738+0.011742 ## [5] train-rmse:8.926258+0.002201 test-rmse:8.926259+0.011869 ## [6] train-rmse:8.481207+0.002096 test-rmse:8.481372+0.012111 ## [7] train-rmse:8.058389+0.001994 test-rmse:8.058555+0.011639 ## [8] train-rmse:7.656692+0.001894 test-rmse:7.656859+0.011849 ## [9] train-rmse:7.275063+0.001801 test-rmse:7.275431+0.011417 ## [10] train-rmse:6.912498+0.001711 test-rmse:6.912623+0.010976 ## [11] train-rmse:6.568046+0.001629 test-rmse:6.568478+0.010851 ## [12] train-rmse:6.240804+0.001547 test-rmse:6.240951+0.010706 ## [13] train-rmse:5.929916+0.001471 test-rmse:5.930394+0.010632 ## [14] train-rmse:5.634563+0.001398 test-rmse:5.634505+0.010604 ## [15] train-rmse:5.353969+0.001329 test-rmse:5.354346+0.010289 ## [16] train-rmse:5.087400+0.001263 test-rmse:5.087583+0.010231 ## [17] train-rmse:4.834158+0.001198 test-rmse:4.834051+0.010390 ## [18] train-rmse:4.593575+0.001139 test-rmse:4.593887+0.009861 ## [19] train-rmse:4.365023+0.001083 test-rmse:4.365229+0.009583 ## [20] train-rmse:4.147903+0.001030 test-rmse:4.147886+0.009588 ## [21] train-rmse:3.941640+0.000976 test-rmse:3.941590+0.009583 ## [22] train-rmse:3.745685+0.000930 test-rmse:3.745705+0.009580 ## [23] train-rmse:3.559526+0.000880 test-rmse:3.559161+0.009203 ## [24] train-rmse:3.382670+0.000831 test-rmse:3.382171+0.009355 ## [25] train-rmse:3.214658+0.000789 test-rmse:3.214105+0.009070 ## [26] train-rmse:3.055035+0.000754 test-rmse:3.054762+0.009103 ## [27] train-rmse:2.903400+0.000711 test-rmse:2.903178+0.009339 ## [28] train-rmse:2.759342+0.000675 test-rmse:2.759266+0.009237 ## [29] train-rmse:2.622475+0.000636 test-rmse:2.622701+0.009289 ## [30] train-rmse:2.492446+0.000593 test-rmse:2.492574+0.009357 ## [31] train-rmse:2.368930+0.000565 test-rmse:2.368912+0.009414 ## [32] train-rmse:2.251575+0.000549 test-rmse:2.251385+0.009793 ## [33] train-rmse:2.140104+0.000520 test-rmse:2.139894+0.009558 ## [34] train-rmse:2.034207+0.000489 test-rmse:2.034325+0.009486 ## [35] train-rmse:1.933595+0.000466 test-rmse:1.933504+0.009618 ## [36] train-rmse:1.838021+0.000442 test-rmse:1.837935+0.009536 ## [37] train-rmse:1.747226+0.000427 test-rmse:1.747222+0.009566 ## [38] train-rmse:1.660984+0.000395 test-rmse:1.661250+0.009462 ## [39] train-rmse:1.579069+0.000377 test-rmse:1.579350+0.009699 ## [40] train-rmse:1.501250+0.000364 test-rmse:1.501468+0.009740 ## [41] train-rmse:1.427334+0.000348 test-rmse:1.427608+0.009724 ## [42] train-rmse:1.357122+0.000329 test-rmse:1.357372+0.009754 ## [43] train-rmse:1.290431+0.000311 test-rmse:1.290948+0.009706 ## [44] train-rmse:1.227095+0.000305 test-rmse:1.227542+0.009601 ## [45] train-rmse:1.166930+0.000309 test-rmse:1.167430+0.009664 ## [46] train-rmse:1.109773+0.000313 test-rmse:1.110418+0.009641 ## [47] train-rmse:1.055510+0.000312 test-rmse:1.056204+0.009705 ## [48] train-rmse:1.003953+0.000291 test-rmse:1.004609+0.009401 ## [49] train-rmse:0.954989+0.000305 test-rmse:0.955836+0.009388 ## [50] train-rmse:0.908505+0.000308 test-rmse:0.909544+0.009386 ## [51] train-rmse:0.864328+0.000302 test-rmse:0.865553+0.009170 ## [52] train-rmse:0.822398+0.000306 test-rmse:0.823729+0.009069 ## [53] train-rmse:0.782591+0.000309 test-rmse:0.784122+0.008928 ## [54] train-rmse:0.744773+0.000315 test-rmse:0.746417+0.008772 ## [55] train-rmse:0.708864+0.000335 test-rmse:0.710733+0.008631 ## [56] train-rmse:0.674771+0.000340 test-rmse:0.676792+0.008518 ## [57] train-rmse:0.642397+0.000350 test-rmse:0.644716+0.008424 ## [58] train-rmse:0.611671+0.000363 test-rmse:0.614350+0.008328 ## [59] train-rmse:0.582489+0.000379 test-rmse:0.585387+0.008089 ## [60] train-rmse:0.554802+0.000405 test-rmse:0.557844+0.007957 ## [61] train-rmse:0.528515+0.000438 test-rmse:0.531768+0.007987 ## [62] train-rmse:0.503541+0.000451 test-rmse:0.507436+0.007835 ## [63] train-rmse:0.479847+0.000440 test-rmse:0.484284+0.007884 ## [64] train-rmse:0.457369+0.000446 test-rmse:0.462039+0.007809 ## [65] train-rmse:0.436032+0.000469 test-rmse:0.441179+0.007759 ## [66] train-rmse:0.415778+0.000474 test-rmse:0.421474+0.007806 ## [67] train-rmse:0.396563+0.000478 test-rmse:0.402679+0.007802 ## [68] train-rmse:0.378304+0.000493 test-rmse:0.384998+0.007727 ## [69] train-rmse:0.361010+0.000509 test-rmse:0.368250+0.007626 ## [70] train-rmse:0.344615+0.000519 test-rmse:0.352482+0.007567 ## [71] train-rmse:0.329046+0.000538 test-rmse:0.337491+0.007509 ## [72] train-rmse:0.314294+0.000534 test-rmse:0.323483+0.007389 ## [73] train-rmse:0.300291+0.000554 test-rmse:0.310218+0.007333 ## [74] train-rmse:0.287039+0.000564 test-rmse:0.297528+0.007181 ## [75] train-rmse:0.274457+0.000550 test-rmse:0.285703+0.007074 ## [76] train-rmse:0.262548+0.000551 test-rmse:0.274595+0.007097 ## [77] train-rmse:0.251266+0.000602 test-rmse:0.264129+0.007034 ## [78] train-rmse:0.240584+0.000613 test-rmse:0.254369+0.007087 ## [79] train-rmse:0.230480+0.000642 test-rmse:0.245206+0.007038 ## [80] train-rmse:0.220903+0.000670 test-rmse:0.236374+0.007032 ## [81] train-rmse:0.211861+0.000701 test-rmse:0.228288+0.006980 ## [82] train-rmse:0.203254+0.000708 test-rmse:0.220612+0.006813 ## [83] train-rmse:0.195151+0.000718 test-rmse:0.213478+0.006736 ## [84] train-rmse:0.187469+0.000705 test-rmse:0.206846+0.006694 ## [85] train-rmse:0.180213+0.000708 test-rmse:0.200539+0.006668 ## [86] train-rmse:0.173364+0.000730 test-rmse:0.194680+0.006611 ## [87] train-rmse:0.166874+0.000763 test-rmse:0.189286+0.006525 ## [88] train-rmse:0.160735+0.000805 test-rmse:0.184193+0.006434 ## [89] train-rmse:0.154921+0.000829 test-rmse:0.179433+0.006418 ## [90] train-rmse:0.149479+0.000854 test-rmse:0.175101+0.006402 ## [91] train-rmse:0.144336+0.000856 test-rmse:0.171043+0.006303 ## [92] train-rmse:0.139483+0.000921 test-rmse:0.167266+0.006210 ## [93] train-rmse:0.134862+0.000956 test-rmse:0.163805+0.006214 ## [94] train-rmse:0.130473+0.000948 test-rmse:0.160556+0.006201 ## [95] train-rmse:0.126380+0.000990 test-rmse:0.157546+0.006104 ## [96] train-rmse:0.122531+0.001017 test-rmse:0.154845+0.005983 ## [97] train-rmse:0.118931+0.001034 test-rmse:0.152217+0.005826 ## [98] train-rmse:0.115578+0.001052 test-rmse:0.149739+0.005760 ## [99] train-rmse:0.112366+0.001043 test-rmse:0.147436+0.005612 ## [100] train-rmse:0.109314+0.001079 test-rmse:0.145427+0.005504 ## [101] train-rmse:0.106473+0.001115 test-rmse:0.143636+0.005468 ## [102] train-rmse:0.103867+0.001089 test-rmse:0.141874+0.005426 ## [103] train-rmse:0.101354+0.001069 test-rmse:0.140297+0.005344 ## [104] train-rmse:0.099028+0.001117 test-rmse:0.138833+0.005311 ## [105] train-rmse:0.096839+0.001098 test-rmse:0.137570+0.005304 ## [106] train-rmse:0.094805+0.001054 test-rmse:0.136295+0.005254 ## [107] train-rmse:0.092885+0.001042 test-rmse:0.135233+0.005278 ## [108] train-rmse:0.091067+0.001043 test-rmse:0.134260+0.005238 ## [109] train-rmse:0.089406+0.001055 test-rmse:0.133389+0.005237 ## [110] train-rmse:0.087738+0.001034 test-rmse:0.132444+0.005219 ## [111] train-rmse:0.086227+0.001034 test-rmse:0.131624+0.005182 ## [112] train-rmse:0.084791+0.001035 test-rmse:0.130840+0.005181 ## [113] train-rmse:0.083434+0.000984 test-rmse:0.130097+0.005214 ## [114] train-rmse:0.082178+0.000965 test-rmse:0.129512+0.005172 ## [115] train-rmse:0.081006+0.000992 test-rmse:0.128944+0.005153 ## [116] train-rmse:0.079896+0.000981 test-rmse:0.128416+0.005154 ## [117] train-rmse:0.078860+0.000970 test-rmse:0.127933+0.005181 ## [118] train-rmse:0.077855+0.000998 test-rmse:0.127476+0.005202 ## [119] train-rmse:0.076890+0.000992 test-rmse:0.127017+0.005212 ## [120] train-rmse:0.075970+0.000990 test-rmse:0.126728+0.005171 ## [121] train-rmse:0.075147+0.001004 test-rmse:0.126367+0.005158 ## [122] train-rmse:0.074300+0.001009 test-rmse:0.126011+0.005164 ## [123] train-rmse:0.073515+0.000997 test-rmse:0.125725+0.005244 ## [124] train-rmse:0.072737+0.000951 test-rmse:0.125451+0.005214 ## [125] train-rmse:0.072030+0.000995 test-rmse:0.125222+0.005238 ## [126] train-rmse:0.071333+0.001023 test-rmse:0.124928+0.005211 ## [127] train-rmse:0.070672+0.001045 test-rmse:0.124678+0.005223 ## [128] train-rmse:0.070039+0.001048 test-rmse:0.124509+0.005232 ## [129] train-rmse:0.069458+0.001091 test-rmse:0.124378+0.005313 ## [130] train-rmse:0.068888+0.001182 test-rmse:0.124222+0.005365 ## [131] train-rmse:0.068351+0.001155 test-rmse:0.124061+0.005331 ## [132] train-rmse:0.067799+0.001137 test-rmse:0.123878+0.005335 ## [133] train-rmse:0.067282+0.001070 test-rmse:0.123718+0.005393 ## [134] train-rmse:0.066808+0.001134 test-rmse:0.123649+0.005443 ## [135] train-rmse:0.066353+0.001011 test-rmse:0.123494+0.005567 ## [136] train-rmse:0.065917+0.000970 test-rmse:0.123389+0.005598 ## [137] train-rmse:0.065432+0.000881 test-rmse:0.123264+0.005637 ## [138] train-rmse:0.064958+0.000864 test-rmse:0.123180+0.005666 ## [139] train-rmse:0.064510+0.000941 test-rmse:0.123115+0.005711 ## [140] train-rmse:0.064169+0.000919 test-rmse:0.122997+0.005730 ## [141] train-rmse:0.063687+0.000850 test-rmse:0.122962+0.005700 ## [142] train-rmse:0.063311+0.000782 test-rmse:0.122891+0.005732 ## [143] train-rmse:0.062984+0.000768 test-rmse:0.122841+0.005741 ## [144] train-rmse:0.062638+0.000769 test-rmse:0.122715+0.005741 ## [145] train-rmse:0.062291+0.000781 test-rmse:0.122663+0.005797 ## [146] train-rmse:0.061941+0.000751 test-rmse:0.122614+0.005782 ## [147] train-rmse:0.061664+0.000754 test-rmse:0.122546+0.005762 ## [148] train-rmse:0.061290+0.000729 test-rmse:0.122474+0.005795 ## [149] train-rmse:0.061028+0.000710 test-rmse:0.122407+0.005812 ## [150] train-rmse:0.060742+0.000670 test-rmse:0.122370+0.005849 ## [151] train-rmse:0.060512+0.000701 test-rmse:0.122330+0.005841 ## [152] train-rmse:0.060188+0.000638 test-rmse:0.122257+0.005904 ## [153] train-rmse:0.059867+0.000700 test-rmse:0.122229+0.005944 ## [154] train-rmse:0.059528+0.000718 test-rmse:0.122175+0.005927 ## [155] train-rmse:0.059106+0.000696 test-rmse:0.122148+0.005902 ## [156] train-rmse:0.058870+0.000666 test-rmse:0.122145+0.005894 ## [157] train-rmse:0.058633+0.000672 test-rmse:0.122058+0.005910 ## [158] train-rmse:0.058340+0.000707 test-rmse:0.122022+0.005916 ## [159] train-rmse:0.058092+0.000754 test-rmse:0.121986+0.005919 ## [160] train-rmse:0.057842+0.000682 test-rmse:0.121972+0.005934 ## [161] train-rmse:0.057591+0.000715 test-rmse:0.121918+0.005924 ## [162] train-rmse:0.057254+0.000742 test-rmse:0.121823+0.005945 ## [163] train-rmse:0.057007+0.000659 test-rmse:0.121833+0.005970 ## [164] train-rmse:0.056737+0.000718 test-rmse:0.121818+0.005995 ## [165] train-rmse:0.056405+0.000724 test-rmse:0.121707+0.006046 ## [166] train-rmse:0.056100+0.000813 test-rmse:0.121631+0.006005 ## [167] train-rmse:0.055787+0.000865 test-rmse:0.121579+0.006069 ## [168] train-rmse:0.055573+0.000843 test-rmse:0.121614+0.006113 ## [169] train-rmse:0.055377+0.000794 test-rmse:0.121600+0.006153 ## [170] train-rmse:0.055147+0.000820 test-rmse:0.121558+0.006166 ## [171] train-rmse:0.054912+0.000762 test-rmse:0.121549+0.006197 ## [172] train-rmse:0.054658+0.000816 test-rmse:0.121519+0.006198 ## [173] train-rmse:0.054451+0.000845 test-rmse:0.121506+0.006247 ## [174] train-rmse:0.054259+0.000876 test-rmse:0.121521+0.006246 ## [175] train-rmse:0.054025+0.000891 test-rmse:0.121488+0.006204 ## [176] train-rmse:0.053732+0.000844 test-rmse:0.121478+0.006227 ## [177] train-rmse:0.053487+0.000771 test-rmse:0.121460+0.006240 ## [178] train-rmse:0.053300+0.000762 test-rmse:0.121454+0.006288 ## [179] train-rmse:0.053150+0.000765 test-rmse:0.121432+0.006316 ## [180] train-rmse:0.052921+0.000714 test-rmse:0.121413+0.006341 ## [181] train-rmse:0.052697+0.000619 test-rmse:0.121376+0.006351 ## [182] train-rmse:0.052447+0.000563 test-rmse:0.121362+0.006369 ## [183] train-rmse:0.052269+0.000624 test-rmse:0.121342+0.006382 ## [184] train-rmse:0.052119+0.000592 test-rmse:0.121364+0.006386 ## [185] train-rmse:0.051854+0.000603 test-rmse:0.121337+0.006362 ## [186] train-rmse:0.051673+0.000630 test-rmse:0.121329+0.006389 ## [187] train-rmse:0.051421+0.000571 test-rmse:0.121329+0.006423 ## [188] train-rmse:0.051206+0.000631 test-rmse:0.121326+0.006457 ## [189] train-rmse:0.050970+0.000664 test-rmse:0.121316+0.006466 ## [190] train-rmse:0.050775+0.000594 test-rmse:0.121289+0.006481 ## [191] train-rmse:0.050630+0.000593 test-rmse:0.121284+0.006470 ## [192] train-rmse:0.050451+0.000645 test-rmse:0.121280+0.006462 ## [193] train-rmse:0.050239+0.000732 test-rmse:0.121299+0.006498 ## [194] train-rmse:0.050052+0.000774 test-rmse:0.121256+0.006506 ## [195] train-rmse:0.049875+0.000744 test-rmse:0.121239+0.006504 ## [196] train-rmse:0.049656+0.000824 test-rmse:0.121238+0.006508 ## [197] train-rmse:0.049428+0.000871 test-rmse:0.121213+0.006518 ## [198] train-rmse:0.049213+0.000905 test-rmse:0.121228+0.006514 ## [199] train-rmse:0.048989+0.000911 test-rmse:0.121195+0.006489 ## [200] train-rmse:0.048837+0.000880 test-rmse:0.121204+0.006492 ## [201] train-rmse:0.048618+0.000821 test-rmse:0.121196+0.006510 ## [202] train-rmse:0.048401+0.000904 test-rmse:0.121195+0.006510 ## [203] train-rmse:0.048241+0.000913 test-rmse:0.121195+0.006515 ## [204] train-rmse:0.048009+0.000934 test-rmse:0.121202+0.006520 ## [205] train-rmse:0.047814+0.000952 test-rmse:0.121196+0.006518 ## [206] train-rmse:0.047621+0.000924 test-rmse:0.121186+0.006538 ## [207] train-rmse:0.047467+0.000987 test-rmse:0.121220+0.006555 ## [208] train-rmse:0.047159+0.000939 test-rmse:0.121216+0.006536 ## [209] train-rmse:0.047003+0.000986 test-rmse:0.121194+0.006515 ## [210] train-rmse:0.046851+0.001006 test-rmse:0.121211+0.006532 ## [211] train-rmse:0.046560+0.001004 test-rmse:0.121186+0.006565 ## [212] train-rmse:0.046400+0.001020 test-rmse:0.121196+0.006605 ## [213] train-rmse:0.046185+0.001069 test-rmse:0.121201+0.006624 ## [214] train-rmse:0.045946+0.000943 test-rmse:0.121202+0.006649 ## [215] train-rmse:0.045717+0.000941 test-rmse:0.121172+0.006646 ## [216] train-rmse:0.045559+0.000923 test-rmse:0.121201+0.006673 ## [217] train-rmse:0.045425+0.000946 test-rmse:0.121198+0.006683 ## [218] train-rmse:0.045215+0.001012 test-rmse:0.121201+0.006679 ## [219] train-rmse:0.045004+0.001003 test-rmse:0.121209+0.006691 ## [220] train-rmse:0.044800+0.001056 test-rmse:0.121243+0.006718 ## [221] train-rmse:0.044626+0.001107 test-rmse:0.121258+0.006728 ## [222] train-rmse:0.044475+0.001128 test-rmse:0.121241+0.006707 ## [223] train-rmse:0.044302+0.001122 test-rmse:0.121251+0.006731 ## [224] train-rmse:0.044079+0.001115 test-rmse:0.121248+0.006746 ## [225] train-rmse:0.043937+0.001128 test-rmse:0.121237+0.006755 ## Stopping. Best iteration: ## [215] train-rmse:0.045717+0.000941 test-rmse:0.121172+0.006646 ## The XGB model includes 71 features, and the optimal number of rounds: 215 was selected based on RMSE of 0.121172 ## ##### xgb.Booster ## raw: 606.1 Kb ## call: ## xgb.train(params = default_param_sf, data = dtrain_fs, nrounds = xgbcv_fs$best_iteration) ## params (as set within xgb.train): ## booster = &quot;gbtree&quot;, eta = &quot;0.05&quot;, gamma = &quot;0&quot;, max_depth = &quot;6&quot;, min_child_weight = &quot;4&quot;, subsample = &quot;1&quot;, colsample_bytree = &quot;1&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.print.evaluation(period = print_every_n) ## # of features: 71 ## niter: 215 ## nfeatures : 71 5.7 Evaluating XGBoost Model’s Performance The XGB model performs much better (with R-squared=0.9831584) than the best performing regularized regression model: Lasso (R-squared=0.9226179). So, 98% of the variation in the in SalePrice is explained by the XGB model vs only 92% by the Lasso model (similarly with the Ridge and Elastic Net models). Table 5.2: Models Performance Comparison Models Parameters Features Corr Rsquared RMSE Ridge 0.032464278081627 75 0.9604194 0.9222852 0.1103787 Lasso 0.00122225922578168 51 0.9605517 0.9226179 0.1101422 Elastic Net 0.0111367708494731 55 0.9604168 0.9223046 0.1103650 XGBoost 71 0.9916635 0.9831584 0.0513837 References "],["prediction.html", "Chapter 6 Prediction 6.1 Prediction with Regularized Regression Models 6.2 Prediction with XGboost Model", " Chapter 6 Prediction 6.1 Prediction with Regularized Regression Models ## 15 Discrete Features: Id YearBuilt YearRemodAdd BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars MoSold YrSold20 Continous Feature: LotFrontage LotArea MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF X1stFlrSF X2ndFlrSF LowQualFinSF GrLivArea GarageArea WoodDeckSF OpenPorchSF EnclosedPorch X3SsnPorch ScreenPorch PoolArea MiscVal SalePrice23 Nominal Categorical Features: MSSubClass MSZoning Street Alley LandContour LotConfig Neighborhood Condition1 Condition2 BldgType HouseStyle RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType Foundation Heating CentralAir GarageType MiscFeature SaleType SaleCondition23 Ordered Categorical Features: LotShape Utilities LandSlope OverallQual OverallCond ExterQual ExterCond BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 HeatingQC Electrical KitchenQual Functional FireplaceQu GarageFinish GarageQual GarageCond PavedDrive PoolQC Fence[1] train-rmse:10.953498+0.002662 test-rmse:10.953500+0.011326 ## Multiple eval metrics are present. Will use test_rmse for early stopping. ## Will train until test_rmse hasn&#39;t improved in 10 rounds. ## ## [2] train-rmse:10.407027+0.002537 test-rmse:10.407029+0.011470 ## [3] train-rmse:9.887894+0.002419 test-rmse:9.887896+0.011608 ## [4] train-rmse:9.394737+0.002307 test-rmse:9.394738+0.011742 ## [5] train-rmse:8.926258+0.002201 test-rmse:8.926259+0.011869 ## [6] train-rmse:8.481207+0.002096 test-rmse:8.481372+0.012111 ## [7] train-rmse:8.058389+0.001994 test-rmse:8.058555+0.011639 ## [8] train-rmse:7.656692+0.001894 test-rmse:7.656859+0.011849 ## [9] train-rmse:7.275063+0.001801 test-rmse:7.275431+0.011417 ## [10] train-rmse:6.912498+0.001711 test-rmse:6.912623+0.010976 ## [11] train-rmse:6.568046+0.001629 test-rmse:6.568478+0.010851 ## [12] train-rmse:6.240804+0.001547 test-rmse:6.240951+0.010706 ## [13] train-rmse:5.929916+0.001471 test-rmse:5.930394+0.010632 ## [14] train-rmse:5.634563+0.001398 test-rmse:5.634505+0.010604 ## [15] train-rmse:5.353969+0.001329 test-rmse:5.354346+0.010289 ## [16] train-rmse:5.087400+0.001263 test-rmse:5.087583+0.010231 ## [17] train-rmse:4.834158+0.001198 test-rmse:4.834051+0.010390 ## [18] train-rmse:4.593575+0.001139 test-rmse:4.593887+0.009861 ## [19] train-rmse:4.365023+0.001083 test-rmse:4.365229+0.009583 ## [20] train-rmse:4.147903+0.001030 test-rmse:4.147886+0.009588 ## [21] train-rmse:3.941640+0.000976 test-rmse:3.941590+0.009583 ## [22] train-rmse:3.745685+0.000930 test-rmse:3.745705+0.009580 ## [23] train-rmse:3.559526+0.000880 test-rmse:3.559161+0.009203 ## [24] train-rmse:3.382670+0.000831 test-rmse:3.382171+0.009355 ## [25] train-rmse:3.214658+0.000789 test-rmse:3.214105+0.009070 ## [26] train-rmse:3.055035+0.000754 test-rmse:3.054762+0.009103 ## [27] train-rmse:2.903400+0.000711 test-rmse:2.903178+0.009339 ## [28] train-rmse:2.759342+0.000675 test-rmse:2.759266+0.009237 ## [29] train-rmse:2.622475+0.000636 test-rmse:2.622701+0.009289 ## [30] train-rmse:2.492446+0.000593 test-rmse:2.492574+0.009357 ## [31] train-rmse:2.368930+0.000565 test-rmse:2.368912+0.009414 ## [32] train-rmse:2.251575+0.000549 test-rmse:2.251385+0.009793 ## [33] train-rmse:2.140104+0.000520 test-rmse:2.139894+0.009558 ## [34] train-rmse:2.034207+0.000489 test-rmse:2.034325+0.009486 ## [35] train-rmse:1.933595+0.000466 test-rmse:1.933504+0.009618 ## [36] train-rmse:1.838021+0.000442 test-rmse:1.837935+0.009536 ## [37] train-rmse:1.747226+0.000427 test-rmse:1.747222+0.009566 ## [38] train-rmse:1.660984+0.000395 test-rmse:1.661250+0.009462 ## [39] train-rmse:1.579069+0.000377 test-rmse:1.579350+0.009699 ## [40] train-rmse:1.501250+0.000364 test-rmse:1.501468+0.009740 ## [41] train-rmse:1.427334+0.000348 test-rmse:1.427608+0.009724 ## [42] train-rmse:1.357122+0.000329 test-rmse:1.357372+0.009754 ## [43] train-rmse:1.290431+0.000311 test-rmse:1.290948+0.009706 ## [44] train-rmse:1.227095+0.000305 test-rmse:1.227542+0.009601 ## [45] train-rmse:1.166930+0.000309 test-rmse:1.167430+0.009664 ## [46] train-rmse:1.109773+0.000313 test-rmse:1.110418+0.009641 ## [47] train-rmse:1.055510+0.000312 test-rmse:1.056204+0.009705 ## [48] train-rmse:1.003953+0.000291 test-rmse:1.004609+0.009401 ## [49] train-rmse:0.954989+0.000305 test-rmse:0.955836+0.009388 ## [50] train-rmse:0.908505+0.000308 test-rmse:0.909544+0.009386 ## [51] train-rmse:0.864328+0.000302 test-rmse:0.865553+0.009170 ## [52] train-rmse:0.822398+0.000306 test-rmse:0.823729+0.009069 ## [53] train-rmse:0.782591+0.000309 test-rmse:0.784122+0.008928 ## [54] train-rmse:0.744773+0.000315 test-rmse:0.746417+0.008772 ## [55] train-rmse:0.708864+0.000335 test-rmse:0.710733+0.008631 ## [56] train-rmse:0.674771+0.000340 test-rmse:0.676792+0.008518 ## [57] train-rmse:0.642397+0.000350 test-rmse:0.644716+0.008424 ## [58] train-rmse:0.611671+0.000363 test-rmse:0.614350+0.008328 ## [59] train-rmse:0.582489+0.000379 test-rmse:0.585387+0.008089 ## [60] train-rmse:0.554802+0.000405 test-rmse:0.557844+0.007957 ## [61] train-rmse:0.528515+0.000438 test-rmse:0.531768+0.007987 ## [62] train-rmse:0.503541+0.000451 test-rmse:0.507436+0.007835 ## [63] train-rmse:0.479847+0.000440 test-rmse:0.484284+0.007884 ## [64] train-rmse:0.457369+0.000446 test-rmse:0.462039+0.007809 ## [65] train-rmse:0.436032+0.000469 test-rmse:0.441179+0.007759 ## [66] train-rmse:0.415778+0.000474 test-rmse:0.421474+0.007806 ## [67] train-rmse:0.396563+0.000478 test-rmse:0.402679+0.007802 ## [68] train-rmse:0.378304+0.000493 test-rmse:0.384998+0.007727 ## [69] train-rmse:0.361010+0.000509 test-rmse:0.368250+0.007626 ## [70] train-rmse:0.344615+0.000519 test-rmse:0.352482+0.007567 ## [71] train-rmse:0.329046+0.000538 test-rmse:0.337491+0.007509 ## [72] train-rmse:0.314294+0.000534 test-rmse:0.323483+0.007389 ## [73] train-rmse:0.300291+0.000554 test-rmse:0.310218+0.007333 ## [74] train-rmse:0.287039+0.000564 test-rmse:0.297528+0.007181 ## [75] train-rmse:0.274457+0.000550 test-rmse:0.285703+0.007074 ## [76] train-rmse:0.262548+0.000551 test-rmse:0.274595+0.007097 ## [77] train-rmse:0.251266+0.000602 test-rmse:0.264129+0.007034 ## [78] train-rmse:0.240584+0.000613 test-rmse:0.254369+0.007087 ## [79] train-rmse:0.230480+0.000642 test-rmse:0.245206+0.007038 ## [80] train-rmse:0.220903+0.000670 test-rmse:0.236374+0.007032 ## [81] train-rmse:0.211861+0.000701 test-rmse:0.228288+0.006980 ## [82] train-rmse:0.203254+0.000708 test-rmse:0.220612+0.006813 ## [83] train-rmse:0.195151+0.000718 test-rmse:0.213478+0.006736 ## [84] train-rmse:0.187469+0.000705 test-rmse:0.206846+0.006694 ## [85] train-rmse:0.180213+0.000708 test-rmse:0.200539+0.006668 ## [86] train-rmse:0.173364+0.000730 test-rmse:0.194680+0.006611 ## [87] train-rmse:0.166874+0.000763 test-rmse:0.189286+0.006525 ## [88] train-rmse:0.160735+0.000805 test-rmse:0.184193+0.006434 ## [89] train-rmse:0.154921+0.000829 test-rmse:0.179433+0.006418 ## [90] train-rmse:0.149479+0.000854 test-rmse:0.175101+0.006402 ## [91] train-rmse:0.144336+0.000856 test-rmse:0.171043+0.006303 ## [92] train-rmse:0.139483+0.000921 test-rmse:0.167266+0.006210 ## [93] train-rmse:0.134862+0.000956 test-rmse:0.163805+0.006214 ## [94] train-rmse:0.130473+0.000948 test-rmse:0.160556+0.006201 ## [95] train-rmse:0.126380+0.000990 test-rmse:0.157546+0.006104 ## [96] train-rmse:0.122531+0.001017 test-rmse:0.154845+0.005983 ## [97] train-rmse:0.118931+0.001034 test-rmse:0.152217+0.005826 ## [98] train-rmse:0.115578+0.001052 test-rmse:0.149739+0.005760 ## [99] train-rmse:0.112366+0.001043 test-rmse:0.147436+0.005612 ## [100] train-rmse:0.109314+0.001079 test-rmse:0.145427+0.005504 ## [101] train-rmse:0.106473+0.001115 test-rmse:0.143636+0.005468 ## [102] train-rmse:0.103867+0.001089 test-rmse:0.141874+0.005426 ## [103] train-rmse:0.101354+0.001069 test-rmse:0.140297+0.005344 ## [104] train-rmse:0.099028+0.001117 test-rmse:0.138833+0.005311 ## [105] train-rmse:0.096839+0.001098 test-rmse:0.137570+0.005304 ## [106] train-rmse:0.094805+0.001054 test-rmse:0.136295+0.005254 ## [107] train-rmse:0.092885+0.001042 test-rmse:0.135233+0.005278 ## [108] train-rmse:0.091067+0.001043 test-rmse:0.134260+0.005238 ## [109] train-rmse:0.089406+0.001055 test-rmse:0.133389+0.005237 ## [110] train-rmse:0.087738+0.001034 test-rmse:0.132444+0.005219 ## [111] train-rmse:0.086227+0.001034 test-rmse:0.131624+0.005182 ## [112] train-rmse:0.084791+0.001035 test-rmse:0.130840+0.005181 ## [113] train-rmse:0.083434+0.000984 test-rmse:0.130097+0.005214 ## [114] train-rmse:0.082178+0.000965 test-rmse:0.129512+0.005172 ## [115] train-rmse:0.081006+0.000992 test-rmse:0.128944+0.005153 ## [116] train-rmse:0.079896+0.000981 test-rmse:0.128416+0.005154 ## [117] train-rmse:0.078860+0.000970 test-rmse:0.127933+0.005181 ## [118] train-rmse:0.077855+0.000998 test-rmse:0.127476+0.005202 ## [119] train-rmse:0.076890+0.000992 test-rmse:0.127017+0.005212 ## [120] train-rmse:0.075970+0.000990 test-rmse:0.126728+0.005171 ## [121] train-rmse:0.075147+0.001004 test-rmse:0.126367+0.005158 ## [122] train-rmse:0.074300+0.001009 test-rmse:0.126011+0.005164 ## [123] train-rmse:0.073515+0.000997 test-rmse:0.125725+0.005244 ## [124] train-rmse:0.072737+0.000951 test-rmse:0.125451+0.005214 ## [125] train-rmse:0.072030+0.000995 test-rmse:0.125222+0.005238 ## [126] train-rmse:0.071333+0.001023 test-rmse:0.124928+0.005211 ## [127] train-rmse:0.070672+0.001045 test-rmse:0.124678+0.005223 ## [128] train-rmse:0.070039+0.001048 test-rmse:0.124509+0.005232 ## [129] train-rmse:0.069458+0.001091 test-rmse:0.124378+0.005313 ## [130] train-rmse:0.068888+0.001182 test-rmse:0.124222+0.005365 ## [131] train-rmse:0.068351+0.001155 test-rmse:0.124061+0.005331 ## [132] train-rmse:0.067799+0.001137 test-rmse:0.123878+0.005335 ## [133] train-rmse:0.067282+0.001070 test-rmse:0.123718+0.005393 ## [134] train-rmse:0.066808+0.001134 test-rmse:0.123649+0.005443 ## [135] train-rmse:0.066353+0.001011 test-rmse:0.123494+0.005567 ## [136] train-rmse:0.065917+0.000970 test-rmse:0.123389+0.005598 ## [137] train-rmse:0.065432+0.000881 test-rmse:0.123264+0.005637 ## [138] train-rmse:0.064958+0.000864 test-rmse:0.123180+0.005666 ## [139] train-rmse:0.064510+0.000941 test-rmse:0.123115+0.005711 ## [140] train-rmse:0.064169+0.000919 test-rmse:0.122997+0.005730 ## [141] train-rmse:0.063687+0.000850 test-rmse:0.122962+0.005700 ## [142] train-rmse:0.063311+0.000782 test-rmse:0.122891+0.005732 ## [143] train-rmse:0.062984+0.000768 test-rmse:0.122841+0.005741 ## [144] train-rmse:0.062638+0.000769 test-rmse:0.122715+0.005741 ## [145] train-rmse:0.062291+0.000781 test-rmse:0.122663+0.005797 ## [146] train-rmse:0.061941+0.000751 test-rmse:0.122614+0.005782 ## [147] train-rmse:0.061664+0.000754 test-rmse:0.122546+0.005762 ## [148] train-rmse:0.061290+0.000729 test-rmse:0.122474+0.005795 ## [149] train-rmse:0.061028+0.000710 test-rmse:0.122407+0.005812 ## [150] train-rmse:0.060742+0.000670 test-rmse:0.122370+0.005849 ## [151] train-rmse:0.060512+0.000701 test-rmse:0.122330+0.005841 ## [152] train-rmse:0.060188+0.000638 test-rmse:0.122257+0.005904 ## [153] train-rmse:0.059867+0.000700 test-rmse:0.122229+0.005944 ## [154] train-rmse:0.059528+0.000718 test-rmse:0.122175+0.005927 ## [155] train-rmse:0.059106+0.000696 test-rmse:0.122148+0.005902 ## [156] train-rmse:0.058870+0.000666 test-rmse:0.122145+0.005894 ## [157] train-rmse:0.058633+0.000672 test-rmse:0.122058+0.005910 ## [158] train-rmse:0.058340+0.000707 test-rmse:0.122022+0.005916 ## [159] train-rmse:0.058092+0.000754 test-rmse:0.121986+0.005919 ## [160] train-rmse:0.057842+0.000682 test-rmse:0.121972+0.005934 ## [161] train-rmse:0.057591+0.000715 test-rmse:0.121918+0.005924 ## [162] train-rmse:0.057254+0.000742 test-rmse:0.121823+0.005945 ## [163] train-rmse:0.057007+0.000659 test-rmse:0.121833+0.005970 ## [164] train-rmse:0.056737+0.000718 test-rmse:0.121818+0.005995 ## [165] train-rmse:0.056405+0.000724 test-rmse:0.121707+0.006046 ## [166] train-rmse:0.056100+0.000813 test-rmse:0.121631+0.006005 ## [167] train-rmse:0.055787+0.000865 test-rmse:0.121579+0.006069 ## [168] train-rmse:0.055573+0.000843 test-rmse:0.121614+0.006113 ## [169] train-rmse:0.055377+0.000794 test-rmse:0.121600+0.006153 ## [170] train-rmse:0.055147+0.000820 test-rmse:0.121558+0.006166 ## [171] train-rmse:0.054912+0.000762 test-rmse:0.121549+0.006197 ## [172] train-rmse:0.054658+0.000816 test-rmse:0.121519+0.006198 ## [173] train-rmse:0.054451+0.000845 test-rmse:0.121506+0.006247 ## [174] train-rmse:0.054259+0.000876 test-rmse:0.121521+0.006246 ## [175] train-rmse:0.054025+0.000891 test-rmse:0.121488+0.006204 ## [176] train-rmse:0.053732+0.000844 test-rmse:0.121478+0.006227 ## [177] train-rmse:0.053487+0.000771 test-rmse:0.121460+0.006240 ## [178] train-rmse:0.053300+0.000762 test-rmse:0.121454+0.006288 ## [179] train-rmse:0.053150+0.000765 test-rmse:0.121432+0.006316 ## [180] train-rmse:0.052921+0.000714 test-rmse:0.121413+0.006341 ## [181] train-rmse:0.052697+0.000619 test-rmse:0.121376+0.006351 ## [182] train-rmse:0.052447+0.000563 test-rmse:0.121362+0.006369 ## [183] train-rmse:0.052269+0.000624 test-rmse:0.121342+0.006382 ## [184] train-rmse:0.052119+0.000592 test-rmse:0.121364+0.006386 ## [185] train-rmse:0.051854+0.000603 test-rmse:0.121337+0.006362 ## [186] train-rmse:0.051673+0.000630 test-rmse:0.121329+0.006389 ## [187] train-rmse:0.051421+0.000571 test-rmse:0.121329+0.006423 ## [188] train-rmse:0.051206+0.000631 test-rmse:0.121326+0.006457 ## [189] train-rmse:0.050970+0.000664 test-rmse:0.121316+0.006466 ## [190] train-rmse:0.050775+0.000594 test-rmse:0.121289+0.006481 ## [191] train-rmse:0.050630+0.000593 test-rmse:0.121284+0.006470 ## [192] train-rmse:0.050451+0.000645 test-rmse:0.121280+0.006462 ## [193] train-rmse:0.050239+0.000732 test-rmse:0.121299+0.006498 ## [194] train-rmse:0.050052+0.000774 test-rmse:0.121256+0.006506 ## [195] train-rmse:0.049875+0.000744 test-rmse:0.121239+0.006504 ## [196] train-rmse:0.049656+0.000824 test-rmse:0.121238+0.006508 ## [197] train-rmse:0.049428+0.000871 test-rmse:0.121213+0.006518 ## [198] train-rmse:0.049213+0.000905 test-rmse:0.121228+0.006514 ## [199] train-rmse:0.048989+0.000911 test-rmse:0.121195+0.006489 ## [200] train-rmse:0.048837+0.000880 test-rmse:0.121204+0.006492 ## [201] train-rmse:0.048618+0.000821 test-rmse:0.121196+0.006510 ## [202] train-rmse:0.048401+0.000904 test-rmse:0.121195+0.006510 ## [203] train-rmse:0.048241+0.000913 test-rmse:0.121195+0.006515 ## [204] train-rmse:0.048009+0.000934 test-rmse:0.121202+0.006520 ## [205] train-rmse:0.047814+0.000952 test-rmse:0.121196+0.006518 ## [206] train-rmse:0.047621+0.000924 test-rmse:0.121186+0.006538 ## [207] train-rmse:0.047467+0.000987 test-rmse:0.121220+0.006555 ## [208] train-rmse:0.047159+0.000939 test-rmse:0.121216+0.006536 ## [209] train-rmse:0.047003+0.000986 test-rmse:0.121194+0.006515 ## [210] train-rmse:0.046851+0.001006 test-rmse:0.121211+0.006532 ## [211] train-rmse:0.046560+0.001004 test-rmse:0.121186+0.006565 ## [212] train-rmse:0.046400+0.001020 test-rmse:0.121196+0.006605 ## [213] train-rmse:0.046185+0.001069 test-rmse:0.121201+0.006624 ## [214] train-rmse:0.045946+0.000943 test-rmse:0.121202+0.006649 ## [215] train-rmse:0.045717+0.000941 test-rmse:0.121172+0.006646 ## [216] train-rmse:0.045559+0.000923 test-rmse:0.121201+0.006673 ## [217] train-rmse:0.045425+0.000946 test-rmse:0.121198+0.006683 ## [218] train-rmse:0.045215+0.001012 test-rmse:0.121201+0.006679 ## [219] train-rmse:0.045004+0.001003 test-rmse:0.121209+0.006691 ## [220] train-rmse:0.044800+0.001056 test-rmse:0.121243+0.006718 ## [221] train-rmse:0.044626+0.001107 test-rmse:0.121258+0.006728 ## [222] train-rmse:0.044475+0.001128 test-rmse:0.121241+0.006707 ## [223] train-rmse:0.044302+0.001122 test-rmse:0.121251+0.006731 ## [224] train-rmse:0.044079+0.001115 test-rmse:0.121248+0.006746 ## [225] train-rmse:0.043937+0.001128 test-rmse:0.121237+0.006755 ## Stopping. Best iteration: ## [215] train-rmse:0.045717+0.000941 test-rmse:0.121172+0.006646 ## ## The XGB model includes 71 features, and the optimal number of rounds: 215 was selected based on RMSE of 0.121172 ## [1] 113867.4 157183.7 177970.1 197784.8 189829.9 170346.0 6.2 Prediction with XGboost Model #Prediction XGBpred_fs &lt;- predict(xgb_mod_fs, dtest_fs) predictions_XGB_fs &lt;- exp(XGBpred_fs) #reverse the log to the real values head(predictions_XGB_fs) ## [1] 123398.8 163053.9 192411.7 183921.2 193044.5 170095.6 Store the item IDs and class labels in a csv file Ridge_model_final_data2&lt;- data.frame(&quot;ID&quot;=test_final$Id,&quot;SalePrice&quot;=pred_ridge_test_final) write.csv(Ridge_model_final_data2, &quot;Ridge_ModelFinal2&quot;, row.names=FALSE) Lasso_model_final_data2&lt;- data.frame(&quot;ID&quot;=test_final$Id,&quot;SalePrice&quot;=pred_lasso_test_final) write.csv(Lasso_model_final_data2, &quot;Lasso_ModelFinal2&quot;, row.names=FALSE) ElasticNet_model_final_data2&lt;- data.frame(&quot;ID&quot;=test_final$Id,&quot;SalePrice&quot;=pred_net_test_final) write.csv(ElasticNet_model_final_data2, &quot;ElasticNet_ModelFinal2&quot;, row.names=FALSE) xgbmodel_pred_d2_fs&lt;- data.frame(&quot;ID&quot;=test_ID,&quot;SalePrice&quot;=predictions_XGB_fs) write.csv(xgbmodel_pred_d2_fs, &quot;xgbModel_fs_outl&quot;, row.names=FALSE) "],["conclusion.html", "Chapter 7 Conclusion 7.1 Answer to Research Questions 7.2 Future Work", " Chapter 7 Conclusion 7.1 Answer to Research Questions Which are the most relevant features that impact sales price? Based on the best performing model, the following features are the top 20 predictors of SalesPrice for the given data. 11 The most important features for the XGB model were Total Area, Overall Quality, Neighborhood Category, Total Baths and Overall Score. Also, Fireplace Quality, Lot Area, Exterior Quality, GarageCars, Garage Finish, Garage Grade and Central Air had significant impact on sales price. So, what does this mean? For example, Central Air seems to be an important predictor of price, so if you are a buyer you could consider getting a house without central and installing it yourself after the purchase. On the other hand if you were a seller do the opposite to fetch a better sale price. The basement finish quality seems to be important, so you could consider saving by getting an unfinished basement if you were a buyer, and finish the basement if you were a seller. Also the fire place quality had significant impact on price, so as a buyer maybe consider a fireplace that needs some work, while as a seller fix up the fire place before selling, and the same goes for the quality of the garage. Which pre-processing techniques improve the predictive value of the data? Data Cleaning, data engineering, near zero variance filtering, normalizing the response, normalizing and standardizing the numerical features were the pre-processing techniques used. The feature engineering definitely paid off since a few of our transformed features are right up in the top 20 on level of importance at predicting the sales price of homes. Combining the total square footage was successful as was combining the total numbers of baths, total basement square-footage, porch square-footage were also successful (in the top 20). Bining the neighborhoods definitely had an impact since neighborhood category is in the top three predictors for the models. The age category also is in the top 20. By performing feature engineering, feature selection, regularization, and transforming the target variable, we can reduce loss and greatly improve the performance. Also, the model seems to perform better without removing the 4 outliers in the training set. With outliers removed the best score in Kaggle RMSE is 0.12758, while without removing the outliers the best Kaggle RMSE is 0.12592. These values are for the XGB model, but the results are similar for all the other models attempted here. Which model performs best at predicting price? Overall the XGB model performed better both in training and in the final Kaggle prediction score. Which approach in dealing with multicollinearity yields a better prediction? The XGBoost model with feature selection performed better at predicting price than the regularized regression models. Finally, in the quest for a better score in this addictive Kaggle competition, I tried something I had read other competitors were doing to improve their scores. They say, that if you average the top performing model’s predictions, you can get a better prediction score on Kaggle.com’s competitions. I tried it, by averaging the predictions from the XGB, Elastic Net, Lasso and GBM models, and it worked. This trick got me into the top 19%, with an RMSE score of 0.12595. stacking &lt;- data.frame(&quot;ID&quot;=test_ID,&quot;SalePrice&quot;=(predictions_XGB_fs+pred_net_test_final+pred_lasso_test_final)/3) head(stacking) ## ID SalePrice ## 1 1461 116895.9 ## 2 1462 159452.0 ## 3 1463 182998.1 ## 4 1464 193112.0 ## 5 1465 191038.5 ## 6 1466 170291.2 write.csv(stacking , &quot;stacked_fs_outl&quot;, row.names=FALSE) #KAGGLE RMSE: 0.12592 7.2 Future Work Two areas that could be improved is the outlier analyis in the dataset and the search grid for the XGB model. The plot was produced with the package ‘vip’ (Greenwell and Boehmke 2020).↩ "],["bibliography.html", "Chapter 8 Bibliography", " Chapter 8 Bibliography "],["references.html", "References", " References "]]
