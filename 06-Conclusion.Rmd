---
title: "Conclusion"
output:
  html_document:
    df_print: paged
bibliography: [book.bib, packages.bib]
---

# Conclusion

```{r, include=FALSE, eval=TRUE, echo=FALSE}

source('source_script.R') 

```

## Answer to Research Questions

1. Which are the most relevant features that impact sales price?

Based on the best performing model, the following features are the top 20 predictors of SalesPrice for the given data. 

```{r xgbtop20, include=FALSE, eval =TRUE}

feature_importance_fs <- xgb.importance (feature_names = colnames(dtrain_fs),model = xgb_mod_fs)
xgb.plot.importance(importance_matrix = feature_importance_fs[1:20])
xgb.ggplot.importance(importance_matrix = feature_importance_fs[1:20], rel_to_first = TRUE)+ggtitle("XGB Top 20 Features")

```
^[The plot was produced with the package 'vip' [@vip].]

The most important features for the XGB model were Total Area, Overall Quality,  Neighborhood Category, Total Baths and Overall Score. Also, Fireplace Quality, Lot Area, Exterior Quality, GarageCars, Garage Finish, Garage Grade and Central Air had significant impact on sales price. 

So, what does this mean?

For example, Central Air seems to be an important predictor of price, so if you are a buyer you could consider getting a house without central and installing it yourself after the purchase. On the other hand if you were a seller do the opposite to fetch a better sale price. The basement finish quality seems to be important, so you could consider saving by getting an unfinished basement if you were a buyer, and finish the basement if you were a seller. Also the fire place quality had significant impact on price, so as a buyer maybe consider a fireplace that needs some work, while as a seller fix up the fire place before selling, and the same goes for the quality of the garage. 

2. Which pre-processing techniques improve the predictive value of the data?
Data Cleaning, data engineering, near zero variance filtering, normalizing the response, normalizing and standardizing the numerical features were the pre-processing techniques used. 

The feature engineering definitely paid off since a few of our transformed features are right up in the top 20 on level of importance at predicting the sales price of homes. Combining the total square footage was successful as was combining the total numbers of baths, total basement square-footage, porch square-footage were also successful (in the top 20).  Bining the neighborhoods definitely had an impact since neighborhood category is in the top three predictors for the models. The age category also is in the top 20. 

By performing feature engineering, feature selection, regularization, and transforming the target variable,  we can reduce loss and greatly improve the performance.

Also, the model seems to perform better without removing the 4 outliers in the training set. With outliers removed the best score in Kaggle RMSE is 0.12758, while without removing the outliers the best Kaggle RMSE is 0.12592. These values are for the XGB model, but the results are similar for all the other models attempted here. 

3. Which model performs best at predicting price?

Overall the XGB model performed better both in training and in the final Kaggle prediction score.

4. Which approach in dealing with multicollinearity yields a better prediction?
The XGBoost model with feature selection performed better at predicting price than the regularized regression models. 

Finally, in the quest for a better score in this addictive Kaggle competition, I tried something I had read other competitors were doing to improve their scores. They say, that if you average the top performing model's predictions, you can get a better prediction score on Kaggle.com's competitions. I tried it, by averaging the predictions from the XGB, Elastic Net, Lasso and GBM models, and it worked.  This trick got me into the top 19%, with an RMSE score of 0.12595. 

```{r,}

stacking <- data.frame("ID"=test_ID,"SalePrice"=(predictions_XGB_fs+pred_net_test_final+pred_lasso_test_final)/3)
head(stacking)
write.csv(stacking , "stacked_fs_outl", row.names=FALSE) #KAGGLE RMSE: 0.12592

```

## Future Work

Two areas that could be improved is the outlier analyis in the dataset and the search grid for the XGB model. 

# Bibliography
